{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5cd526",
   "metadata": {
    "papermill": {
     "duration": 0.087291,
     "end_time": "2022-04-29T00:20:09.137886",
     "exception": false,
     "start_time": "2022-04-29T00:20:09.050595",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Determining an Effective Math Teaching Method for a Junior High School"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0ecc87",
   "metadata": {
    "papermill": {
     "duration": 0.085131,
     "end_time": "2022-04-29T00:20:09.307942",
     "exception": false,
     "start_time": "2022-04-29T00:20:09.222811",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "    Author: Roelle Kim\n",
    "    Date: 2022-03-27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e21338",
   "metadata": {
    "papermill": {
     "duration": 0.086432,
     "end_time": "2022-04-29T00:20:09.479496",
     "exception": false,
     "start_time": "2022-04-29T00:20:09.393064",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Problem Definition\n",
    "    A Junior High School in your School District is looking for an efficient way to teach math courses for their 8th graders. The school has three math teachers - Ms. Wessen, Ms. Smith and Ms. Ruger. Ms. Wessen believes students learn from the traditional method better while the other two teachers believe they do from the standard-based method.\n",
    "    \n",
    "    So far, all 3 teachers have been delivering classes using the teaching approaches they prefer. However, at the recent school board meeting, the school suggested they want to apply the same teaching method and the same textbook for all students. They expect to see the selected method generally improve students' math scores.\n",
    "    \n",
    "    Using student performance data that contains each student's gender, ethnicity, socio-economic status, math scores and the assigned math teacher, various models will be built to find out if one teaching method is superior to the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ec03c",
   "metadata": {
    "papermill": {
     "duration": 0.084274,
     "end_time": "2022-04-29T00:20:09.648546",
     "exception": false,
     "start_time": "2022-04-29T00:20:09.564272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99298b4a",
   "metadata": {
    "papermill": {
     "duration": 0.085142,
     "end_time": "2022-04-29T00:20:09.818174",
     "exception": false,
     "start_time": "2022-04-29T00:20:09.733032",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Packages and Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b83e07",
   "metadata": {
    "papermill": {
     "duration": 0.086339,
     "end_time": "2022-04-29T00:20:09.991850",
     "exception": false,
     "start_time": "2022-04-29T00:20:09.905511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Before we get further into EDA, we need to import required packages and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5459dac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:10.163590Z",
     "iopub.status.busy": "2022-04-29T00:20:10.163140Z",
     "iopub.status.idle": "2022-04-29T00:20:23.495679Z",
     "shell.execute_reply": "2022-04-29T00:20:23.494364Z"
    },
    "papermill": {
     "duration": 13.420988,
     "end_time": "2022-04-29T00:20:23.498248",
     "exception": false,
     "start_time": "2022-04-29T00:20:10.077260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadstat\r\n",
      "  Downloading pyreadstat-1.1.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from pyreadstat) (1.3.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.2.0->pyreadstat) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.2.0->pyreadstat) (2021.3)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.2.0->pyreadstat) (1.21.5)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyreadstat) (1.16.0)\r\n",
      "Installing collected packages: pyreadstat\r\n",
      "Successfully installed pyreadstat-1.1.5\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyreadstat # To read sav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f04e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:23.681662Z",
     "iopub.status.busy": "2022-04-29T00:20:23.681378Z",
     "iopub.status.idle": "2022-04-29T00:20:32.260238Z",
     "shell.execute_reply": "2022-04-29T00:20:32.259106Z"
    },
    "papermill": {
     "duration": 8.672471,
     "end_time": "2022-04-29T00:20:32.262962",
     "exception": false,
     "start_time": "2022-04-29T00:20:23.590491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import makedirs\n",
    "from os import path\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api       as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from   sklearn.metrics         import mean_squared_error\n",
    "\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from keras.models import Sequential\n",
    "from keras.layers                import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection     import GridSearchCV\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.ensemble     import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed10b272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:32.447937Z",
     "iopub.status.busy": "2022-04-29T00:20:32.447385Z",
     "iopub.status.idle": "2022-04-29T00:20:32.517616Z",
     "shell.execute_reply": "2022-04-29T00:20:32.516741Z"
    },
    "papermill": {
     "duration": 0.163247,
     "end_time": "2022-04-29T00:20:32.519921",
     "exception": false,
     "start_time": "2022-04-29T00:20:32.356674",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student</th>\n",
       "      <th>Teacher</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnic</th>\n",
       "      <th>Freeredu</th>\n",
       "      <th>Score</th>\n",
       "      <th>wesson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>76.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>56.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>34.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>73.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Male</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>58.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>62.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>82.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>78.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student Teacher  Gender            Ethnic    Freeredu  Score       wesson\n",
       "0      1.0   Ruger  Female             Asian  Free lunch   76.0  Ruger_Smith\n",
       "1      2.0   Ruger  Female          Hispanic  Paid lunch   56.0  Ruger_Smith\n",
       "2      3.0   Ruger  Female  African-American  Free lunch   34.0  Ruger_Smith\n",
       "3      4.0   Ruger  Female             Asian  Paid lunch   59.0  Ruger_Smith\n",
       "4      5.0   Ruger    Male          Hispanic  Free lunch   73.0  Ruger_Smith\n",
       "5      6.0   Ruger    Male         Caucasian  Paid lunch   58.0  Ruger_Smith\n",
       "6      7.0   Ruger  Female  African-American  Paid lunch   62.0  Ruger_Smith\n",
       "7      8.0   Ruger    Male          Hispanic  Free lunch   40.0  Ruger_Smith\n",
       "8      9.0   Ruger  Female  African-American  Free lunch   82.0  Ruger_Smith\n",
       "9     10.0   Ruger    Male  African-American  Paid lunch   78.0  Ruger_Smith"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "PATH = '../input/students-math-score-for-different-teaching-style/'\n",
    "FILE = \"1ResearchProjectData.sav\"\n",
    "df = pd.read_spss('../input/students-math-score-for-different-teaching-style/1ResearchProjectData.sav')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ba7ab",
   "metadata": {
    "papermill": {
     "duration": 0.089461,
     "end_time": "2022-04-29T00:20:32.701259",
     "exception": false,
     "start_time": "2022-04-29T00:20:32.611798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50bf93",
   "metadata": {
    "papermill": {
     "duration": 0.089663,
     "end_time": "2022-04-29T00:20:32.880925",
     "exception": false,
     "start_time": "2022-04-29T00:20:32.791262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Data dictionary shows the nice overview of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "829d458d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:33.067525Z",
     "iopub.status.busy": "2022-04-29T00:20:33.067121Z",
     "iopub.status.idle": "2022-04-29T00:20:33.113589Z",
     "shell.execute_reply": "2022-04-29T00:20:33.112738Z"
    },
    "papermill": {
     "duration": 0.141491,
     "end_time": "2022-04-29T00:20:33.115685",
     "exception": false,
     "start_time": "2022-04-29T00:20:32.974194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample Data</th>\n",
       "      <th>Unique Counts</th>\n",
       "      <th>Null Counts</th>\n",
       "      <th>Average</th>\n",
       "      <th>SD</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Student</th>\n",
       "      <td>1.0</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "      <td>116.18725</td>\n",
       "      <td>77.542899</td>\n",
       "      <td>1.0</td>\n",
       "      <td>216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Teacher</th>\n",
       "      <td>Ruger</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ethnic</th>\n",
       "      <td>Asian</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Freeredu</th>\n",
       "      <td>Free lunch</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score</th>\n",
       "      <td>76.0</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>77.454325</td>\n",
       "      <td>61.401374</td>\n",
       "      <td>16.361453</td>\n",
       "      <td>216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wesson</th>\n",
       "      <td>Ruger_Smith</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sample Data  Unique Counts  Null Counts    Average         SD  \\\n",
       "Student           1.0            217            1  116.18725  77.542899   \n",
       "Teacher         Ruger              4            1                         \n",
       "Gender         Female              3            1                         \n",
       "Ethnic          Asian              5            1                         \n",
       "Freeredu   Free lunch              3            1                         \n",
       "Score            76.0             64            1  77.454325  61.401374   \n",
       "wesson    Ruger_Smith              2            0                         \n",
       "\n",
       "                Min    Max  \n",
       "Student         1.0  216.0  \n",
       "Teacher                     \n",
       "Gender                      \n",
       "Ethnic                      \n",
       "Freeredu                    \n",
       "Score     16.361453  216.0  \n",
       "wesson                      "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lists to create a new dataframe\n",
    "column_n_li = df.columns.values\n",
    "sample_data_li = df.iloc[0].values\n",
    "\n",
    "# count unique values per column\n",
    "unique_counts_li = []\n",
    "for col_n in column_n_li:\n",
    "    unique_counts_li.append(len(df[col_n].unique()))\n",
    "\n",
    "# count null values per column\n",
    "null_counts = df.isna().sum()\n",
    "null_counts_li = null_counts.values\n",
    "\n",
    "# for numeric only\n",
    "dfNum = df.describe()\n",
    "average_li = dfNum.mean(axis=0).rename('Average')\n",
    "sd_li = dfNum.std(axis=0).rename('SD')\n",
    "min_li = dfNum.min(axis=0).rename('Min')\n",
    "max_li = dfNum.max(axis=0).rename('Max')\n",
    "numeric_dfs = [average_li, sd_li, min_li, max_li]\n",
    "\n",
    "numeric_df_joined = pd.concat(numeric_dfs, join='inner', axis=1)\n",
    "\n",
    "data_dict = {\n",
    "    \"Sample Data\" : sample_data_li\n",
    "    , \"Unique Counts\" : unique_counts_li\n",
    "    , \"Null Counts\" : null_counts_li\n",
    "}\n",
    "\n",
    "data_dict_df = pd.DataFrame(data_dict, index=column_n_li)\n",
    "data_dict_df = pd.concat([data_dict_df, numeric_df_joined], join='outer', axis=1)\n",
    "\n",
    "# replace nan with an empty string\n",
    "data_dict_df.fillna('', inplace=True)\n",
    "\n",
    "data_dict_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708cd41",
   "metadata": {
    "papermill": {
     "duration": 0.089911,
     "end_time": "2022-04-29T00:20:33.296109",
     "exception": false,
     "start_time": "2022-04-29T00:20:33.206198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Student column is replaced with incremental numbers to protect students' privacy. Other columns are showing each student's ethnicity, socioeconomic status (free lunch or paid lunch), gender, math score and whom they have for their math classes.\n",
    "\n",
    "\"wesson\" column shows what style of math education each student is receiving. \"Ruger_Smith\" is the standards-based method, conducted by Ms. Ruger and Ms. Smith. \"Wesson\" is the traditional method, conducted by Ms. Wesson.\n",
    "\n",
    "All rows except for the \"wesson\" have 1 null counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d3b35",
   "metadata": {
    "papermill": {
     "duration": 0.090848,
     "end_time": "2022-04-29T00:20:33.477825",
     "exception": false,
     "start_time": "2022-04-29T00:20:33.386977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Null Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f1c2e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:33.660040Z",
     "iopub.status.busy": "2022-04-29T00:20:33.659266Z",
     "iopub.status.idle": "2022-04-29T00:20:33.675571Z",
     "shell.execute_reply": "2022-04-29T00:20:33.674953Z"
    },
    "papermill": {
     "duration": 0.1095,
     "end_time": "2022-04-29T00:20:33.677459",
     "exception": false,
     "start_time": "2022-04-29T00:20:33.567959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student</th>\n",
       "      <th>Teacher</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnic</th>\n",
       "      <th>Freeredu</th>\n",
       "      <th>Score</th>\n",
       "      <th>wesson</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Student Teacher Gender Ethnic Freeredu  Score       wesson\n",
       "216      NaN     NaN    NaN    NaN      NaN    NaN  Ruger_Smith"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ab630",
   "metadata": {
    "papermill": {
     "duration": 0.090527,
     "end_time": "2022-04-29T00:20:33.859846",
     "exception": false,
     "start_time": "2022-04-29T00:20:33.769319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It appeared almost all columns have 1 null count because there was an entry that has everything missing except for the \"wesson\" value. Although we have to avoid dropping anything from the original dataset, we will be removing this row as it is almost certain that this row will not be able to contribute much to training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32bb849c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:34.047148Z",
     "iopub.status.busy": "2022-04-29T00:20:34.046534Z",
     "iopub.status.idle": "2022-04-29T00:20:34.065336Z",
     "shell.execute_reply": "2022-04-29T00:20:34.064743Z"
    },
    "papermill": {
     "duration": 0.114448,
     "end_time": "2022-04-29T00:20:34.067193",
     "exception": false,
     "start_time": "2022-04-29T00:20:33.952745",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** After dropping a null row\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 216 entries, 0 to 215\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype   \n",
      "---  ------    --------------  -----   \n",
      " 0   Student   216 non-null    float64 \n",
      " 1   Teacher   216 non-null    category\n",
      " 2   Gender    216 non-null    category\n",
      " 3   Ethnic    216 non-null    category\n",
      " 4   Freeredu  216 non-null    category\n",
      " 5   Score     216 non-null    float64 \n",
      " 6   wesson    216 non-null    category\n",
      "dtypes: category(5), float64(2)\n",
      "memory usage: 6.8 KB\n"
     ]
    }
   ],
   "source": [
    "# drop the null row.\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "print(\"\\n***** After dropping a null row\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b1107",
   "metadata": {
    "papermill": {
     "duration": 0.091725,
     "end_time": "2022-04-29T00:20:34.249908",
     "exception": false,
     "start_time": "2022-04-29T00:20:34.158183",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now all entries are non-null."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883b7ba7",
   "metadata": {
    "papermill": {
     "duration": 0.090663,
     "end_time": "2022-04-29T00:20:34.432326",
     "exception": false,
     "start_time": "2022-04-29T00:20:34.341663",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Segmentation\n",
    "Students will be categorized in 4 groups based on their grades (A being the highest and D being the lowest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e79e784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:34.618566Z",
     "iopub.status.busy": "2022-04-29T00:20:34.618019Z",
     "iopub.status.idle": "2022-04-29T00:20:34.621397Z",
     "shell.execute_reply": "2022-04-29T00:20:34.620807Z"
    },
    "papermill": {
     "duration": 0.099428,
     "end_time": "2022-04-29T00:20:34.623304",
     "exception": false,
     "start_time": "2022-04-29T00:20:34.523876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the number of partitions\n",
    "NUM_OF_GROUPS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f560646",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:34.809124Z",
     "iopub.status.busy": "2022-04-29T00:20:34.808690Z",
     "iopub.status.idle": "2022-04-29T00:20:34.857997Z",
     "shell.execute_reply": "2022-04-29T00:20:34.857149Z"
    },
    "papermill": {
     "duration": 0.145472,
     "end_time": "2022-04-29T00:20:34.860539",
     "exception": false,
     "start_time": "2022-04-29T00:20:34.715067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sort dataframe\n",
    "sortedDf = df.sort_values(by=\"Score\", ascending=False, ignore_index=True)\n",
    "\n",
    "def assignGroup(df, numGroup):\n",
    "    \"\"\"\n",
    "    Create a \"Group\" column to the passed dataframe and assign an alphabet based on the index.\n",
    "    :param df: sorted dataframe\n",
    "    :param numGroup: number of groups desired to create\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    numRow, numCol = df.shape[0], df.shape[1]\n",
    "    groupRowSize = int(np.floor(numRow / numGroup))\n",
    "\n",
    "    for i in range(numGroup):\n",
    "        for j in range(groupRowSize):\n",
    "            df.loc[i*groupRowSize + j, \"Group\"] = chr(65 + i)\n",
    "\n",
    "assignGroup(sortedDf, NUM_OF_GROUPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d8db5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:35.047278Z",
     "iopub.status.busy": "2022-04-29T00:20:35.046520Z",
     "iopub.status.idle": "2022-04-29T00:20:35.070001Z",
     "shell.execute_reply": "2022-04-29T00:20:35.069305Z"
    },
    "papermill": {
     "duration": 0.117952,
     "end_time": "2022-04-29T00:20:35.071972",
     "exception": false,
     "start_time": "2022-04-29T00:20:34.954020",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student</th>\n",
       "      <th>Teacher</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnic</th>\n",
       "      <th>Freeredu</th>\n",
       "      <th>Score</th>\n",
       "      <th>wesson</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>Smith</td>\n",
       "      <td>Male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>95.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119.0</td>\n",
       "      <td>Smith</td>\n",
       "      <td>Male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>95.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>133.0</td>\n",
       "      <td>Smith</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>95.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>213.0</td>\n",
       "      <td>Wesson</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>94.0</td>\n",
       "      <td>Wesson</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.0</td>\n",
       "      <td>Wesson</td>\n",
       "      <td>Male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>93.0</td>\n",
       "      <td>Wesson</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>44.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>54.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>48.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>21.0</td>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Student Teacher  Gender            Ethnic    Freeredu  Score  \\\n",
       "0      100.0   Smith    Male             Asian  Paid lunch   95.0   \n",
       "1      119.0   Smith    Male  African-American  Free lunch   95.0   \n",
       "2      133.0   Smith  Female         Caucasian  Paid lunch   95.0   \n",
       "3      213.0  Wesson    Male          Hispanic  Free lunch   94.0   \n",
       "4      151.0  Wesson    Male  African-American  Free lunch   93.0   \n",
       "..       ...     ...     ...               ...         ...    ...   \n",
       "211     44.0   Ruger    Male  African-American  Free lunch   32.0   \n",
       "212     29.0   Ruger    Male          Hispanic  Paid lunch   32.0   \n",
       "213     54.0   Ruger  Female             Asian  Free lunch   30.0   \n",
       "214     48.0   Ruger  Female             Asian  Free lunch   30.0   \n",
       "215     21.0   Ruger  Female          Hispanic  Paid lunch   30.0   \n",
       "\n",
       "          wesson Group  \n",
       "0    Ruger_Smith     A  \n",
       "1    Ruger_Smith     A  \n",
       "2    Ruger_Smith     A  \n",
       "3         Wesson     A  \n",
       "4         Wesson     A  \n",
       "..           ...   ...  \n",
       "211  Ruger_Smith     D  \n",
       "212  Ruger_Smith     D  \n",
       "213  Ruger_Smith     D  \n",
       "214  Ruger_Smith     D  \n",
       "215  Ruger_Smith     D  \n",
       "\n",
       "[216 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffbfce4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:35.258868Z",
     "iopub.status.busy": "2022-04-29T00:20:35.258333Z",
     "iopub.status.idle": "2022-04-29T00:20:35.630204Z",
     "shell.execute_reply": "2022-04-29T00:20:35.629304Z"
    },
    "papermill": {
     "duration": 0.468938,
     "end_time": "2022-04-29T00:20:35.632585",
     "exception": false,
     "start_time": "2022-04-29T00:20:35.163647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Group', ylabel='Score'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x=\"Group\", y=\"Score\", data=sortedDf, ci =None, hue=\"Group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abcc499b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:35.821967Z",
     "iopub.status.busy": "2022-04-29T00:20:35.821658Z",
     "iopub.status.idle": "2022-04-29T00:20:35.854945Z",
     "shell.execute_reply": "2022-04-29T00:20:35.854280Z"
    },
    "papermill": {
     "duration": 0.130531,
     "end_time": "2022-04-29T00:20:35.856830",
     "exception": false,
     "start_time": "2022-04-29T00:20:35.726299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>average</th>\n",
       "      <th>sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>79.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>86.203704</td>\n",
       "      <td>4.691943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>65.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>71.462963</td>\n",
       "      <td>3.849592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>53.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>59.425926</td>\n",
       "      <td>3.663187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>30.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.374101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    min   max    average        sd\n",
       "A  79.0  95.0  86.203704  4.691943\n",
       "B  65.0  79.0  71.462963  3.849592\n",
       "C  53.0  65.0  59.425926  3.663187\n",
       "D  30.0  53.0  44.000000  7.374101"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GroupMinMax(groupNLi: list, df: pd.DataFrame, groupColN: str, minmaxColN: str):\n",
    "    \"\"\"\n",
    "    Create a dataframe that shows the min and max scores of each group.\n",
    "    :param groupNLi: list of group names\n",
    "    :param df: dataframe\n",
    "    :param groupColN: name of the group column\n",
    "    :param minmaxColN: name of the score column\n",
    "    :return: a new dataframe\n",
    "    \"\"\"\n",
    "    resDf = pd.DataFrame()\n",
    "    for n in groupNLi:\n",
    "        groupItem = {\n",
    "             \"min\" : df[df[groupColN] == n][minmaxColN].min()\n",
    "            , \"max\" : df[df[groupColN] == n][minmaxColN].max()\n",
    "            , \"average\" : df[df[groupColN] == n][minmaxColN].mean()\n",
    "            , \"sd\" : df[df[groupColN] == n][minmaxColN].std()\n",
    "        }\n",
    "        groupDf = pd.DataFrame(groupItem, index=[n])\n",
    "        resDf = pd.concat([resDf, groupDf])\n",
    "    return resDf\n",
    "\n",
    "GroupMinMax([\"A\", \"B\", \"C\", \"D\"], sortedDf, \"Group\", \"Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224b2a28",
   "metadata": {
    "papermill": {
     "duration": 0.097784,
     "end_time": "2022-04-29T00:20:36.050824",
     "exception": false,
     "start_time": "2022-04-29T00:20:35.953040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Group A is composed of studenets with the highest grades (>= 79 and <= 95)\n",
    "- Group B is compossed of students with the second highest grades (>= 65 and <= 79)\n",
    "- Group C is composed of students with the second lowest grades (>= 53 and <= 65)\n",
    "- Group D is composed of students with the failing grades (>= 30 and <= 53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db21aeaa",
   "metadata": {
    "papermill": {
     "duration": 0.095655,
     "end_time": "2022-04-29T00:20:36.240216",
     "exception": false,
     "start_time": "2022-04-29T00:20:36.144561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To clarify what method is being used to teach each student, a new column called \"method\" will be created. If the student is having Ms. Ruger or Ms. Smith for their math class, they are receiving a \"standard\" style of education. If the studetn is having Ms. Wesson for their math class, they are receiving a \"Traditional\" style of education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c76176f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:36.435109Z",
     "iopub.status.busy": "2022-04-29T00:20:36.434658Z",
     "iopub.status.idle": "2022-04-29T00:20:36.452802Z",
     "shell.execute_reply": "2022-04-29T00:20:36.452072Z"
    },
    "papermill": {
     "duration": 0.118507,
     "end_time": "2022-04-29T00:20:36.454771",
     "exception": false,
     "start_time": "2022-04-29T00:20:36.336264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student</th>\n",
       "      <th>Teacher</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnic</th>\n",
       "      <th>Freeredu</th>\n",
       "      <th>Score</th>\n",
       "      <th>wesson</th>\n",
       "      <th>Group</th>\n",
       "      <th>Method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>Smith</td>\n",
       "      <td>Male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>95.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "      <td>A</td>\n",
       "      <td>Standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119.0</td>\n",
       "      <td>Smith</td>\n",
       "      <td>Male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>95.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "      <td>A</td>\n",
       "      <td>Standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>133.0</td>\n",
       "      <td>Smith</td>\n",
       "      <td>Female</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>95.0</td>\n",
       "      <td>Ruger_Smith</td>\n",
       "      <td>A</td>\n",
       "      <td>Standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>213.0</td>\n",
       "      <td>Wesson</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>94.0</td>\n",
       "      <td>Wesson</td>\n",
       "      <td>A</td>\n",
       "      <td>Traditional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.0</td>\n",
       "      <td>Wesson</td>\n",
       "      <td>Male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>93.0</td>\n",
       "      <td>Wesson</td>\n",
       "      <td>A</td>\n",
       "      <td>Traditional</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Student Teacher  Gender            Ethnic    Freeredu  Score       wesson  \\\n",
       "0    100.0   Smith    Male             Asian  Paid lunch   95.0  Ruger_Smith   \n",
       "1    119.0   Smith    Male  African-American  Free lunch   95.0  Ruger_Smith   \n",
       "2    133.0   Smith  Female         Caucasian  Paid lunch   95.0  Ruger_Smith   \n",
       "3    213.0  Wesson    Male          Hispanic  Free lunch   94.0       Wesson   \n",
       "4    151.0  Wesson    Male  African-American  Free lunch   93.0       Wesson   \n",
       "\n",
       "  Group       Method  \n",
       "0     A     Standard  \n",
       "1     A     Standard  \n",
       "2     A     Standard  \n",
       "3     A  Traditional  \n",
       "4     A  Traditional  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedDf['Method'] = sortedDf['wesson'].apply(lambda x: 'Standard' if 'Ruger_Smith' in x else 'Traditional')\n",
    "\n",
    "sortedDf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec30ef3",
   "metadata": {
    "papermill": {
     "duration": 0.094914,
     "end_time": "2022-04-29T00:20:36.643893",
     "exception": false,
     "start_time": "2022-04-29T00:20:36.548979",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Number of Methods Delivered in Each Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb097d9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:36.837075Z",
     "iopub.status.busy": "2022-04-29T00:20:36.836568Z",
     "iopub.status.idle": "2022-04-29T00:20:37.014320Z",
     "shell.execute_reply": "2022-04-29T00:20:37.013243Z"
    },
    "papermill": {
     "duration": 0.27933,
     "end_time": "2022-04-29T00:20:37.017528",
     "exception": false,
     "start_time": "2022-04-29T00:20:36.738198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (7, 7))\n",
    "plt.pie(sortedDf.Method.value_counts().values, labels = sortedDf.Method.value_counts().index, autopct = '%2.1f%%', textprops={'fontsize': 15})\n",
    "plt.title('Percentages of Math Teaching Methods Delivered to Students', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f40f28a",
   "metadata": {
    "papermill": {
     "duration": 0.096591,
     "end_time": "2022-04-29T00:20:37.228275",
     "exception": false,
     "start_time": "2022-04-29T00:20:37.131684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Note that only 35.2% of students are learning traditionally. Standard teaching is being delivered almost twice more since 2 teachers are adhering to the standard-based approach while only 1 teaccher is to the traditional approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6edfc447",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:37.422317Z",
     "iopub.status.busy": "2022-04-29T00:20:37.422036Z",
     "iopub.status.idle": "2022-04-29T00:20:37.665751Z",
     "shell.execute_reply": "2022-04-29T00:20:37.664545Z"
    },
    "papermill": {
     "duration": 0.343643,
     "end_time": "2022-04-29T00:20:37.668094",
     "exception": false,
     "start_time": "2022-04-29T00:20:37.324451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "groupMethod = pd.pivot_table(sortedDf, index =\"Group\", columns=\"Method\", aggfunc=\"count\", values=\"Score\")\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "groupMethod.plot(kind ='bar')\n",
    "plt.title(\"Number of Methods Delivered in Each Group\")\n",
    "plt.ylabel(\"Number of Students\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8ce1d",
   "metadata": {
    "papermill": {
     "duration": 0.100645,
     "end_time": "2022-04-29T00:20:37.867554",
     "exception": false,
     "start_time": "2022-04-29T00:20:37.766909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Interestingly enough, group A has the same number of students for both Standard and Traditional methods. This could signify there are some other factors contributing to their high scores. Group B has almost twice more students taking Standard-based style math classes. C also has more students from Standard classes; however, the discrepancy between the two methods is very close.\n",
    "\n",
    "What we probably take a closer look at is group D. Even with the consideration that twice more students are taking Standard math classes, a noticeable number of failing students are from the Standard classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b19ce2e",
   "metadata": {
    "papermill": {
     "duration": 0.098685,
     "end_time": "2022-04-29T00:20:38.064214",
     "exception": false,
     "start_time": "2022-04-29T00:20:37.965529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Number of Students Learning from Different Teachers by Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44d6eb06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:38.259157Z",
     "iopub.status.busy": "2022-04-29T00:20:38.258890Z",
     "iopub.status.idle": "2022-04-29T00:20:38.500626Z",
     "shell.execute_reply": "2022-04-29T00:20:38.499735Z"
    },
    "papermill": {
     "duration": 0.34173,
     "end_time": "2022-04-29T00:20:38.502614",
     "exception": false,
     "start_time": "2022-04-29T00:20:38.160884",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "groupTeacher = pd.pivot_table(sortedDf, index =\"Group\", columns=\"Teacher\", aggfunc=\"count\", values=\"Score\")\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "groupTeacher.plot(kind ='bar')\n",
    "plt.title(\"Number of Students Learning from Different Teachers by Group\")\n",
    "plt.ylabel(\"Number of Students\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2fe30f",
   "metadata": {
    "papermill": {
     "duration": 0.09788,
     "end_time": "2022-04-29T00:20:38.698471",
     "exception": false,
     "start_time": "2022-04-29T00:20:38.600591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Earlier in the previous section where we inspected the number of each method delivered to students, we observed teaching methods did not matter to the group A students. However, in this graph, we can see almost all students in group A who received standard-based style teaching were from Ms. Smith's class. It seems like students from Ms. Ruger's class are generally receiving lower grades. Her students account for most group D students.\n",
    "\n",
    "The school was looking for a more efficient method to educate their students; however, they may have to focus more on evaluating their instructors' teaching capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff4aa6",
   "metadata": {
    "papermill": {
     "duration": 0.099128,
     "end_time": "2022-04-29T00:20:38.896016",
     "exception": false,
     "start_time": "2022-04-29T00:20:38.796888",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Number of Student Genders by Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "721156da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:39.098541Z",
     "iopub.status.busy": "2022-04-29T00:20:39.098239Z",
     "iopub.status.idle": "2022-04-29T00:20:39.330242Z",
     "shell.execute_reply": "2022-04-29T00:20:39.329232Z"
    },
    "papermill": {
     "duration": 0.335976,
     "end_time": "2022-04-29T00:20:39.332720",
     "exception": false,
     "start_time": "2022-04-29T00:20:38.996744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "groupGender = pd.pivot_table(sortedDf, index =\"Group\", columns=\"Gender\", aggfunc=\"count\", values=\"Score\")\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "groupGender.plot(kind ='bar')\n",
    "plt.title(\"Number of Student Genders by Group\")\n",
    "plt.ylabel(\"Number of Students\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7339a5d0",
   "metadata": {
    "papermill": {
     "duration": 0.100161,
     "end_time": "2022-04-29T00:20:39.533711",
     "exception": false,
     "start_time": "2022-04-29T00:20:39.433550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There is not much difference in genders for groups A, B, and C; however, group D has significantly more male students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce6b1ff",
   "metadata": {
    "papermill": {
     "duration": 0.100777,
     "end_time": "2022-04-29T00:20:39.735083",
     "exception": false,
     "start_time": "2022-04-29T00:20:39.634306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Number of Student Ethnicities by Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c1301cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:39.942281Z",
     "iopub.status.busy": "2022-04-29T00:20:39.940166Z",
     "iopub.status.idle": "2022-04-29T00:20:40.209641Z",
     "shell.execute_reply": "2022-04-29T00:20:40.209043Z"
    },
    "papermill": {
     "duration": 0.376499,
     "end_time": "2022-04-29T00:20:40.211726",
     "exception": false,
     "start_time": "2022-04-29T00:20:39.835227",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "groupEth = pd.pivot_table(sortedDf, index =\"Group\", columns=\"Ethnic\", aggfunc=\"count\", values=\"Score\")\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "groupEth.plot(kind ='bar')\n",
    "plt.title(\"Number of Student Ethnicities by Group\")\n",
    "plt.ylabel(\"Number of Students\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67d71b",
   "metadata": {
    "papermill": {
     "duration": 0.108146,
     "end_time": "2022-04-29T00:20:40.428345",
     "exception": false,
     "start_time": "2022-04-29T00:20:40.320199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Nothing too significant is found in students' ethnicities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06b0b53",
   "metadata": {
    "papermill": {
     "duration": 0.104651,
     "end_time": "2022-04-29T00:20:40.638619",
     "exception": false,
     "start_time": "2022-04-29T00:20:40.533968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Number of Students Receiving \"Freer Education\" by Group\n",
    "The school provides free lunch to eligible students based on their socioeconomic status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a81d21f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:40.853065Z",
     "iopub.status.busy": "2022-04-29T00:20:40.852625Z",
     "iopub.status.idle": "2022-04-29T00:20:41.088690Z",
     "shell.execute_reply": "2022-04-29T00:20:41.087725Z"
    },
    "papermill": {
     "duration": 0.34391,
     "end_time": "2022-04-29T00:20:41.090841",
     "exception": false,
     "start_time": "2022-04-29T00:20:40.746931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "groupFreerEdu = pd.pivot_table(sortedDf, index =\"Group\", columns=\"Freeredu\", aggfunc=\"count\", values=\"Score\")\n",
    "\n",
    "plt.figure(figsize = (20, 20))\n",
    "groupFreerEdu.plot(kind ='bar')\n",
    "plt.title('Number of Students Receiving \"Freer Education\" by Group')\n",
    "plt.ylabel(\"Number of Students\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5dff27",
   "metadata": {
    "papermill": {
     "duration": 0.10252,
     "end_time": "2022-04-29T00:20:41.297645",
     "exception": false,
     "start_time": "2022-04-29T00:20:41.195125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Students who are paying for lunch seem to be either doing very well or very poorly academically. Both groups A and D have more paid lunch students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1437c23f",
   "metadata": {
    "papermill": {
     "duration": 0.126677,
     "end_time": "2022-04-29T00:20:41.545293",
     "exception": false,
     "start_time": "2022-04-29T00:20:41.418616",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1cd42",
   "metadata": {
    "papermill": {
     "duration": 0.113477,
     "end_time": "2022-04-29T00:20:41.772147",
     "exception": false,
     "start_time": "2022-04-29T00:20:41.658670",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Removing Null Row\n",
    "As seen in the EDA, there was a single row with everything empty. Although we should avoid removing any rows from the original dataset, we have deleted this row as it is hard to how this row can contribute to building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e997251",
   "metadata": {
    "papermill": {
     "duration": 0.112077,
     "end_time": "2022-04-29T00:20:41.994072",
     "exception": false,
     "start_time": "2022-04-29T00:20:41.881995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dropping Irrelevant Columns\n",
    "Following columns are dropped due to the lack of their correlations with the student scores.\n",
    "- Student: this column is simply an incremental number to distinguish each student.\n",
    "- wesson: The teacher column can also show which method is being delivered. Based on EDA, it is the teacher who affects more to students' performances rather than the teaching method; therefore, it is reaesonable to remove wesson over Teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70ca08be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:42.226497Z",
     "iopub.status.busy": "2022-04-29T00:20:42.225621Z",
     "iopub.status.idle": "2022-04-29T00:20:42.246357Z",
     "shell.execute_reply": "2022-04-29T00:20:42.245691Z"
    },
    "papermill": {
     "duration": 0.137749,
     "end_time": "2022-04-29T00:20:42.248584",
     "exception": false,
     "start_time": "2022-04-29T00:20:42.110835",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teacher</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnic</th>\n",
       "      <th>Freeredu</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ruger</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Wesson</td>\n",
       "      <td>Male</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Wesson</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Wesson</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Wesson</td>\n",
       "      <td>Female</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Wesson</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Teacher  Gender            Ethnic    Freeredu  Score\n",
       "0     Ruger  Female             Asian  Free lunch   76.0\n",
       "1     Ruger  Female          Hispanic  Paid lunch   56.0\n",
       "2     Ruger  Female  African-American  Free lunch   34.0\n",
       "3     Ruger  Female             Asian  Paid lunch   59.0\n",
       "4     Ruger    Male          Hispanic  Free lunch   73.0\n",
       "..      ...     ...               ...         ...    ...\n",
       "211  Wesson    Male  African-American  Paid lunch   56.0\n",
       "212  Wesson    Male          Hispanic  Free lunch   94.0\n",
       "213  Wesson    Male          Hispanic  Paid lunch   91.0\n",
       "214  Wesson  Female  African-American  Paid lunch   53.0\n",
       "215  Wesson    Male          Hispanic  Paid lunch   57.0\n",
       "\n",
       "[216 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop Student and wesson columns\n",
    "studentDf = df.drop(['Student', 'wesson'], axis=1)\n",
    "\n",
    "studentDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4694e00a",
   "metadata": {
    "papermill": {
     "duration": 0.111984,
     "end_time": "2022-04-29T00:20:42.475505",
     "exception": false,
     "start_time": "2022-04-29T00:20:42.363521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dummy Variables\n",
    "\n",
    "We are working with dataset that all the features are cactegorical. They will be put into dummy variables before developing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b41f758",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:42.702893Z",
     "iopub.status.busy": "2022-04-29T00:20:42.702332Z",
     "iopub.status.idle": "2022-04-29T00:20:42.953651Z",
     "shell.execute_reply": "2022-04-29T00:20:42.952234Z"
    },
    "papermill": {
     "duration": 0.36643,
     "end_time": "2022-04-29T00:20:42.955888",
     "exception": false,
     "start_time": "2022-04-29T00:20:42.589458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 216 entries, 0 to 215\n",
      "Data columns (total 16 columns):\n",
      " #   Column                   Non-Null Count  Dtype   \n",
      "---  ------                   --------------  -----   \n",
      " 0   Teacher                  216 non-null    category\n",
      " 1   Gender                   216 non-null    category\n",
      " 2   Ethnic                   216 non-null    category\n",
      " 3   Freeredu                 216 non-null    category\n",
      " 4   Score                    216 non-null    float64 \n",
      " 5   Teacher_Ruger            216 non-null    uint8   \n",
      " 6   Teacher_Smith            216 non-null    uint8   \n",
      " 7   Teacher_Wesson           216 non-null    uint8   \n",
      " 8   Gender_Female            216 non-null    uint8   \n",
      " 9   Gender_Male              216 non-null    uint8   \n",
      " 10  Ethnic_African-American  216 non-null    uint8   \n",
      " 11  Ethnic_Asian             216 non-null    uint8   \n",
      " 12  Ethnic_Caucasian         216 non-null    uint8   \n",
      " 13  Ethnic_Hispanic          216 non-null    uint8   \n",
      " 14  Freeredu_Free lunch      216 non-null    uint8   \n",
      " 15  Freeredu_Paid lunch      216 non-null    uint8   \n",
      "dtypes: category(4), float64(1), uint8(11)\n",
      "memory usage: 7.1 KB\n"
     ]
    }
   ],
   "source": [
    "cat_col_li = ['Teacher', 'Gender', 'Ethnic', 'Freeredu']\n",
    "\n",
    "# to grab the added dummy columns later.\n",
    "original_col_num = len(studentDf.columns.values)\n",
    "\n",
    "# create dummy variables for all flags\n",
    "temp_cat_df = studentDf[cat_col_li]\n",
    "dummy_cat_df = pd.get_dummies(temp_cat_df, columns=cat_col_li)\n",
    "studentDf = pd.concat(([studentDf, dummy_cat_df]), axis=1)\n",
    "\n",
    "changed_col_num = len(studentDf.columns.values)\n",
    "n_added = changed_col_num - original_col_num\n",
    "# grab a list of added column names\n",
    "dummy_flag_col_li = studentDf.columns.values[n_added * -1:]\n",
    "\n",
    "studentDf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d07e198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:43.188573Z",
     "iopub.status.busy": "2022-04-29T00:20:43.187936Z",
     "iopub.status.idle": "2022-04-29T00:20:43.208389Z",
     "shell.execute_reply": "2022-04-29T00:20:43.207303Z"
    },
    "papermill": {
     "duration": 0.138608,
     "end_time": "2022-04-29T00:20:43.210840",
     "exception": false,
     "start_time": "2022-04-29T00:20:43.072232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teacher</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnic</th>\n",
       "      <th>Freeredu</th>\n",
       "      <th>Score</th>\n",
       "      <th>Teacher_Ruger</th>\n",
       "      <th>Teacher_Smith</th>\n",
       "      <th>Teacher_Wesson</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Ethnic_African-American</th>\n",
       "      <th>Ethnic_Asian</th>\n",
       "      <th>Ethnic_Caucasian</th>\n",
       "      <th>Ethnic_Hispanic</th>\n",
       "      <th>Freeredu_Free lunch</th>\n",
       "      <th>Freeredu_Paid lunch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ruger</td>\n",
       "      <td>Female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Paid lunch</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ruger</td>\n",
       "      <td>Male</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>Free lunch</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Teacher  Gender            Ethnic    Freeredu  Score  Teacher_Ruger  \\\n",
       "0   Ruger  Female             Asian  Free lunch   76.0              1   \n",
       "1   Ruger  Female          Hispanic  Paid lunch   56.0              1   \n",
       "2   Ruger  Female  African-American  Free lunch   34.0              1   \n",
       "3   Ruger  Female             Asian  Paid lunch   59.0              1   \n",
       "4   Ruger    Male          Hispanic  Free lunch   73.0              1   \n",
       "\n",
       "   Teacher_Smith  Teacher_Wesson  Gender_Female  Gender_Male  \\\n",
       "0              0               0              1            0   \n",
       "1              0               0              1            0   \n",
       "2              0               0              1            0   \n",
       "3              0               0              1            0   \n",
       "4              0               0              0            1   \n",
       "\n",
       "   Ethnic_African-American  Ethnic_Asian  Ethnic_Caucasian  Ethnic_Hispanic  \\\n",
       "0                        0             1                 0                0   \n",
       "1                        0             0                 0                1   \n",
       "2                        1             0                 0                0   \n",
       "3                        0             1                 0                0   \n",
       "4                        0             0                 0                1   \n",
       "\n",
       "   Freeredu_Free lunch  Freeredu_Paid lunch  \n",
       "0                    1                    0  \n",
       "1                    0                    1  \n",
       "2                    1                    0  \n",
       "3                    0                    1  \n",
       "4                    1                    0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "studentDf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f47ce3a",
   "metadata": {
    "papermill": {
     "duration": 0.118231,
     "end_time": "2022-04-29T00:20:43.440654",
     "exception": false,
     "start_time": "2022-04-29T00:20:43.322423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Notice newly created columns that contain binary numbers to indicate the student's traits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7f027",
   "metadata": {
    "papermill": {
     "duration": 0.110648,
     "end_time": "2022-04-29T00:20:43.664061",
     "exception": false,
     "start_time": "2022-04-29T00:20:43.553413",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Development and Tuning\n",
    "Models will predict students' math grade group based on their genders, socioeconomic status, ethnicities and teachers. They will be used to predict the ideal class (teacher) a student should attend to get the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8ea1f0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:43.882159Z",
     "iopub.status.busy": "2022-04-29T00:20:43.881865Z",
     "iopub.status.idle": "2022-04-29T00:20:43.886737Z",
     "shell.execute_reply": "2022-04-29T00:20:43.886108Z"
    },
    "papermill": {
     "duration": 0.118752,
     "end_time": "2022-04-29T00:20:43.889096",
     "exception": false,
     "start_time": "2022-04-29T00:20:43.770344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "featureNLi = [\n",
    "'Teacher_Ruger'\n",
    ", 'Teacher_Smith'\n",
    ", 'Teacher_Wesson'\n",
    ", 'Gender_Female'\n",
    ", 'Gender_Male'\n",
    ", 'Ethnic_African-American'\n",
    ", 'Ethnic_Asian'\n",
    ", 'Ethnic_Caucasian'\n",
    ", 'Ethnic_Hispanic'\n",
    ", 'Freeredu_Free lunch'\n",
    ", 'Freeredu_Paid lunch'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb123b0c",
   "metadata": {
    "papermill": {
     "duration": 0.115497,
     "end_time": "2022-04-29T00:20:44.123644",
     "exception": false,
     "start_time": "2022-04-29T00:20:44.008147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Chi-Square Scores for the Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "139c93d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:44.342855Z",
     "iopub.status.busy": "2022-04-29T00:20:44.341870Z",
     "iopub.status.idle": "2022-04-29T00:20:44.348085Z",
     "shell.execute_reply": "2022-04-29T00:20:44.347109Z"
    },
    "papermill": {
     "duration": 0.117569,
     "end_time": "2022-04-29T00:20:44.350168",
     "exception": false,
     "start_time": "2022-04-29T00:20:44.232599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = studentDf[featureNLi]\n",
    "y = studentDf['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "805befd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:44.567184Z",
     "iopub.status.busy": "2022-04-29T00:20:44.566676Z",
     "iopub.status.idle": "2022-04-29T00:20:44.600863Z",
     "shell.execute_reply": "2022-04-29T00:20:44.600257Z"
    },
    "papermill": {
     "duration": 0.14306,
     "end_time": "2022-04-29T00:20:44.603083",
     "exception": false,
     "start_time": "2022-04-29T00:20:44.460023",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    feature  chi-square score\n",
      "4               Gender_Male         25.587857\n",
      "10      Freeredu_Paid lunch         28.723724\n",
      "9       Freeredu_Free lunch         30.933242\n",
      "3             Gender_Female         31.984821\n",
      "1             Teacher_Smith         38.925466\n",
      "8           Ethnic_Hispanic         42.144156\n",
      "5   Ethnic_African-American         42.811538\n",
      "6              Ethnic_Asian         43.705121\n",
      "2            Teacher_Wesson         46.457519\n",
      "7          Ethnic_Caucasian         48.988571\n",
      "0             Teacher_Ruger         62.500604\n"
     ]
    }
   ],
   "source": [
    "test = SelectKBest(score_func=chi2, k=\"all\")\n",
    "\n",
    "# Use scaled data to fit KBest\n",
    "chiScores = test.fit(X, y)  # Summarize scores\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Create a sorted list of the top features.\n",
    "dfFeatures = pd.DataFrame()\n",
    "for i in range(0, len(chiScores.scores_)):\n",
    "    headers = list(X.keys())\n",
    "    featureObject = {\"feature\": headers[i], \"chi-square score\": chiScores.scores_[i]}\n",
    "    dfFeatures = dfFeatures.append(featureObject, ignore_index=True)\n",
    "\n",
    "dfFeatures = dfFeatures.sort_values(by=['chi-square score'])\n",
    "print(dfFeatures.tail(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d203c86b",
   "metadata": {
    "papermill": {
     "duration": 0.105877,
     "end_time": "2022-04-29T00:20:44.814189",
     "exception": false,
     "start_time": "2022-04-29T00:20:44.708312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Generally with the Chi-square scores, we select features that have scores equal to or higher than 3.8. All predictor variables of our model seem to have good scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30146d08",
   "metadata": {
    "papermill": {
     "duration": 0.105536,
     "end_time": "2022-04-29T00:20:45.026172",
     "exception": false,
     "start_time": "2022-04-29T00:20:44.920636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4c00027",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:45.240813Z",
     "iopub.status.busy": "2022-04-29T00:20:45.240313Z",
     "iopub.status.idle": "2022-04-29T00:20:45.270400Z",
     "shell.execute_reply": "2022-04-29T00:20:45.269382Z"
    },
    "papermill": {
     "duration": 0.140395,
     "end_time": "2022-04-29T00:20:45.273224",
     "exception": false,
     "start_time": "2022-04-29T00:20:45.132829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Score   R-squared:                       0.223\n",
      "Model:                            OLS   Adj. R-squared:                  0.190\n",
      "Method:                 Least Squares   F-statistic:                     6.740\n",
      "Date:                Fri, 29 Apr 2022   Prob (F-statistic):           5.01e-07\n",
      "Time:                        00:20:45   Log-Likelihood:                -707.21\n",
      "No. Observations:                 172   AIC:                             1430.\n",
      "Df Residuals:                     164   BIC:                             1456.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "const                      25.4082      0.458     55.460      0.000      24.504      26.313\n",
      "Teacher_Ruger              -2.5885      1.703     -1.520      0.130      -5.951       0.774\n",
      "Teacher_Smith              14.4101      1.698      8.486      0.000      11.057      17.763\n",
      "Teacher_Wesson             13.5866      1.623      8.369      0.000      10.381      16.792\n",
      "Gender_Female              14.7073      1.222     12.036      0.000      12.295      17.120\n",
      "Gender_Male                10.7009      1.172      9.132      0.000       8.387      13.015\n",
      "Ethnic_African-American     8.1378      2.068      3.935      0.000       4.055      12.221\n",
      "Ethnic_Asian                7.0502      2.026      3.479      0.001       3.049      11.051\n",
      "Ethnic_Caucasian            4.2745      2.212      1.932      0.055      -0.093       8.642\n",
      "Ethnic_Hispanic             5.9457      1.881      3.160      0.002       2.231       9.660\n",
      "Freeredu_Free lunch        13.3193      1.228     10.843      0.000      10.894      15.745\n",
      "Freeredu_Paid lunch        12.0889      1.187     10.182      0.000       9.745      14.433\n",
      "==============================================================================\n",
      "Omnibus:                      119.892   Durbin-Watson:                   1.931\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               11.831\n",
      "Skew:                          -0.049   Prob(JB):                      0.00270\n",
      "Kurtosis:                       1.719   Cond. No.                     3.59e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 3.47e-31. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "Root Mean Squared Error: 13.660711048897134\n"
     ]
    }
   ],
   "source": [
    "X = studentDf[featureNLi]\n",
    "\n",
    "# Adding an intercept\n",
    "X = sm.add_constant(X)\n",
    "y = studentDf['Score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "lrModel = sm.OLS(y_train, X_train).fit()\n",
    "predictions = lrModel.predict(X_test) # make the predictions by the model\n",
    "print(lrModel.summary())\n",
    "print('Root Mean Squared Error:',\n",
    "      np.sqrt(metrics.mean_squared_error(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18286eda",
   "metadata": {
    "papermill": {
     "duration": 0.105547,
     "end_time": "2022-04-29T00:20:45.484964",
     "exception": false,
     "start_time": "2022-04-29T00:20:45.379417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As advised by the chi-square score, all features were used to build a basic linear regression model. Teacher_Ruger has a p-value as high as 0.441, which makes it statistically insignificant. Moreover, it is the only coefficient that is negatively correlated with the target variable. We are certain Ms.Ruger is negatively impacting studuents' performances.\n",
    "\n",
    "Teacher_Ruger is removed from the feature list as well as \"Freeredu_Free lunch\" and \"Freeredu_Paid lunch\" as those two generated low Chi-Square scores compared to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39d33996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:45.699208Z",
     "iopub.status.busy": "2022-04-29T00:20:45.698258Z",
     "iopub.status.idle": "2022-04-29T00:20:45.702558Z",
     "shell.execute_reply": "2022-04-29T00:20:45.701978Z"
    },
    "papermill": {
     "duration": 0.114002,
     "end_time": "2022-04-29T00:20:45.704668",
     "exception": false,
     "start_time": "2022-04-29T00:20:45.590666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "featureNLi = [\n",
    "'Teacher_Smith'\n",
    ", 'Teacher_Wesson'\n",
    ", 'Gender_Female'\n",
    ", 'Gender_Male'\n",
    ", 'Ethnic_African-American'\n",
    ", 'Ethnic_Asian'\n",
    ", 'Ethnic_Caucasian'\n",
    ", 'Ethnic_Hispanic'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eeee7d26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:45.920342Z",
     "iopub.status.busy": "2022-04-29T00:20:45.919698Z",
     "iopub.status.idle": "2022-04-29T00:20:45.941551Z",
     "shell.execute_reply": "2022-04-29T00:20:45.940939Z"
    },
    "papermill": {
     "duration": 0.131499,
     "end_time": "2022-04-29T00:20:45.944040",
     "exception": false,
     "start_time": "2022-04-29T00:20:45.812541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Score   R-squared:                       0.193\n",
      "Model:                            OLS   Adj. R-squared:                  0.163\n",
      "Method:                 Least Squares   F-statistic:                     6.562\n",
      "Date:                Fri, 29 Apr 2022   Prob (F-statistic):           3.11e-06\n",
      "Time:                        00:20:45   Log-Likelihood:                -704.66\n",
      "No. Observations:                 172   AIC:                             1423.\n",
      "Df Residuals:                     165   BIC:                             1445.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "const                      31.5735      1.144     27.604      0.000      29.315      33.832\n",
      "Teacher_Smith              14.2755      2.756      5.181      0.000       8.835      19.716\n",
      "Teacher_Wesson             14.8969      2.843      5.240      0.000       9.284      20.510\n",
      "Gender_Female              17.5424      1.310     13.389      0.000      14.955      20.129\n",
      "Gender_Male                14.0312      1.262     11.117      0.000      11.539      16.523\n",
      "Ethnic_African-American     8.5396      2.020      4.227      0.000       4.551      12.528\n",
      "Ethnic_Asian                7.2695      2.012      3.614      0.000       3.298      11.241\n",
      "Ethnic_Caucasian            6.5761      2.234      2.943      0.004       2.165      10.988\n",
      "Ethnic_Hispanic             9.1883      1.824      5.037      0.000       5.586      12.790\n",
      "==============================================================================\n",
      "Omnibus:                       67.813   Durbin-Watson:                   2.024\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               10.052\n",
      "Skew:                          -0.034   Prob(JB):                      0.00656\n",
      "Kurtosis:                       1.818   Cond. No.                     1.46e+16\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.6e-30. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "X = studentDf[featureNLi]\n",
    "\n",
    "# Adding an intercept\n",
    "X = sm.add_constant(X)\n",
    "y = studentDf['Score']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "predictions = model.predict(X_test) # make the predictions by the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a466eb5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:46.161443Z",
     "iopub.status.busy": "2022-04-29T00:20:46.160907Z",
     "iopub.status.idle": "2022-04-29T00:20:46.166928Z",
     "shell.execute_reply": "2022-04-29T00:20:46.165649Z"
    },
    "papermill": {
     "duration": 0.116323,
     "end_time": "2022-04-29T00:20:46.168788",
     "exception": false,
     "start_time": "2022-04-29T00:20:46.052465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 14.614322158075678\n"
     ]
    }
   ],
   "source": [
    "lrRMSE = np.sqrt(metrics.mean_squared_error(y_test, predictions))\n",
    "print('Root Mean Squared Error:', lrRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0bbc5",
   "metadata": {
    "papermill": {
     "duration": 0.11059,
     "end_time": "2022-04-29T00:20:46.386459",
     "exception": false,
     "start_time": "2022-04-29T00:20:46.275869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The RMSE may be a bit higher; however, all feature variables are statistically significant now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120dcc08",
   "metadata": {
    "papermill": {
     "duration": 0.106748,
     "end_time": "2022-04-29T00:20:46.600388",
     "exception": false,
     "start_time": "2022-04-29T00:20:46.493640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8808c4a2",
   "metadata": {
    "papermill": {
     "duration": 0.106483,
     "end_time": "2022-04-29T00:20:46.813524",
     "exception": false,
     "start_time": "2022-04-29T00:20:46.707041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Optimized Parameters\n",
    "Below are the optimized parameters found after running multiple grid searches and calibrations.\n",
    "\n",
    "| Parameter | Value |\n",
    "| --- | --- |\n",
    "| Batch size | 30 |\n",
    "| epochs | 750 |\n",
    "| Optimizer | Adamax |\n",
    "| Learning Rate | 0.001 |\n",
    "| Kernel Initializer | uniform |\n",
    "| Number of neurons | 10 |\n",
    "| Number of hidden layers | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ce9097",
   "metadata": {
    "papermill": {
     "duration": 0.107218,
     "end_time": "2022-04-29T00:20:47.028902",
     "exception": false,
     "start_time": "2022-04-29T00:20:46.921684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d1dc8d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:47.245449Z",
     "iopub.status.busy": "2022-04-29T00:20:47.245139Z",
     "iopub.status.idle": "2022-04-29T00:20:47.251444Z",
     "shell.execute_reply": "2022-04-29T00:20:47.250605Z"
    },
    "papermill": {
     "duration": 0.117177,
     "end_time": "2022-04-29T00:20:47.253402",
     "exception": false,
     "start_time": "2022-04-29T00:20:47.136225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = studentDf[featureNLi].values\n",
    "y = studentDf['Score'].values\n",
    "\n",
    "ROW_DIM = 0\n",
    "COL_DIM = 1\n",
    "\n",
    "x_arrayReshaped = X.reshape(X.shape[ROW_DIM], X.shape[COL_DIM])\n",
    "y_arrayReshaped = y.reshape(y.shape[ROW_DIM],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8aa9919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:47.468407Z",
     "iopub.status.busy": "2022-04-29T00:20:47.468100Z",
     "iopub.status.idle": "2022-04-29T00:20:47.473523Z",
     "shell.execute_reply": "2022-04-29T00:20:47.472672Z"
    },
    "papermill": {
     "duration": 0.11532,
     "end_time": "2022-04-29T00:20:47.475430",
     "exception": false,
     "start_time": "2022-04-29T00:20:47.360110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_arrayReshaped,\n",
    "         y_arrayReshaped, test_size=0.2)\n",
    "\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8f90adc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:47.690947Z",
     "iopub.status.busy": "2022-04-29T00:20:47.690363Z",
     "iopub.status.idle": "2022-04-29T00:20:47.695796Z",
     "shell.execute_reply": "2022-04-29T00:20:47.695123Z"
    },
    "papermill": {
     "duration": 0.115915,
     "end_time": "2022-04-29T00:20:47.697808",
     "exception": false,
     "start_time": "2022-04-29T00:20:47.581893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "   model = Sequential()\n",
    "   model.add(Dense(10, input_dim=n_features, kernel_initializer='uniform',\n",
    "             activation=\"softplus\"))\n",
    "\n",
    "   model.add(Dense(1, kernel_initializer='uniform'))\n",
    "\n",
    "   opt = Adamax(lr=0.001)\n",
    "   model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5bac272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:20:47.915563Z",
     "iopub.status.busy": "2022-04-29T00:20:47.915059Z",
     "iopub.status.idle": "2022-04-29T00:21:24.833071Z",
     "shell.execute_reply": "2022-04-29T00:21:24.831860Z"
    },
    "papermill": {
     "duration": 37.029745,
     "end_time": "2022-04-29T00:21:24.835884",
     "exception": false,
     "start_time": "2022-04-29T00:20:47.806139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 00:20:47.952333: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
      "2022-04-29 00:20:48.119073: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/750\n",
      "6/6 [==============================] - 1s 36ms/step - loss: 4427.4790 - val_loss: 4852.3926\n",
      "Epoch 2/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4421.3330 - val_loss: 4845.9849\n",
      "Epoch 3/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4415.1611 - val_loss: 4839.4155\n",
      "Epoch 4/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4408.8501 - val_loss: 4832.8911\n",
      "Epoch 5/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4402.5576 - val_loss: 4826.3335\n",
      "Epoch 6/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4396.3208 - val_loss: 4819.5879\n",
      "Epoch 7/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4389.7578 - val_loss: 4812.7734\n",
      "Epoch 8/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4383.3120 - val_loss: 4805.9531\n",
      "Epoch 9/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4376.6694 - val_loss: 4798.9565\n",
      "Epoch 10/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4369.8628 - val_loss: 4791.8330\n",
      "Epoch 11/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4363.0615 - val_loss: 4784.5161\n",
      "Epoch 12/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4355.9102 - val_loss: 4777.0752\n",
      "Epoch 13/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4348.7236 - val_loss: 4769.4912\n",
      "Epoch 14/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4341.4336 - val_loss: 4761.6621\n",
      "Epoch 15/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4333.8037 - val_loss: 4753.7749\n",
      "Epoch 16/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4326.2471 - val_loss: 4745.7803\n",
      "Epoch 17/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4318.5151 - val_loss: 4737.6836\n",
      "Epoch 18/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4310.6553 - val_loss: 4729.5234\n",
      "Epoch 19/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4302.8530 - val_loss: 4721.1406\n",
      "Epoch 20/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4294.6831 - val_loss: 4712.6060\n",
      "Epoch 21/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4286.5459 - val_loss: 4703.9922\n",
      "Epoch 22/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4278.1187 - val_loss: 4695.1582\n",
      "Epoch 23/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4269.6963 - val_loss: 4686.2778\n",
      "Epoch 24/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4261.0874 - val_loss: 4677.2256\n",
      "Epoch 25/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4252.2695 - val_loss: 4668.0420\n",
      "Epoch 26/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4243.3398 - val_loss: 4658.6948\n",
      "Epoch 27/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4234.3442 - val_loss: 4649.0903\n",
      "Epoch 28/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4224.8652 - val_loss: 4639.4424\n",
      "Epoch 29/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4215.7319 - val_loss: 4629.8345\n",
      "Epoch 30/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4206.4111 - val_loss: 4619.9966\n",
      "Epoch 31/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4196.9028 - val_loss: 4609.9536\n",
      "Epoch 32/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4187.1753 - val_loss: 4599.7607\n",
      "Epoch 33/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4177.3652 - val_loss: 4589.4575\n",
      "Epoch 34/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4167.3457 - val_loss: 4578.9312\n",
      "Epoch 35/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4157.1426 - val_loss: 4568.2466\n",
      "Epoch 36/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4146.7153 - val_loss: 4557.3013\n",
      "Epoch 37/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4136.3120 - val_loss: 4546.6392\n",
      "Epoch 38/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4126.0020 - val_loss: 4535.7925\n",
      "Epoch 39/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4115.5376 - val_loss: 4524.6948\n",
      "Epoch 40/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4104.7139 - val_loss: 4513.3730\n",
      "Epoch 41/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4093.7524 - val_loss: 4501.8872\n",
      "Epoch 42/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4082.4954 - val_loss: 4490.2964\n",
      "Epoch 43/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4071.4717 - val_loss: 4478.4683\n",
      "Epoch 44/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4059.9509 - val_loss: 4466.4995\n",
      "Epoch 45/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4048.2908 - val_loss: 4454.3315\n",
      "Epoch 46/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4036.7666 - val_loss: 4442.1670\n",
      "Epoch 47/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4024.9451 - val_loss: 4429.7261\n",
      "Epoch 48/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4012.9404 - val_loss: 4417.1440\n",
      "Epoch 49/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4000.6958 - val_loss: 4404.2275\n",
      "Epoch 50/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3988.2039 - val_loss: 4391.0337\n",
      "Epoch 51/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3975.3794 - val_loss: 4377.6753\n",
      "Epoch 52/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3962.6438 - val_loss: 4364.2456\n",
      "Epoch 53/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3949.5864 - val_loss: 4350.7715\n",
      "Epoch 54/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3936.5139 - val_loss: 4337.0864\n",
      "Epoch 55/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3923.3491 - val_loss: 4323.2090\n",
      "Epoch 56/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3909.8801 - val_loss: 4309.2671\n",
      "Epoch 57/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3896.3525 - val_loss: 4295.0518\n",
      "Epoch 58/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3882.8521 - val_loss: 4280.6245\n",
      "Epoch 59/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3868.7212 - val_loss: 4266.1636\n",
      "Epoch 60/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3854.7729 - val_loss: 4251.2646\n",
      "Epoch 61/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3840.3035 - val_loss: 4236.1035\n",
      "Epoch 62/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3825.5095 - val_loss: 4220.7217\n",
      "Epoch 63/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3810.8774 - val_loss: 4205.5015\n",
      "Epoch 64/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3796.0120 - val_loss: 4190.3223\n",
      "Epoch 65/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3781.6572 - val_loss: 4175.0918\n",
      "Epoch 66/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3766.8164 - val_loss: 4159.8833\n",
      "Epoch 67/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3752.3555 - val_loss: 4144.7544\n",
      "Epoch 68/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3737.8066 - val_loss: 4129.3218\n",
      "Epoch 69/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3722.8953 - val_loss: 4113.6030\n",
      "Epoch 70/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3707.6650 - val_loss: 4097.6621\n",
      "Epoch 71/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3692.1719 - val_loss: 4081.5671\n",
      "Epoch 72/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3676.8274 - val_loss: 4065.3181\n",
      "Epoch 73/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3661.1133 - val_loss: 4048.7664\n",
      "Epoch 74/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3645.0083 - val_loss: 4032.0544\n",
      "Epoch 75/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3629.1338 - val_loss: 4015.3853\n",
      "Epoch 76/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3613.1147 - val_loss: 3998.9780\n",
      "Epoch 77/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3597.4521 - val_loss: 3982.5789\n",
      "Epoch 78/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3581.4800 - val_loss: 3965.8513\n",
      "Epoch 79/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3565.2312 - val_loss: 3948.8369\n",
      "Epoch 80/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3548.9553 - val_loss: 3931.6375\n",
      "Epoch 81/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3532.2314 - val_loss: 3914.2766\n",
      "Epoch 82/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3515.5342 - val_loss: 3896.4744\n",
      "Epoch 83/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3498.5708 - val_loss: 3878.3210\n",
      "Epoch 84/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3480.9004 - val_loss: 3859.9705\n",
      "Epoch 85/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3463.0857 - val_loss: 3841.5312\n",
      "Epoch 86/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3445.6443 - val_loss: 3823.5679\n",
      "Epoch 87/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3428.5239 - val_loss: 3805.2205\n",
      "Epoch 88/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3410.7766 - val_loss: 3786.6494\n",
      "Epoch 89/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3392.7583 - val_loss: 3767.8018\n",
      "Epoch 90/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3374.5574 - val_loss: 3748.5845\n",
      "Epoch 91/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3356.2239 - val_loss: 3729.1499\n",
      "Epoch 92/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3337.4851 - val_loss: 3709.6599\n",
      "Epoch 93/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3318.7595 - val_loss: 3689.8787\n",
      "Epoch 94/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3299.8655 - val_loss: 3669.9971\n",
      "Epoch 95/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3280.6770 - val_loss: 3650.3828\n",
      "Epoch 96/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3262.1606 - val_loss: 3630.5974\n",
      "Epoch 97/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3242.9673 - val_loss: 3610.6265\n",
      "Epoch 98/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3223.7688 - val_loss: 3590.5334\n",
      "Epoch 99/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3204.4702 - val_loss: 3570.2671\n",
      "Epoch 100/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3185.0610 - val_loss: 3549.9561\n",
      "Epoch 101/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3165.4026 - val_loss: 3529.7046\n",
      "Epoch 102/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3146.3953 - val_loss: 3510.0000\n",
      "Epoch 103/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3127.5112 - val_loss: 3489.8743\n",
      "Epoch 104/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3108.1169 - val_loss: 3469.4094\n",
      "Epoch 105/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3088.0864 - val_loss: 3448.8494\n",
      "Epoch 106/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3068.5195 - val_loss: 3428.1038\n",
      "Epoch 107/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3048.7915 - val_loss: 3406.9197\n",
      "Epoch 108/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3028.1501 - val_loss: 3385.5947\n",
      "Epoch 109/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3008.1902 - val_loss: 3364.1956\n",
      "Epoch 110/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2987.5203 - val_loss: 3342.6108\n",
      "Epoch 111/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2966.6326 - val_loss: 3320.9026\n",
      "Epoch 112/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2945.7822 - val_loss: 3298.8459\n",
      "Epoch 113/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2924.8735 - val_loss: 3276.4009\n",
      "Epoch 114/750\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 2903.1279 - val_loss: 3254.0381\n",
      "Epoch 115/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2882.1638 - val_loss: 3232.4062\n",
      "Epoch 116/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2861.5527 - val_loss: 3210.3928\n",
      "Epoch 117/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2840.5222 - val_loss: 3188.3704\n",
      "Epoch 118/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2819.5142 - val_loss: 3166.0618\n",
      "Epoch 119/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2798.0198 - val_loss: 3143.5623\n",
      "Epoch 120/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2776.5708 - val_loss: 3120.6641\n",
      "Epoch 121/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2754.8010 - val_loss: 3097.4680\n",
      "Epoch 122/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2732.6904 - val_loss: 3074.1389\n",
      "Epoch 123/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2710.3845 - val_loss: 3050.5781\n",
      "Epoch 124/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2687.9607 - val_loss: 3026.9585\n",
      "Epoch 125/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2665.7573 - val_loss: 3003.5217\n",
      "Epoch 126/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 2643.2610 - val_loss: 2980.4141\n",
      "Epoch 127/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2621.6948 - val_loss: 2957.1292\n",
      "Epoch 128/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2599.4294 - val_loss: 2933.7417\n",
      "Epoch 129/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2577.2681 - val_loss: 2910.1741\n",
      "Epoch 130/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2554.8447 - val_loss: 2886.3557\n",
      "Epoch 131/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2532.3804 - val_loss: 2863.0613\n",
      "Epoch 132/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2510.3267 - val_loss: 2839.3826\n",
      "Epoch 133/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2487.6311 - val_loss: 2815.6150\n",
      "Epoch 134/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2465.3025 - val_loss: 2791.9363\n",
      "Epoch 135/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2442.6477 - val_loss: 2768.1270\n",
      "Epoch 136/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2420.0342 - val_loss: 2743.9958\n",
      "Epoch 137/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2397.1980 - val_loss: 2719.5925\n",
      "Epoch 138/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2373.6836 - val_loss: 2695.2178\n",
      "Epoch 139/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2350.7000 - val_loss: 2670.8706\n",
      "Epoch 140/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2328.1724 - val_loss: 2646.7583\n",
      "Epoch 141/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2305.5400 - val_loss: 2623.2805\n",
      "Epoch 142/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2283.2446 - val_loss: 2599.7766\n",
      "Epoch 143/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2260.8750 - val_loss: 2576.2129\n",
      "Epoch 144/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2238.5754 - val_loss: 2552.4209\n",
      "Epoch 145/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2216.5181 - val_loss: 2528.7937\n",
      "Epoch 146/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2194.0493 - val_loss: 2505.0945\n",
      "Epoch 147/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2171.7000 - val_loss: 2481.2878\n",
      "Epoch 148/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2149.1638 - val_loss: 2457.3999\n",
      "Epoch 149/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2126.8491 - val_loss: 2433.0994\n",
      "Epoch 150/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2103.6143 - val_loss: 2408.9443\n",
      "Epoch 151/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2080.9807 - val_loss: 2384.3855\n",
      "Epoch 152/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2057.7578 - val_loss: 2359.8572\n",
      "Epoch 153/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2034.9635 - val_loss: 2335.1272\n",
      "Epoch 154/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2011.6971 - val_loss: 2310.4194\n",
      "Epoch 155/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1988.6952 - val_loss: 2285.5837\n",
      "Epoch 156/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1965.4777 - val_loss: 2261.5657\n",
      "Epoch 157/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1942.7482 - val_loss: 2237.7109\n",
      "Epoch 158/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1920.6027 - val_loss: 2213.5498\n",
      "Epoch 159/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1897.8405 - val_loss: 2189.3176\n",
      "Epoch 160/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1875.3654 - val_loss: 2164.8362\n",
      "Epoch 161/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1852.3363 - val_loss: 2140.3057\n",
      "Epoch 162/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1829.1969 - val_loss: 2115.7593\n",
      "Epoch 163/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1806.4189 - val_loss: 2092.0752\n",
      "Epoch 164/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1785.4170 - val_loss: 2069.9954\n",
      "Epoch 165/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1764.6781 - val_loss: 2048.0574\n",
      "Epoch 166/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1744.7574 - val_loss: 2025.6890\n",
      "Epoch 167/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1723.3311 - val_loss: 2003.7523\n",
      "Epoch 168/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1703.0465 - val_loss: 1981.4191\n",
      "Epoch 169/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1682.2291 - val_loss: 1959.0156\n",
      "Epoch 170/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1661.4025 - val_loss: 1936.5637\n",
      "Epoch 171/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1640.4380 - val_loss: 1914.0984\n",
      "Epoch 172/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1619.9707 - val_loss: 1891.2882\n",
      "Epoch 173/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1598.8712 - val_loss: 1868.5737\n",
      "Epoch 174/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1577.7426 - val_loss: 1846.0188\n",
      "Epoch 175/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1556.8857 - val_loss: 1823.3927\n",
      "Epoch 176/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1535.8126 - val_loss: 1800.8032\n",
      "Epoch 177/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1515.1086 - val_loss: 1778.0046\n",
      "Epoch 178/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1494.0416 - val_loss: 1755.3036\n",
      "Epoch 179/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1473.0624 - val_loss: 1732.6655\n",
      "Epoch 180/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1452.2683 - val_loss: 1710.0057\n",
      "Epoch 181/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1432.0472 - val_loss: 1687.1403\n",
      "Epoch 182/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1410.9308 - val_loss: 1664.6785\n",
      "Epoch 183/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1390.0168 - val_loss: 1642.4790\n",
      "Epoch 184/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1369.7350 - val_loss: 1620.3367\n",
      "Epoch 185/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1349.4808 - val_loss: 1598.1544\n",
      "Epoch 186/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1329.0487 - val_loss: 1576.2983\n",
      "Epoch 187/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1309.4109 - val_loss: 1554.4755\n",
      "Epoch 188/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1289.3386 - val_loss: 1532.8348\n",
      "Epoch 189/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1269.5737 - val_loss: 1511.2792\n",
      "Epoch 190/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1249.8887 - val_loss: 1489.7605\n",
      "Epoch 191/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1230.4259 - val_loss: 1468.1444\n",
      "Epoch 192/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1210.4348 - val_loss: 1446.8218\n",
      "Epoch 193/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1191.5376 - val_loss: 1425.4500\n",
      "Epoch 194/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1172.0574 - val_loss: 1404.3546\n",
      "Epoch 195/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1152.9650 - val_loss: 1383.2924\n",
      "Epoch 196/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1134.1370 - val_loss: 1362.1704\n",
      "Epoch 197/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1115.0811 - val_loss: 1341.2745\n",
      "Epoch 198/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1096.4060 - val_loss: 1320.4000\n",
      "Epoch 199/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1077.7111 - val_loss: 1299.6329\n",
      "Epoch 200/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1059.3160 - val_loss: 1278.8882\n",
      "Epoch 201/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1040.7396 - val_loss: 1258.4115\n",
      "Epoch 202/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1021.9846 - val_loss: 1238.3315\n",
      "Epoch 203/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1004.5416 - val_loss: 1218.1532\n",
      "Epoch 204/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 986.2656 - val_loss: 1198.4213\n",
      "Epoch 205/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 968.9164 - val_loss: 1178.6229\n",
      "Epoch 206/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 951.3040 - val_loss: 1158.9854\n",
      "Epoch 207/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 934.0671 - val_loss: 1139.4214\n",
      "Epoch 208/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 916.6773 - val_loss: 1120.1714\n",
      "Epoch 209/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 899.4525 - val_loss: 1101.1427\n",
      "Epoch 210/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 882.6423 - val_loss: 1082.2039\n",
      "Epoch 211/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 866.2699 - val_loss: 1063.2351\n",
      "Epoch 212/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 849.4037 - val_loss: 1044.6826\n",
      "Epoch 213/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 833.2511 - val_loss: 1026.1405\n",
      "Epoch 214/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 816.9327 - val_loss: 1007.9365\n",
      "Epoch 215/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 801.0904 - val_loss: 989.8104\n",
      "Epoch 216/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 785.4738 - val_loss: 971.9361\n",
      "Epoch 217/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 769.9334 - val_loss: 954.2773\n",
      "Epoch 218/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 754.8914 - val_loss: 936.6307\n",
      "Epoch 219/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 739.4139 - val_loss: 919.4183\n",
      "Epoch 220/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 724.7388 - val_loss: 902.2209\n",
      "Epoch 221/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 710.0818 - val_loss: 885.2955\n",
      "Epoch 222/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 695.5618 - val_loss: 868.6763\n",
      "Epoch 223/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 681.5331 - val_loss: 852.2324\n",
      "Epoch 224/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 667.1710 - val_loss: 836.3860\n",
      "Epoch 225/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 653.7505 - val_loss: 820.5520\n",
      "Epoch 226/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 640.7010 - val_loss: 804.7615\n",
      "Epoch 227/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 627.4275 - val_loss: 789.3326\n",
      "Epoch 228/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 614.3549 - val_loss: 774.3354\n",
      "Epoch 229/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 601.9941 - val_loss: 759.4355\n",
      "Epoch 230/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 589.4056 - val_loss: 744.9396\n",
      "Epoch 231/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 577.2048 - val_loss: 730.7045\n",
      "Epoch 232/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 564.9092 - val_loss: 716.9088\n",
      "Epoch 233/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 553.9326 - val_loss: 702.9465\n",
      "Epoch 234/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 542.2487 - val_loss: 689.5054\n",
      "Epoch 235/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 531.3422 - val_loss: 676.1555\n",
      "Epoch 236/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 520.5119 - val_loss: 663.0713\n",
      "Epoch 237/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 509.8729 - val_loss: 650.2974\n",
      "Epoch 238/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 499.5569 - val_loss: 637.7242\n",
      "Epoch 239/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 489.4804 - val_loss: 625.4301\n",
      "Epoch 240/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 479.5855 - val_loss: 613.4460\n",
      "Epoch 241/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 469.8343 - val_loss: 601.8181\n",
      "Epoch 242/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 460.5022 - val_loss: 590.4368\n",
      "Epoch 243/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 451.7334 - val_loss: 578.9819\n",
      "Epoch 244/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 442.4800 - val_loss: 568.0401\n",
      "Epoch 245/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 433.9958 - val_loss: 557.2484\n",
      "Epoch 246/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 425.5663 - val_loss: 546.7559\n",
      "Epoch 247/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 417.3379 - val_loss: 536.4695\n",
      "Epoch 248/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 409.1760 - val_loss: 526.5258\n",
      "Epoch 249/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 401.4734 - val_loss: 516.7578\n",
      "Epoch 250/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 393.8210 - val_loss: 507.2567\n",
      "Epoch 251/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 386.8647 - val_loss: 497.6892\n",
      "Epoch 252/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 379.7825 - val_loss: 488.4286\n",
      "Epoch 253/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 372.6452 - val_loss: 479.6732\n",
      "Epoch 254/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 365.8330 - val_loss: 471.2529\n",
      "Epoch 255/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 359.8391 - val_loss: 462.6998\n",
      "Epoch 256/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 353.4705 - val_loss: 454.5216\n",
      "Epoch 257/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 347.3620 - val_loss: 446.6806\n",
      "Epoch 258/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 341.5563 - val_loss: 439.0839\n",
      "Epoch 259/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 336.0294 - val_loss: 431.6173\n",
      "Epoch 260/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 330.7050 - val_loss: 424.2934\n",
      "Epoch 261/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 325.7466 - val_loss: 417.0424\n",
      "Epoch 262/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 320.4882 - val_loss: 410.2480\n",
      "Epoch 263/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 315.3712 - val_loss: 403.8567\n",
      "Epoch 264/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 310.9952 - val_loss: 397.3313\n",
      "Epoch 265/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 306.4456 - val_loss: 391.0706\n",
      "Epoch 266/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 302.2501 - val_loss: 384.9481\n",
      "Epoch 267/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 297.8235 - val_loss: 379.2527\n",
      "Epoch 268/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 294.0225 - val_loss: 373.5226\n",
      "Epoch 269/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 290.1638 - val_loss: 367.9938\n",
      "Epoch 270/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 286.1938 - val_loss: 362.7940\n",
      "Epoch 271/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 283.1844 - val_loss: 357.2913\n",
      "Epoch 272/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 279.4824 - val_loss: 352.2502\n",
      "Epoch 273/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 276.1085 - val_loss: 347.4302\n",
      "Epoch 274/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 273.1373 - val_loss: 342.6863\n",
      "Epoch 275/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 269.9129 - val_loss: 338.3369\n",
      "Epoch 276/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 267.2613 - val_loss: 333.8813\n",
      "Epoch 277/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 264.3968 - val_loss: 329.6757\n",
      "Epoch 278/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 261.9482 - val_loss: 325.4764\n",
      "Epoch 279/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 259.4505 - val_loss: 321.5335\n",
      "Epoch 280/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 257.0182 - val_loss: 317.8182\n",
      "Epoch 281/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 254.8945 - val_loss: 314.1794\n",
      "Epoch 282/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 252.5250 - val_loss: 310.8886\n",
      "Epoch 283/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 250.5647 - val_loss: 307.5948\n",
      "Epoch 284/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 248.7438 - val_loss: 304.2790\n",
      "Epoch 285/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 246.8653 - val_loss: 301.1765\n",
      "Epoch 286/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 244.9995 - val_loss: 298.2997\n",
      "Epoch 287/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 243.4683 - val_loss: 295.3824\n",
      "Epoch 288/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 241.8457 - val_loss: 292.6110\n",
      "Epoch 289/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 240.3570 - val_loss: 289.9338\n",
      "Epoch 290/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 238.8092 - val_loss: 287.4796\n",
      "Epoch 291/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 237.5372 - val_loss: 285.0068\n",
      "Epoch 292/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 236.2023 - val_loss: 282.6623\n",
      "Epoch 293/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 235.0580 - val_loss: 280.2964\n",
      "Epoch 294/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 233.8454 - val_loss: 278.1171\n",
      "Epoch 295/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232.6782 - val_loss: 276.1154\n",
      "Epoch 296/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 231.6366 - val_loss: 274.1503\n",
      "Epoch 297/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 230.7314 - val_loss: 272.1707\n",
      "Epoch 298/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.8670 - val_loss: 270.2326\n",
      "Epoch 299/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.9096 - val_loss: 268.5053\n",
      "Epoch 300/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.0540 - val_loss: 266.9043\n",
      "Epoch 301/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.3409 - val_loss: 265.2358\n",
      "Epoch 302/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.6453 - val_loss: 263.6085\n",
      "Epoch 303/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 225.8003 - val_loss: 262.1989\n",
      "Epoch 304/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 225.2066 - val_loss: 260.6953\n",
      "Epoch 305/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.5528 - val_loss: 259.3023\n",
      "Epoch 306/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.9633 - val_loss: 257.9566\n",
      "Epoch 307/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4000 - val_loss: 256.6981\n",
      "Epoch 308/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7839 - val_loss: 255.6501\n",
      "Epoch 309/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3924 - val_loss: 254.4010\n",
      "Epoch 310/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9624 - val_loss: 253.1754\n",
      "Epoch 311/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3895 - val_loss: 252.2123\n",
      "Epoch 312/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.0197 - val_loss: 251.1346\n",
      "Epoch 313/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 220.5830 - val_loss: 250.1987\n",
      "Epoch 314/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 220.1989 - val_loss: 249.2019\n",
      "Epoch 315/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 219.9066 - val_loss: 248.1272\n",
      "Epoch 316/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 219.4444 - val_loss: 247.3407\n",
      "Epoch 317/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 219.2036 - val_loss: 246.4755\n",
      "Epoch 318/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 218.8398 - val_loss: 245.7576\n",
      "Epoch 319/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 218.5428 - val_loss: 245.1192\n",
      "Epoch 320/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 218.3311 - val_loss: 244.3079\n",
      "Epoch 321/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 218.0010 - val_loss: 243.6435\n",
      "Epoch 322/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 217.8100 - val_loss: 242.8777\n",
      "Epoch 323/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 217.5310 - val_loss: 242.2389\n",
      "Epoch 324/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 217.2826 - val_loss: 241.6699\n",
      "Epoch 325/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 217.0832 - val_loss: 241.0934\n",
      "Epoch 326/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 216.8949 - val_loss: 240.4444\n",
      "Epoch 327/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 216.6479 - val_loss: 239.9055\n",
      "Epoch 328/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 216.4625 - val_loss: 239.4016\n",
      "Epoch 329/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 216.2799 - val_loss: 238.8914\n",
      "Epoch 330/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 216.1071 - val_loss: 238.4262\n",
      "Epoch 331/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 215.9294 - val_loss: 238.0918\n",
      "Epoch 332/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 215.8467 - val_loss: 237.4861\n",
      "Epoch 333/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 215.6494 - val_loss: 236.9956\n",
      "Epoch 334/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 215.5008 - val_loss: 236.5593\n",
      "Epoch 335/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 215.3286 - val_loss: 236.2918\n",
      "Epoch 336/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 215.2354 - val_loss: 235.9079\n",
      "Epoch 337/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 215.1959 - val_loss: 235.4212\n",
      "Epoch 338/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 214.9594 - val_loss: 235.3996\n",
      "Epoch 339/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 214.8738 - val_loss: 235.0682\n",
      "Epoch 340/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 214.7434 - val_loss: 234.8168\n",
      "Epoch 341/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 214.6319 - val_loss: 234.5801\n",
      "Epoch 342/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 214.5213 - val_loss: 234.4779\n",
      "Epoch 343/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 214.4024 - val_loss: 234.2208\n",
      "Epoch 344/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 214.3295 - val_loss: 233.9308\n",
      "Epoch 345/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 214.2130 - val_loss: 233.7405\n",
      "Epoch 346/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 214.0934 - val_loss: 233.4990\n",
      "Epoch 347/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 214.0121 - val_loss: 233.2602\n",
      "Epoch 348/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 213.9582 - val_loss: 232.9679\n",
      "Epoch 349/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 213.7924 - val_loss: 232.9016\n",
      "Epoch 350/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 213.7339 - val_loss: 232.7180\n",
      "Epoch 351/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 213.6438 - val_loss: 232.6667\n",
      "Epoch 352/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 213.5321 - val_loss: 232.6087\n",
      "Epoch 353/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 213.4565 - val_loss: 232.4966\n",
      "Epoch 354/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 213.4338 - val_loss: 232.1964\n",
      "Epoch 355/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 213.3094 - val_loss: 232.3138\n",
      "Epoch 356/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 213.2080 - val_loss: 232.1651\n",
      "Epoch 357/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 213.1183 - val_loss: 232.0084\n",
      "Epoch 358/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 213.0398 - val_loss: 231.8759\n",
      "Epoch 359/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 212.9529 - val_loss: 231.7766\n",
      "Epoch 360/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 212.8670 - val_loss: 231.6068\n",
      "Epoch 361/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 212.7990 - val_loss: 231.4282\n",
      "Epoch 362/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 212.7406 - val_loss: 231.2546\n",
      "Epoch 363/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 212.6357 - val_loss: 231.2089\n",
      "Epoch 364/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 212.5789 - val_loss: 231.0698\n",
      "Epoch 365/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 212.4896 - val_loss: 231.0394\n",
      "Epoch 366/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 212.4215 - val_loss: 230.9690\n",
      "Epoch 367/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 212.3581 - val_loss: 230.8766\n",
      "Epoch 368/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 212.2925 - val_loss: 230.7773\n",
      "Epoch 369/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 212.2176 - val_loss: 230.8410\n",
      "Epoch 370/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 212.1674 - val_loss: 230.7205\n",
      "Epoch 371/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 212.0908 - val_loss: 230.6645\n",
      "Epoch 372/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 212.0250 - val_loss: 230.7049\n",
      "Epoch 373/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.9714 - val_loss: 230.5732\n",
      "Epoch 374/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 211.8846 - val_loss: 230.4898\n",
      "Epoch 375/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.8273 - val_loss: 230.5625\n",
      "Epoch 376/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.7919 - val_loss: 230.6254\n",
      "Epoch 377/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 211.7109 - val_loss: 230.4474\n",
      "Epoch 378/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.6411 - val_loss: 230.2780\n",
      "Epoch 379/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.5830 - val_loss: 230.1204\n",
      "Epoch 380/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.5400 - val_loss: 230.0448\n",
      "Epoch 381/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.4539 - val_loss: 229.9300\n",
      "Epoch 382/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.4057 - val_loss: 230.0794\n",
      "Epoch 383/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 211.3279 - val_loss: 230.0886\n",
      "Epoch 384/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.2784 - val_loss: 229.9962\n",
      "Epoch 385/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.2131 - val_loss: 229.9162\n",
      "Epoch 386/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.1593 - val_loss: 229.9003\n",
      "Epoch 387/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.1420 - val_loss: 230.0912\n",
      "Epoch 388/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 211.0539 - val_loss: 230.0864\n",
      "Epoch 389/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 210.9950 - val_loss: 229.9950\n",
      "Epoch 390/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.9450 - val_loss: 229.9009\n",
      "Epoch 391/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.8978 - val_loss: 229.8411\n",
      "Epoch 392/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.8663 - val_loss: 229.7717\n",
      "Epoch 393/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.8170 - val_loss: 229.6651\n",
      "Epoch 394/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 210.7484 - val_loss: 229.8087\n",
      "Epoch 395/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.6897 - val_loss: 229.9436\n",
      "Epoch 396/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.6308 - val_loss: 230.0181\n",
      "Epoch 397/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.5842 - val_loss: 230.0134\n",
      "Epoch 398/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.5370 - val_loss: 230.0349\n",
      "Epoch 399/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.4924 - val_loss: 230.1059\n",
      "Epoch 400/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.4200 - val_loss: 230.1197\n",
      "Epoch 401/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.3930 - val_loss: 230.1667\n",
      "Epoch 402/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.3532 - val_loss: 229.9632\n",
      "Epoch 403/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 210.2873 - val_loss: 229.9485\n",
      "Epoch 404/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.2315 - val_loss: 229.9252\n",
      "Epoch 405/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.2107 - val_loss: 230.0380\n",
      "Epoch 406/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.1575 - val_loss: 229.9281\n",
      "Epoch 407/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.1132 - val_loss: 229.9091\n",
      "Epoch 408/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 210.0863 - val_loss: 229.8154\n",
      "Epoch 409/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 210.0238 - val_loss: 229.8048\n",
      "Epoch 410/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.9948 - val_loss: 229.8114\n",
      "Epoch 411/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.9796 - val_loss: 229.9070\n",
      "Epoch 412/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.9623 - val_loss: 229.6850\n",
      "Epoch 413/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.8971 - val_loss: 229.8163\n",
      "Epoch 414/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.8517 - val_loss: 229.7965\n",
      "Epoch 415/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.8285 - val_loss: 229.9693\n",
      "Epoch 416/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.7609 - val_loss: 229.9708\n",
      "Epoch 417/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 209.7243 - val_loss: 229.9768\n",
      "Epoch 418/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.7161 - val_loss: 230.1508\n",
      "Epoch 419/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.6830 - val_loss: 230.2745\n",
      "Epoch 420/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.6267 - val_loss: 230.1127\n",
      "Epoch 421/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.5834 - val_loss: 230.0760\n",
      "Epoch 422/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.5596 - val_loss: 230.0615\n",
      "Epoch 423/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.5284 - val_loss: 230.2474\n",
      "Epoch 424/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.4815 - val_loss: 230.2062\n",
      "Epoch 425/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.4487 - val_loss: 230.1061\n",
      "Epoch 426/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.4036 - val_loss: 230.0940\n",
      "Epoch 427/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.3836 - val_loss: 230.1751\n",
      "Epoch 428/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.3361 - val_loss: 230.1927\n",
      "Epoch 429/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.3232 - val_loss: 230.1483\n",
      "Epoch 430/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.2841 - val_loss: 230.2617\n",
      "Epoch 431/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.2637 - val_loss: 230.3852\n",
      "Epoch 432/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.2206 - val_loss: 230.5011\n",
      "Epoch 433/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.1956 - val_loss: 230.5759\n",
      "Epoch 434/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.1578 - val_loss: 230.5461\n",
      "Epoch 435/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.1364 - val_loss: 230.5565\n",
      "Epoch 436/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.1060 - val_loss: 230.6379\n",
      "Epoch 437/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 209.0959 - val_loss: 230.4774\n",
      "Epoch 438/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.0429 - val_loss: 230.4758\n",
      "Epoch 439/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 209.0233 - val_loss: 230.5259\n",
      "Epoch 440/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.9881 - val_loss: 230.4197\n",
      "Epoch 441/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.9568 - val_loss: 230.5257\n",
      "Epoch 442/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.9304 - val_loss: 230.5091\n",
      "Epoch 443/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.9098 - val_loss: 230.5020\n",
      "Epoch 444/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.8820 - val_loss: 230.5300\n",
      "Epoch 445/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.8590 - val_loss: 230.5575\n",
      "Epoch 446/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.8211 - val_loss: 230.6415\n",
      "Epoch 447/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.7778 - val_loss: 230.7400\n",
      "Epoch 448/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.7666 - val_loss: 230.8639\n",
      "Epoch 449/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.7407 - val_loss: 231.0035\n",
      "Epoch 450/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.7338 - val_loss: 231.0413\n",
      "Epoch 451/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.6967 - val_loss: 231.0536\n",
      "Epoch 452/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.6631 - val_loss: 231.1301\n",
      "Epoch 453/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.6783 - val_loss: 231.0590\n",
      "Epoch 454/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.6571 - val_loss: 231.2906\n",
      "Epoch 455/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.6894 - val_loss: 231.0325\n",
      "Epoch 456/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.5822 - val_loss: 231.1377\n",
      "Epoch 457/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.5563 - val_loss: 231.1865\n",
      "Epoch 458/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.5862 - val_loss: 231.4758\n",
      "Epoch 459/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.5418 - val_loss: 231.4870\n",
      "Epoch 460/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.4990 - val_loss: 231.4030\n",
      "Epoch 461/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.4744 - val_loss: 231.4321\n",
      "Epoch 462/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.4427 - val_loss: 231.3444\n",
      "Epoch 463/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.4348 - val_loss: 231.3184\n",
      "Epoch 464/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.4140 - val_loss: 231.2221\n",
      "Epoch 465/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.4237 - val_loss: 231.3593\n",
      "Epoch 466/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.3786 - val_loss: 231.2729\n",
      "Epoch 467/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.3613 - val_loss: 231.3014\n",
      "Epoch 468/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.3344 - val_loss: 231.3331\n",
      "Epoch 469/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.3388 - val_loss: 231.2811\n",
      "Epoch 470/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.3083 - val_loss: 231.3793\n",
      "Epoch 471/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.2945 - val_loss: 231.5119\n",
      "Epoch 472/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.3006 - val_loss: 231.3157\n",
      "Epoch 473/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.2424 - val_loss: 231.3377\n",
      "Epoch 474/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.2294 - val_loss: 231.3777\n",
      "Epoch 475/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.2187 - val_loss: 231.2985\n",
      "Epoch 476/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.1877 - val_loss: 231.3851\n",
      "Epoch 477/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.1651 - val_loss: 231.5291\n",
      "Epoch 478/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.1594 - val_loss: 231.5358\n",
      "Epoch 479/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.1832 - val_loss: 231.6019\n",
      "Epoch 480/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.1250 - val_loss: 231.5635\n",
      "Epoch 481/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.1300 - val_loss: 231.4698\n",
      "Epoch 482/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.1010 - val_loss: 231.7258\n",
      "Epoch 483/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.0754 - val_loss: 231.8071\n",
      "Epoch 484/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.0561 - val_loss: 231.7798\n",
      "Epoch 485/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.0423 - val_loss: 231.8253\n",
      "Epoch 486/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 208.0188 - val_loss: 231.8857\n",
      "Epoch 487/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.0239 - val_loss: 231.9485\n",
      "Epoch 488/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 208.0327 - val_loss: 231.7871\n",
      "Epoch 489/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.9795 - val_loss: 231.7168\n",
      "Epoch 490/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.9643 - val_loss: 231.6583\n",
      "Epoch 491/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.9574 - val_loss: 231.6721\n",
      "Epoch 492/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.9680 - val_loss: 231.7564\n",
      "Epoch 493/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.9482 - val_loss: 231.6160\n",
      "Epoch 494/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.9156 - val_loss: 231.6027\n",
      "Epoch 495/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.9180 - val_loss: 231.6327\n",
      "Epoch 496/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.8997 - val_loss: 231.5178\n",
      "Epoch 497/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.8949 - val_loss: 231.4370\n",
      "Epoch 498/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.8701 - val_loss: 231.5588\n",
      "Epoch 499/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.8609 - val_loss: 231.5529\n",
      "Epoch 500/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.8500 - val_loss: 231.5805\n",
      "Epoch 501/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.8356 - val_loss: 231.6839\n",
      "Epoch 502/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.8281 - val_loss: 231.8532\n",
      "Epoch 503/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.8461 - val_loss: 231.9937\n",
      "Epoch 504/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.8037 - val_loss: 232.0101\n",
      "Epoch 505/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.8013 - val_loss: 231.9839\n",
      "Epoch 506/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.7874 - val_loss: 231.9507\n",
      "Epoch 507/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.8137 - val_loss: 232.1588\n",
      "Epoch 508/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.7606 - val_loss: 232.0894\n",
      "Epoch 509/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.7668 - val_loss: 232.0330\n",
      "Epoch 510/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.7376 - val_loss: 232.0733\n",
      "Epoch 511/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.7404 - val_loss: 232.1646\n",
      "Epoch 512/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.7246 - val_loss: 232.1552\n",
      "Epoch 513/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.7345 - val_loss: 232.1090\n",
      "Epoch 514/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.7263 - val_loss: 232.1907\n",
      "Epoch 515/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.7043 - val_loss: 232.2008\n",
      "Epoch 516/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.6977 - val_loss: 232.2419\n",
      "Epoch 517/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.6818 - val_loss: 232.2349\n",
      "Epoch 518/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.6765 - val_loss: 232.2402\n",
      "Epoch 519/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.6869 - val_loss: 232.1915\n",
      "Epoch 520/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.6688 - val_loss: 232.2876\n",
      "Epoch 521/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.7055 - val_loss: 232.1342\n",
      "Epoch 522/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.6554 - val_loss: 232.2857\n",
      "Epoch 523/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.6375 - val_loss: 232.3294\n",
      "Epoch 524/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.6283 - val_loss: 232.4333\n",
      "Epoch 525/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.6270 - val_loss: 232.5043\n",
      "Epoch 526/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.6406 - val_loss: 232.5764\n",
      "Epoch 527/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.6227 - val_loss: 232.5801\n",
      "Epoch 528/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.6062 - val_loss: 232.5646\n",
      "Epoch 529/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.6057 - val_loss: 232.4508\n",
      "Epoch 530/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5993 - val_loss: 232.4148\n",
      "Epoch 531/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5936 - val_loss: 232.4475\n",
      "Epoch 532/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.6293 - val_loss: 232.2053\n",
      "Epoch 533/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5900 - val_loss: 232.2154\n",
      "Epoch 534/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.5842 - val_loss: 232.3278\n",
      "Epoch 535/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5703 - val_loss: 232.4100\n",
      "Epoch 536/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 207.5611 - val_loss: 232.4333\n",
      "Epoch 537/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5677 - val_loss: 232.3547\n",
      "Epoch 538/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.5973 - val_loss: 232.2748\n",
      "Epoch 539/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 207.5511 - val_loss: 232.5128\n",
      "Epoch 540/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5478 - val_loss: 232.6581\n",
      "Epoch 541/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5392 - val_loss: 232.7383\n",
      "Epoch 542/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 207.5246 - val_loss: 232.7707\n",
      "Epoch 543/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5207 - val_loss: 232.8132\n",
      "Epoch 544/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5130 - val_loss: 232.8096\n",
      "Epoch 545/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5195 - val_loss: 232.9134\n",
      "Epoch 546/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4994 - val_loss: 232.9668\n",
      "Epoch 547/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5010 - val_loss: 232.9793\n",
      "Epoch 548/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5095 - val_loss: 232.9611\n",
      "Epoch 549/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5264 - val_loss: 233.0544\n",
      "Epoch 550/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.5041 - val_loss: 233.1576\n",
      "Epoch 551/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 207.4875 - val_loss: 233.1927\n",
      "Epoch 552/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4760 - val_loss: 233.2291\n",
      "Epoch 553/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4820 - val_loss: 233.2576\n",
      "Epoch 554/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4655 - val_loss: 233.2525\n",
      "Epoch 555/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4986 - val_loss: 233.1805\n",
      "Epoch 556/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4649 - val_loss: 233.1604\n",
      "Epoch 557/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4859 - val_loss: 233.3902\n",
      "Epoch 558/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4566 - val_loss: 233.3494\n",
      "Epoch 559/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.5452 - val_loss: 233.0518\n",
      "Epoch 560/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4471 - val_loss: 233.1959\n",
      "Epoch 561/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.4479 - val_loss: 233.2580\n",
      "Epoch 562/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.4377 - val_loss: 233.3492\n",
      "Epoch 563/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.4400 - val_loss: 233.3918\n",
      "Epoch 564/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4407 - val_loss: 233.3042\n",
      "Epoch 565/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4233 - val_loss: 233.3711\n",
      "Epoch 566/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.4376 - val_loss: 233.3301\n",
      "Epoch 567/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4231 - val_loss: 233.3163\n",
      "Epoch 568/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4409 - val_loss: 233.4956\n",
      "Epoch 569/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.4200 - val_loss: 233.4046\n",
      "Epoch 570/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4088 - val_loss: 233.4047\n",
      "Epoch 571/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4070 - val_loss: 233.4088\n",
      "Epoch 572/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.4202 - val_loss: 233.3275\n",
      "Epoch 573/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4467 - val_loss: 233.5475\n",
      "Epoch 574/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 207.4080 - val_loss: 233.4757\n",
      "Epoch 575/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.4001 - val_loss: 233.4399\n",
      "Epoch 576/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.4115 - val_loss: 233.5583\n",
      "Epoch 577/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3974 - val_loss: 233.5953\n",
      "Epoch 578/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.4065 - val_loss: 233.4048\n",
      "Epoch 579/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3893 - val_loss: 233.2724\n",
      "Epoch 580/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3844 - val_loss: 233.2285\n",
      "Epoch 581/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3865 - val_loss: 233.2255\n",
      "Epoch 582/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3763 - val_loss: 233.2791\n",
      "Epoch 583/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3832 - val_loss: 233.3163\n",
      "Epoch 584/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3751 - val_loss: 233.3355\n",
      "Epoch 585/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3674 - val_loss: 233.4096\n",
      "Epoch 586/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3694 - val_loss: 233.4746\n",
      "Epoch 587/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3908 - val_loss: 233.5860\n",
      "Epoch 588/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3676 - val_loss: 233.5484\n",
      "Epoch 589/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3839 - val_loss: 233.4262\n",
      "Epoch 590/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3573 - val_loss: 233.5060\n",
      "Epoch 591/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3757 - val_loss: 233.6080\n",
      "Epoch 592/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3596 - val_loss: 233.5342\n",
      "Epoch 593/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 207.3610 - val_loss: 233.5725\n",
      "Epoch 594/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3724 - val_loss: 233.7643\n",
      "Epoch 595/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3464 - val_loss: 233.7235\n",
      "Epoch 596/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3596 - val_loss: 233.6788\n",
      "Epoch 597/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3808 - val_loss: 233.8369\n",
      "Epoch 598/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3583 - val_loss: 233.7445\n",
      "Epoch 599/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3486 - val_loss: 233.7568\n",
      "Epoch 600/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3477 - val_loss: 233.7344\n",
      "Epoch 601/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3445 - val_loss: 233.6801\n",
      "Epoch 602/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3360 - val_loss: 233.6922\n",
      "Epoch 603/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3351 - val_loss: 233.7306\n",
      "Epoch 604/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3347 - val_loss: 233.7994\n",
      "Epoch 605/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3788 - val_loss: 233.6360\n",
      "Epoch 606/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3568 - val_loss: 233.8545\n",
      "Epoch 607/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3297 - val_loss: 233.8688\n",
      "Epoch 608/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3424 - val_loss: 233.9663\n",
      "Epoch 609/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3278 - val_loss: 233.8984\n",
      "Epoch 610/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3168 - val_loss: 233.8462\n",
      "Epoch 611/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3138 - val_loss: 233.8147\n",
      "Epoch 612/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3193 - val_loss: 233.8309\n",
      "Epoch 613/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3250 - val_loss: 233.8043\n",
      "Epoch 614/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3204 - val_loss: 233.7566\n",
      "Epoch 615/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3125 - val_loss: 233.7650\n",
      "Epoch 616/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3189 - val_loss: 233.8200\n",
      "Epoch 617/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3140 - val_loss: 233.7653\n",
      "Epoch 618/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3100 - val_loss: 233.7150\n",
      "Epoch 619/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3055 - val_loss: 233.7880\n",
      "Epoch 620/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3084 - val_loss: 233.8388\n",
      "Epoch 621/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3401 - val_loss: 233.9123\n",
      "Epoch 622/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3056 - val_loss: 233.8260\n",
      "Epoch 623/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3151 - val_loss: 233.6425\n",
      "Epoch 624/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3025 - val_loss: 233.5805\n",
      "Epoch 625/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3236 - val_loss: 233.6496\n",
      "Epoch 626/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3106 - val_loss: 233.4889\n",
      "Epoch 627/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3167 - val_loss: 233.6072\n",
      "Epoch 628/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3023 - val_loss: 233.6022\n",
      "Epoch 629/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3177 - val_loss: 233.6804\n",
      "Epoch 630/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3084 - val_loss: 233.7405\n",
      "Epoch 631/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2964 - val_loss: 233.7314\n",
      "Epoch 632/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3078 - val_loss: 233.8730\n",
      "Epoch 633/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2983 - val_loss: 233.9206\n",
      "Epoch 634/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3016 - val_loss: 234.0125\n",
      "Epoch 635/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2845 - val_loss: 233.9268\n",
      "Epoch 636/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2948 - val_loss: 233.8256\n",
      "Epoch 637/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3115 - val_loss: 233.9360\n",
      "Epoch 638/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2843 - val_loss: 233.9109\n",
      "Epoch 639/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3007 - val_loss: 233.7673\n",
      "Epoch 640/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2855 - val_loss: 233.7882\n",
      "Epoch 641/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3266 - val_loss: 233.7160\n",
      "Epoch 642/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2944 - val_loss: 233.9461\n",
      "Epoch 643/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2778 - val_loss: 233.9806\n",
      "Epoch 644/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2852 - val_loss: 234.0504\n",
      "Epoch 645/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2812 - val_loss: 234.1562\n",
      "Epoch 646/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2884 - val_loss: 234.2606\n",
      "Epoch 647/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2880 - val_loss: 234.2860\n",
      "Epoch 648/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2813 - val_loss: 234.2421\n",
      "Epoch 649/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2861 - val_loss: 234.1827\n",
      "Epoch 650/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2950 - val_loss: 234.0236\n",
      "Epoch 651/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2872 - val_loss: 234.0418\n",
      "Epoch 652/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2803 - val_loss: 234.1531\n",
      "Epoch 653/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2898 - val_loss: 234.0816\n",
      "Epoch 654/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2696 - val_loss: 234.1111\n",
      "Epoch 655/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.3007 - val_loss: 234.2939\n",
      "Epoch 656/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2700 - val_loss: 234.2659\n",
      "Epoch 657/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2957 - val_loss: 234.1335\n",
      "Epoch 658/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2660 - val_loss: 234.1655\n",
      "Epoch 659/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2631 - val_loss: 234.1985\n",
      "Epoch 660/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2814 - val_loss: 234.2680\n",
      "Epoch 661/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2662 - val_loss: 234.2076\n",
      "Epoch 662/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2725 - val_loss: 234.2682\n",
      "Epoch 663/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2709 - val_loss: 234.1516\n",
      "Epoch 664/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2708 - val_loss: 234.0777\n",
      "Epoch 665/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2852 - val_loss: 234.0478\n",
      "Epoch 666/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2680 - val_loss: 234.1243\n",
      "Epoch 667/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2676 - val_loss: 234.2386\n",
      "Epoch 668/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2770 - val_loss: 234.1394\n",
      "Epoch 669/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2896 - val_loss: 234.3272\n",
      "Epoch 670/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2782 - val_loss: 234.3382\n",
      "Epoch 671/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2605 - val_loss: 234.2886\n",
      "Epoch 672/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2569 - val_loss: 234.2733\n",
      "Epoch 673/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2909 - val_loss: 234.1304\n",
      "Epoch 674/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2986 - val_loss: 234.2848\n",
      "Epoch 675/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2699 - val_loss: 234.2921\n",
      "Epoch 676/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2628 - val_loss: 234.2466\n",
      "Epoch 677/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2527 - val_loss: 234.2562\n",
      "Epoch 678/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2739 - val_loss: 234.2858\n",
      "Epoch 679/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2838 - val_loss: 234.4288\n",
      "Epoch 680/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2942 - val_loss: 234.5873\n",
      "Epoch 681/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2538 - val_loss: 234.5161\n",
      "Epoch 682/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2704 - val_loss: 234.5759\n",
      "Epoch 683/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2528 - val_loss: 234.4061\n",
      "Epoch 684/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2615 - val_loss: 234.2811\n",
      "Epoch 685/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2719 - val_loss: 234.3282\n",
      "Epoch 686/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2627 - val_loss: 234.2360\n",
      "Epoch 687/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2653 - val_loss: 234.3257\n",
      "Epoch 688/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2528 - val_loss: 234.3159\n",
      "Epoch 689/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2531 - val_loss: 234.3190\n",
      "Epoch 690/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2589 - val_loss: 234.3689\n",
      "Epoch 691/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2645 - val_loss: 234.3456\n",
      "Epoch 692/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3100 - val_loss: 234.6090\n",
      "Epoch 693/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2466 - val_loss: 234.5539\n",
      "Epoch 694/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2831 - val_loss: 234.4881\n",
      "Epoch 695/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2490 - val_loss: 234.4714\n",
      "Epoch 696/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2568 - val_loss: 234.4575\n",
      "Epoch 697/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2538 - val_loss: 234.4376\n",
      "Epoch 698/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2559 - val_loss: 234.4570\n",
      "Epoch 699/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2617 - val_loss: 234.3439\n",
      "Epoch 700/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2429 - val_loss: 234.3677\n",
      "Epoch 701/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2517 - val_loss: 234.4167\n",
      "Epoch 702/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2668 - val_loss: 234.3401\n",
      "Epoch 703/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2444 - val_loss: 234.4720\n",
      "Epoch 704/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2560 - val_loss: 234.5882\n",
      "Epoch 705/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2567 - val_loss: 234.6165\n",
      "Epoch 706/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2464 - val_loss: 234.6308\n",
      "Epoch 707/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2567 - val_loss: 234.7389\n",
      "Epoch 708/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2496 - val_loss: 234.7864\n",
      "Epoch 709/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2599 - val_loss: 234.6689\n",
      "Epoch 710/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2464 - val_loss: 234.6684\n",
      "Epoch 711/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2560 - val_loss: 234.7403\n",
      "Epoch 712/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2724 - val_loss: 234.6162\n",
      "Epoch 713/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2977 - val_loss: 234.8021\n",
      "Epoch 714/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2491 - val_loss: 234.6944\n",
      "Epoch 715/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2730 - val_loss: 234.4813\n",
      "Epoch 716/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2738 - val_loss: 234.6109\n",
      "Epoch 717/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2555 - val_loss: 234.4984\n",
      "Epoch 718/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2567 - val_loss: 234.5920\n",
      "Epoch 719/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2484 - val_loss: 234.4940\n",
      "Epoch 720/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2429 - val_loss: 234.4529\n",
      "Epoch 721/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2430 - val_loss: 234.4700\n",
      "Epoch 722/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2471 - val_loss: 234.4647\n",
      "Epoch 723/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2600 - val_loss: 234.5071\n",
      "Epoch 724/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2398 - val_loss: 234.3257\n",
      "Epoch 725/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3444 - val_loss: 234.0592\n",
      "Epoch 726/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2443 - val_loss: 234.1409\n",
      "Epoch 727/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2574 - val_loss: 234.2721\n",
      "Epoch 728/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2451 - val_loss: 234.3160\n",
      "Epoch 729/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2458 - val_loss: 234.3341\n",
      "Epoch 730/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2471 - val_loss: 234.3448\n",
      "Epoch 731/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2417 - val_loss: 234.3637\n",
      "Epoch 732/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2395 - val_loss: 234.4185\n",
      "Epoch 733/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2467 - val_loss: 234.4840\n",
      "Epoch 734/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2725 - val_loss: 234.3718\n",
      "Epoch 735/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2414 - val_loss: 234.4414\n",
      "Epoch 736/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2447 - val_loss: 234.4536\n",
      "Epoch 737/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.3158 - val_loss: 234.2706\n",
      "Epoch 738/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2474 - val_loss: 234.4898\n",
      "Epoch 739/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2601 - val_loss: 234.6333\n",
      "Epoch 740/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2888 - val_loss: 234.8617\n",
      "Epoch 741/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2326 - val_loss: 234.7779\n",
      "Epoch 742/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2401 - val_loss: 234.7515\n",
      "Epoch 743/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2507 - val_loss: 234.5899\n",
      "Epoch 744/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 207.2396 - val_loss: 234.5076\n",
      "Epoch 745/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2655 - val_loss: 234.3795\n",
      "Epoch 746/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2409 - val_loss: 234.4597\n",
      "Epoch 747/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2630 - val_loss: 234.6090\n",
      "Epoch 748/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 207.2514 - val_loss: 234.4986\n",
      "Epoch 749/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2529 - val_loss: 234.5126\n",
      "Epoch 750/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 207.2405 - val_loss: 234.5590\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "model = baseline_model()\n",
    "history = model.fit(X_train, y_train, epochs=750,\n",
    "                    batch_size=30, verbose=1,\n",
    "                    validation_data=(X_test, y_test))\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "mse = metrics.mean_squared_error(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8837d380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:21:26.222041Z",
     "iopub.status.busy": "2022-04-29T00:21:26.221751Z",
     "iopub.status.idle": "2022-04-29T00:21:26.226834Z",
     "shell.execute_reply": "2022-04-29T00:21:26.225948Z"
    },
    "papermill": {
     "duration": 0.691611,
     "end_time": "2022-04-29T00:21:26.229709",
     "exception": false,
     "start_time": "2022-04-29T00:21:25.538098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network MSE: 234.55903682379224\n",
      "Neural network RMSE: 15.315320330433583\n"
     ]
    }
   ],
   "source": [
    "annMSE = str(mse)\n",
    "annRMSE = str(np.sqrt(mse))\n",
    "\n",
    "print(\"Neural network MSE: \" + annMSE)\n",
    "print(\"Neural network RMSE: \" + annRMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297123b",
   "metadata": {
    "papermill": {
     "duration": 0.73088,
     "end_time": "2022-04-29T00:21:27.646346",
     "exception": false,
     "start_time": "2022-04-29T00:21:26.915466",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bagged Linear Regression\n",
    "The linear regression model is bagged to reduce existing variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75b38952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:21:29.027386Z",
     "iopub.status.busy": "2022-04-29T00:21:29.026905Z",
     "iopub.status.idle": "2022-04-29T00:21:29.033677Z",
     "shell.execute_reply": "2022-04-29T00:21:29.033044Z"
    },
    "papermill": {
     "duration": 0.692716,
     "end_time": "2022-04-29T00:21:29.035655",
     "exception": false,
     "start_time": "2022-04-29T00:21:28.342939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = studentDf[featureNLi]\n",
    "y = studentDf['Score']\n",
    "\n",
    "# Split the data for linear regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45bfa4db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:21:30.414799Z",
     "iopub.status.busy": "2022-04-29T00:21:30.414501Z",
     "iopub.status.idle": "2022-04-29T00:21:30.420335Z",
     "shell.execute_reply": "2022-04-29T00:21:30.419209Z"
    },
    "papermill": {
     "duration": 0.703339,
     "end_time": "2022-04-29T00:21:30.422406",
     "exception": false,
     "start_time": "2022-04-29T00:21:29.719067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluateModel(model, X_test, y_test, title):\n",
    "    print(\"\\n****** \" + title)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    print('Root Mean Squared Error:', rmse)\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40492abd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:21:31.861355Z",
     "iopub.status.busy": "2022-04-29T00:21:31.860558Z",
     "iopub.status.idle": "2022-04-29T00:21:33.408038Z",
     "shell.execute_reply": "2022-04-29T00:21:33.407136Z"
    },
    "papermill": {
     "duration": 2.2358,
     "end_time": "2022-04-29T00:21:33.411211",
     "exception": false,
     "start_time": "2022-04-29T00:21:31.175411",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Bagging\n",
      "Root Mean Squared Error: 16.52989741388127\n"
     ]
    }
   ],
   "source": [
    "# Build linear regression ensemble.\n",
    "ensembleModel = BaggingRegressor(base_estimator=LinearRegression(),max_features=6,\n",
    "                        max_samples =0.5,\n",
    "                        n_estimators=750).fit(X_train, y_train)\n",
    "\n",
    "baggedRMSE = evaluateModel(ensembleModel, X_test, y_test, \"Bagging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35be2303",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:21:34.787171Z",
     "iopub.status.busy": "2022-04-29T00:21:34.786646Z",
     "iopub.status.idle": "2022-04-29T00:21:34.798339Z",
     "shell.execute_reply": "2022-04-29T00:21:34.797129Z"
    },
    "papermill": {
     "duration": 0.701321,
     "end_time": "2022-04-29T00:21:34.800731",
     "exception": false,
     "start_time": "2022-04-29T00:21:34.099410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Linear Regression\n",
      "Root Mean Squared Error: 16.572930356354934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.572930356354934"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build stand alone linear regression model.\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "evaluateModel(model, X_test, y_test, \"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2757f1bd",
   "metadata": {
    "papermill": {
     "duration": 0.685749,
     "end_time": "2022-04-29T00:21:36.171587",
     "exception": false,
     "start_time": "2022-04-29T00:21:35.485838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stacked Model\n",
    "10 ANN models are stacked to improve model performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87f0a8c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:21:37.563703Z",
     "iopub.status.busy": "2022-04-29T00:21:37.563011Z",
     "iopub.status.idle": "2022-04-29T00:21:37.567694Z",
     "shell.execute_reply": "2022-04-29T00:21:37.566987Z"
    },
    "papermill": {
     "duration": 0.692583,
     "end_time": "2022-04-29T00:21:37.569746",
     "exception": false,
     "start_time": "2022-04-29T00:21:36.877163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_MODEL =  \"./models/\"\n",
    "NUM_MODEL = 10\n",
    "MODEL_NAME = \"comp4948_a2_model_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "072db1d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:21:38.947530Z",
     "iopub.status.busy": "2022-04-29T00:21:38.946702Z",
     "iopub.status.idle": "2022-04-29T00:21:38.955596Z",
     "shell.execute_reply": "2022-04-29T00:21:38.954735Z"
    },
    "papermill": {
     "duration": 0.69995,
     "end_time": "2022-04-29T00:21:38.957969",
     "exception": false,
     "start_time": "2022-04-29T00:21:38.258019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = studentDf[featureNLi].values\n",
    "y = studentDf['Score'].values\n",
    "\n",
    "ROW_DIM = 0\n",
    "COL_DIM = 1\n",
    "\n",
    "x_arrayReshaped = X.reshape(X.shape[ROW_DIM], X.shape[COL_DIM])\n",
    "y_arrayReshaped = y.reshape(y.shape[ROW_DIM],1)\n",
    "\n",
    "# Split the data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_arrayReshaped,\n",
    "         y_arrayReshaped, test_size=0.2)\n",
    "\n",
    "n_features = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edc91666",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:21:40.331060Z",
     "iopub.status.busy": "2022-04-29T00:21:40.330782Z",
     "iopub.status.idle": "2022-04-29T00:21:40.337133Z",
     "shell.execute_reply": "2022-04-29T00:21:40.336208Z"
    },
    "papermill": {
     "duration": 0.694864,
     "end_time": "2022-04-29T00:21:40.339230",
     "exception": false,
     "start_time": "2022-04-29T00:21:39.644366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit model on dataset\n",
    "def fit_model(trainX, trainy):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=n_features, kernel_initializer='uniform',\n",
    "                    activation=\"softplus\"))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='uniform'))\n",
    "\n",
    "    opt = Adamax(lr=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "\n",
    "    # fit model\n",
    "    history = model.fit(trainX, trainy, epochs=750, batch_size=30, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d8de3d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:21:41.713773Z",
     "iopub.status.busy": "2022-04-29T00:21:41.713455Z",
     "iopub.status.idle": "2022-04-29T00:21:41.719151Z",
     "shell.execute_reply": "2022-04-29T00:21:41.718483Z"
    },
    "papermill": {
     "duration": 0.694937,
     "end_time": "2022-04-29T00:21:41.720881",
     "exception": false,
     "start_time": "2022-04-29T00:21:41.025944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generateModels(trainX, trainy):\n",
    "    # create directory for models\n",
    "    if (not path.exists(PATH_MODEL)):\n",
    "        makedirs('./models')\n",
    "\n",
    "    # fit and save models\n",
    "    for i in range(NUM_MODEL):\n",
    "        # fit model\n",
    "        model, history = fit_model(trainX, trainy)\n",
    "\n",
    "        # save model\n",
    "        filename = PATH_MODEL + MODEL_NAME + str(i + 1) + '.h5'\n",
    "        model.save(filename)\n",
    "        print('>Saved %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03755087",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:21:43.095474Z",
     "iopub.status.busy": "2022-04-29T00:21:43.094966Z",
     "iopub.status.idle": "2022-04-29T00:21:43.100034Z",
     "shell.execute_reply": "2022-04-29T00:21:43.099221Z"
    },
    "papermill": {
     "duration": 0.6931,
     "end_time": "2022-04-29T00:21:43.102155",
     "exception": false,
     "start_time": "2022-04-29T00:21:42.409055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_all_models(n_models):\n",
    "    all_models = list()\n",
    "    for i in range(n_models):\n",
    "        # define filename for this ensemble\n",
    "        filename = PATH_MODEL + MODEL_NAME + str(i + 1) + '.h5'\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of models\n",
    "        all_models.append(model)\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f1753c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:21:44.489828Z",
     "iopub.status.busy": "2022-04-29T00:21:44.489138Z",
     "iopub.status.idle": "2022-04-29T00:28:09.245938Z",
     "shell.execute_reply": "2022-04-29T00:28:09.244672Z"
    },
    "papermill": {
     "duration": 385.450383,
     "end_time": "2022-04-29T00:28:09.248787",
     "exception": false,
     "start_time": "2022-04-29T00:21:43.798404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/750\n",
      "6/6 [==============================] - 1s 32ms/step - loss: 4511.9492 - val_loss: 4508.5942\n",
      "Epoch 2/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4505.7944 - val_loss: 4502.4907\n",
      "Epoch 3/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4499.6313 - val_loss: 4496.3403\n",
      "Epoch 4/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4493.4497 - val_loss: 4490.0298\n",
      "Epoch 5/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4487.1909 - val_loss: 4483.7031\n",
      "Epoch 6/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4480.8516 - val_loss: 4477.3125\n",
      "Epoch 7/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4474.3984 - val_loss: 4470.8965\n",
      "Epoch 8/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4467.9292 - val_loss: 4464.3726\n",
      "Epoch 9/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4461.4014 - val_loss: 4457.7676\n",
      "Epoch 10/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4454.7300 - val_loss: 4451.0073\n",
      "Epoch 11/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4447.8862 - val_loss: 4444.0645\n",
      "Epoch 12/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4440.8545 - val_loss: 4436.9800\n",
      "Epoch 13/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4433.8975 - val_loss: 4429.8794\n",
      "Epoch 14/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4426.5928 - val_loss: 4422.6948\n",
      "Epoch 15/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4419.5020 - val_loss: 4415.4028\n",
      "Epoch 16/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4412.0908 - val_loss: 4407.9170\n",
      "Epoch 17/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4404.6270 - val_loss: 4400.1973\n",
      "Epoch 18/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4396.7524 - val_loss: 4392.2949\n",
      "Epoch 19/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4388.7866 - val_loss: 4384.1719\n",
      "Epoch 20/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4380.5918 - val_loss: 4375.9233\n",
      "Epoch 21/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4372.4082 - val_loss: 4367.7529\n",
      "Epoch 22/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4364.1211 - val_loss: 4359.3691\n",
      "Epoch 23/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4355.6895 - val_loss: 4350.8394\n",
      "Epoch 24/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4347.1655 - val_loss: 4342.1519\n",
      "Epoch 25/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4338.3516 - val_loss: 4333.3169\n",
      "Epoch 26/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4329.5054 - val_loss: 4324.3325\n",
      "Epoch 27/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4320.5327 - val_loss: 4315.2983\n",
      "Epoch 28/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4311.4595 - val_loss: 4306.1255\n",
      "Epoch 29/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4302.2651 - val_loss: 4296.6792\n",
      "Epoch 30/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4292.6890 - val_loss: 4287.0122\n",
      "Epoch 31/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4282.9253 - val_loss: 4277.0698\n",
      "Epoch 32/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4272.9199 - val_loss: 4267.0171\n",
      "Epoch 33/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4262.8940 - val_loss: 4256.8130\n",
      "Epoch 34/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4252.5952 - val_loss: 4246.4751\n",
      "Epoch 35/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4242.0737 - val_loss: 4236.1006\n",
      "Epoch 36/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4231.8535 - val_loss: 4226.0015\n",
      "Epoch 37/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4221.7285 - val_loss: 4215.5864\n",
      "Epoch 38/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4211.2739 - val_loss: 4204.8589\n",
      "Epoch 39/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4200.4199 - val_loss: 4193.9180\n",
      "Epoch 40/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4189.3535 - val_loss: 4182.8345\n",
      "Epoch 41/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4178.4058 - val_loss: 4171.7095\n",
      "Epoch 42/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4167.1157 - val_loss: 4160.2939\n",
      "Epoch 43/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4155.7563 - val_loss: 4148.8169\n",
      "Epoch 44/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4144.1797 - val_loss: 4137.1362\n",
      "Epoch 45/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4132.5674 - val_loss: 4125.4160\n",
      "Epoch 46/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4120.6558 - val_loss: 4113.5410\n",
      "Epoch 47/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4108.7324 - val_loss: 4101.4473\n",
      "Epoch 48/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 4096.6572 - val_loss: 4089.2725\n",
      "Epoch 49/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 4084.4763 - val_loss: 4076.7913\n",
      "Epoch 50/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4071.8909 - val_loss: 4064.0994\n",
      "Epoch 51/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4059.1770 - val_loss: 4051.1377\n",
      "Epoch 52/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4046.1021 - val_loss: 4038.0603\n",
      "Epoch 53/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4033.1096 - val_loss: 4024.8865\n",
      "Epoch 54/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4019.7256 - val_loss: 4011.7266\n",
      "Epoch 55/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4006.6809 - val_loss: 3998.6912\n",
      "Epoch 56/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3993.7327 - val_loss: 3985.5115\n",
      "Epoch 57/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3980.5222 - val_loss: 3972.0291\n",
      "Epoch 58/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3967.1741 - val_loss: 3958.5159\n",
      "Epoch 59/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3953.5459 - val_loss: 3944.8545\n",
      "Epoch 60/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3939.8086 - val_loss: 3930.8967\n",
      "Epoch 61/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3925.7903 - val_loss: 3916.7329\n",
      "Epoch 62/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3911.5938 - val_loss: 3902.4282\n",
      "Epoch 63/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3897.1704 - val_loss: 3887.8259\n",
      "Epoch 64/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3882.5291 - val_loss: 3872.9490\n",
      "Epoch 65/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3867.4919 - val_loss: 3857.7244\n",
      "Epoch 66/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3852.1360 - val_loss: 3842.2280\n",
      "Epoch 67/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3836.3950 - val_loss: 3826.8296\n",
      "Epoch 68/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3821.3125 - val_loss: 3811.3884\n",
      "Epoch 69/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3805.7715 - val_loss: 3795.8787\n",
      "Epoch 70/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3790.2148 - val_loss: 3780.2681\n",
      "Epoch 71/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3774.6287 - val_loss: 3764.2634\n",
      "Epoch 72/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3758.4397 - val_loss: 3748.2744\n",
      "Epoch 73/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3742.6548 - val_loss: 3732.2686\n",
      "Epoch 74/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3726.3853 - val_loss: 3716.0547\n",
      "Epoch 75/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3710.1414 - val_loss: 3699.5618\n",
      "Epoch 76/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3693.5315 - val_loss: 3682.8657\n",
      "Epoch 77/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3676.8325 - val_loss: 3665.8960\n",
      "Epoch 78/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3659.9114 - val_loss: 3649.0359\n",
      "Epoch 79/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3643.1843 - val_loss: 3631.9460\n",
      "Epoch 80/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3626.1011 - val_loss: 3614.7258\n",
      "Epoch 81/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3608.9771 - val_loss: 3597.3164\n",
      "Epoch 82/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3591.2588 - val_loss: 3579.6819\n",
      "Epoch 83/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3573.6594 - val_loss: 3561.8838\n",
      "Epoch 84/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3556.1416 - val_loss: 3544.3992\n",
      "Epoch 85/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3538.5283 - val_loss: 3526.5837\n",
      "Epoch 86/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3520.6682 - val_loss: 3508.6169\n",
      "Epoch 87/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3502.6814 - val_loss: 3490.4697\n",
      "Epoch 88/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3484.4807 - val_loss: 3472.3071\n",
      "Epoch 89/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3466.5225 - val_loss: 3454.0813\n",
      "Epoch 90/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3448.1555 - val_loss: 3435.5242\n",
      "Epoch 91/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3429.3481 - val_loss: 3416.7458\n",
      "Epoch 92/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3410.5378 - val_loss: 3398.0569\n",
      "Epoch 93/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3392.3333 - val_loss: 3380.1104\n",
      "Epoch 94/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3374.6316 - val_loss: 3362.0327\n",
      "Epoch 95/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3356.3206 - val_loss: 3343.7681\n",
      "Epoch 96/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3337.9243 - val_loss: 3325.1328\n",
      "Epoch 97/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3319.3696 - val_loss: 3306.0752\n",
      "Epoch 98/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3300.3467 - val_loss: 3286.6990\n",
      "Epoch 99/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3281.0857 - val_loss: 3267.1892\n",
      "Epoch 100/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3261.4175 - val_loss: 3247.7585\n",
      "Epoch 101/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3242.3291 - val_loss: 3227.9102\n",
      "Epoch 102/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3222.1565 - val_loss: 3207.9829\n",
      "Epoch 103/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3202.0945 - val_loss: 3187.7373\n",
      "Epoch 104/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3182.4651 - val_loss: 3168.0845\n",
      "Epoch 105/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3162.7048 - val_loss: 3148.2134\n",
      "Epoch 106/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3142.6340 - val_loss: 3128.2542\n",
      "Epoch 107/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3122.7700 - val_loss: 3107.9500\n",
      "Epoch 108/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3102.4194 - val_loss: 3087.3018\n",
      "Epoch 109/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3081.4807 - val_loss: 3066.6719\n",
      "Epoch 110/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3061.2515 - val_loss: 3046.1782\n",
      "Epoch 111/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3040.8103 - val_loss: 3025.5774\n",
      "Epoch 112/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3020.2903 - val_loss: 3004.8533\n",
      "Epoch 113/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2999.6538 - val_loss: 2983.7646\n",
      "Epoch 114/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2978.5449 - val_loss: 2962.6365\n",
      "Epoch 115/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2957.6145 - val_loss: 2941.4431\n",
      "Epoch 116/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2937.0774 - val_loss: 2920.6658\n",
      "Epoch 117/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2916.1377 - val_loss: 2899.9385\n",
      "Epoch 118/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2895.3335 - val_loss: 2878.9556\n",
      "Epoch 119/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2874.4509 - val_loss: 2857.5908\n",
      "Epoch 120/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2852.9351 - val_loss: 2836.0698\n",
      "Epoch 121/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2831.4458 - val_loss: 2814.3835\n",
      "Epoch 122/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2810.0989 - val_loss: 2792.6926\n",
      "Epoch 123/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2788.3049 - val_loss: 2770.9248\n",
      "Epoch 124/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2766.5125 - val_loss: 2748.8987\n",
      "Epoch 125/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2744.6538 - val_loss: 2726.7183\n",
      "Epoch 126/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2722.5200 - val_loss: 2704.4541\n",
      "Epoch 127/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2700.4304 - val_loss: 2682.1104\n",
      "Epoch 128/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2678.1108 - val_loss: 2659.6311\n",
      "Epoch 129/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2655.8022 - val_loss: 2637.1460\n",
      "Epoch 130/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2633.6453 - val_loss: 2614.6213\n",
      "Epoch 131/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2610.9653 - val_loss: 2592.2375\n",
      "Epoch 132/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2588.7800 - val_loss: 2570.0906\n",
      "Epoch 133/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2566.4355 - val_loss: 2548.0261\n",
      "Epoch 134/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2544.6917 - val_loss: 2525.3943\n",
      "Epoch 135/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2522.2271 - val_loss: 2503.0515\n",
      "Epoch 136/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2499.7341 - val_loss: 2480.7595\n",
      "Epoch 137/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2477.4814 - val_loss: 2458.3291\n",
      "Epoch 138/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2455.0701 - val_loss: 2436.0291\n",
      "Epoch 139/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2432.9321 - val_loss: 2413.4983\n",
      "Epoch 140/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2410.5286 - val_loss: 2390.7324\n",
      "Epoch 141/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2387.7483 - val_loss: 2367.9158\n",
      "Epoch 142/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2364.7554 - val_loss: 2345.0371\n",
      "Epoch 143/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2342.3823 - val_loss: 2322.1306\n",
      "Epoch 144/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2319.1851 - val_loss: 2299.7593\n",
      "Epoch 145/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2297.5701 - val_loss: 2277.1709\n",
      "Epoch 146/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2275.1836 - val_loss: 2254.6519\n",
      "Epoch 147/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2252.4507 - val_loss: 2232.2437\n",
      "Epoch 148/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2230.1426 - val_loss: 2209.5979\n",
      "Epoch 149/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2207.6968 - val_loss: 2186.7874\n",
      "Epoch 150/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2184.9670 - val_loss: 2163.9326\n",
      "Epoch 151/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2162.3640 - val_loss: 2141.1174\n",
      "Epoch 152/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2139.7031 - val_loss: 2118.1335\n",
      "Epoch 153/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2116.6755 - val_loss: 2095.0645\n",
      "Epoch 154/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2093.6526 - val_loss: 2071.8499\n",
      "Epoch 155/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2070.4150 - val_loss: 2048.5454\n",
      "Epoch 156/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2047.8031 - val_loss: 2025.6268\n",
      "Epoch 157/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2024.7651 - val_loss: 2002.7997\n",
      "Epoch 158/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2001.9230 - val_loss: 1979.8434\n",
      "Epoch 159/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1979.2434 - val_loss: 1956.6274\n",
      "Epoch 160/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1956.0826 - val_loss: 1933.4261\n",
      "Epoch 161/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1933.2876 - val_loss: 1910.5586\n",
      "Epoch 162/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1911.0096 - val_loss: 1888.4403\n",
      "Epoch 163/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1888.8781 - val_loss: 1866.2823\n",
      "Epoch 164/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1866.9471 - val_loss: 1843.9855\n",
      "Epoch 165/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1844.4773 - val_loss: 1821.7218\n",
      "Epoch 166/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1822.2278 - val_loss: 1799.2876\n",
      "Epoch 167/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1799.9614 - val_loss: 1776.6694\n",
      "Epoch 168/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1777.5077 - val_loss: 1754.3230\n",
      "Epoch 169/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1755.5563 - val_loss: 1731.9688\n",
      "Epoch 170/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1733.2322 - val_loss: 1709.6473\n",
      "Epoch 171/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1711.0470 - val_loss: 1687.2759\n",
      "Epoch 172/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1688.8568 - val_loss: 1665.9751\n",
      "Epoch 173/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1668.7473 - val_loss: 1645.4563\n",
      "Epoch 174/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1648.1760 - val_loss: 1625.2423\n",
      "Epoch 175/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1627.9792 - val_loss: 1605.0508\n",
      "Epoch 176/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1607.9513 - val_loss: 1584.6942\n",
      "Epoch 177/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1587.8657 - val_loss: 1564.1980\n",
      "Epoch 178/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1567.6837 - val_loss: 1543.6542\n",
      "Epoch 179/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1547.2544 - val_loss: 1523.2336\n",
      "Epoch 180/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1526.7747 - val_loss: 1502.9481\n",
      "Epoch 181/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1506.8838 - val_loss: 1482.5966\n",
      "Epoch 182/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1486.6294 - val_loss: 1462.4413\n",
      "Epoch 183/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1466.8920 - val_loss: 1442.1041\n",
      "Epoch 184/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1446.7246 - val_loss: 1421.8846\n",
      "Epoch 185/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1426.5587 - val_loss: 1401.7294\n",
      "Epoch 186/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1406.4762 - val_loss: 1381.6316\n",
      "Epoch 187/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1386.8414 - val_loss: 1361.2821\n",
      "Epoch 188/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1366.5122 - val_loss: 1341.2432\n",
      "Epoch 189/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1346.6838 - val_loss: 1321.2450\n",
      "Epoch 190/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1326.7489 - val_loss: 1301.3583\n",
      "Epoch 191/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1307.1937 - val_loss: 1281.3770\n",
      "Epoch 192/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1287.4590 - val_loss: 1261.4041\n",
      "Epoch 193/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1267.6586 - val_loss: 1241.5972\n",
      "Epoch 194/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1248.3542 - val_loss: 1221.7036\n",
      "Epoch 195/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1228.8749 - val_loss: 1201.9541\n",
      "Epoch 196/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1209.2443 - val_loss: 1182.4357\n",
      "Epoch 197/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1189.8547 - val_loss: 1162.9598\n",
      "Epoch 198/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1170.5884 - val_loss: 1143.5841\n",
      "Epoch 199/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1151.7690 - val_loss: 1124.1323\n",
      "Epoch 200/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1132.5852 - val_loss: 1104.9237\n",
      "Epoch 201/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1113.4558 - val_loss: 1085.9452\n",
      "Epoch 202/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1094.9344 - val_loss: 1067.0388\n",
      "Epoch 203/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1076.0094 - val_loss: 1048.4532\n",
      "Epoch 204/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1058.0063 - val_loss: 1029.7716\n",
      "Epoch 205/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1039.4653 - val_loss: 1011.4172\n",
      "Epoch 206/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1021.1519 - val_loss: 993.3252\n",
      "Epoch 207/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1003.7443 - val_loss: 975.1418\n",
      "Epoch 208/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 985.7595 - val_loss: 957.2385\n",
      "Epoch 209/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 968.2864 - val_loss: 939.5125\n",
      "Epoch 210/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 951.0348 - val_loss: 921.9077\n",
      "Epoch 211/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 933.7822 - val_loss: 904.4833\n",
      "Epoch 212/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 916.7230 - val_loss: 887.2761\n",
      "Epoch 213/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 899.9719 - val_loss: 870.2761\n",
      "Epoch 214/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 883.3083 - val_loss: 853.5002\n",
      "Epoch 215/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 866.8773 - val_loss: 836.9563\n",
      "Epoch 216/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 850.5002 - val_loss: 820.7522\n",
      "Epoch 217/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 834.7444 - val_loss: 804.6347\n",
      "Epoch 218/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 819.1060 - val_loss: 788.7432\n",
      "Epoch 219/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 803.6877 - val_loss: 772.9065\n",
      "Epoch 220/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 787.9871 - val_loss: 757.5362\n",
      "Epoch 221/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 772.5294 - val_loss: 742.5724\n",
      "Epoch 222/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 758.1619 - val_loss: 727.6340\n",
      "Epoch 223/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 743.8587 - val_loss: 712.8172\n",
      "Epoch 224/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 729.6479 - val_loss: 698.2543\n",
      "Epoch 225/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 715.5089 - val_loss: 684.1816\n",
      "Epoch 226/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 701.8591 - val_loss: 670.3814\n",
      "Epoch 227/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 688.3819 - val_loss: 656.8804\n",
      "Epoch 228/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 675.2535 - val_loss: 643.5783\n",
      "Epoch 229/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 662.4650 - val_loss: 630.3252\n",
      "Epoch 230/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 649.7625 - val_loss: 617.2095\n",
      "Epoch 231/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 636.9340 - val_loss: 604.3939\n",
      "Epoch 232/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 624.4834 - val_loss: 591.8427\n",
      "Epoch 233/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 612.4564 - val_loss: 579.3731\n",
      "Epoch 234/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 600.5120 - val_loss: 567.1158\n",
      "Epoch 235/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 588.5695 - val_loss: 555.2398\n",
      "Epoch 236/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 577.1688 - val_loss: 543.4995\n",
      "Epoch 237/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 565.5437 - val_loss: 532.1331\n",
      "Epoch 238/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 554.8303 - val_loss: 520.7100\n",
      "Epoch 239/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 543.8593 - val_loss: 509.6088\n",
      "Epoch 240/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 533.0458 - val_loss: 498.9115\n",
      "Epoch 241/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 522.7963 - val_loss: 488.3402\n",
      "Epoch 242/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 512.7525 - val_loss: 477.9096\n",
      "Epoch 243/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 502.7706 - val_loss: 467.7294\n",
      "Epoch 244/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 493.0512 - val_loss: 457.7520\n",
      "Epoch 245/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 483.4302 - val_loss: 448.0876\n",
      "Epoch 246/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 474.3079 - val_loss: 438.5386\n",
      "Epoch 247/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 465.1848 - val_loss: 429.2075\n",
      "Epoch 248/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 456.1792 - val_loss: 420.2472\n",
      "Epoch 249/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 447.6759 - val_loss: 411.4549\n",
      "Epoch 250/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 439.5175 - val_loss: 402.7483\n",
      "Epoch 251/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 431.1153 - val_loss: 394.4059\n",
      "Epoch 252/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 423.3033 - val_loss: 386.2480\n",
      "Epoch 253/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 415.4688 - val_loss: 378.3942\n",
      "Epoch 254/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 408.3225 - val_loss: 370.5445\n",
      "Epoch 255/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 400.7536 - val_loss: 363.0695\n",
      "Epoch 256/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 393.6337 - val_loss: 355.8644\n",
      "Epoch 257/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 386.8348 - val_loss: 348.8074\n",
      "Epoch 258/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 380.1683 - val_loss: 341.9969\n",
      "Epoch 259/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 373.8991 - val_loss: 335.2475\n",
      "Epoch 260/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 367.3800 - val_loss: 328.8985\n",
      "Epoch 261/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 361.2380 - val_loss: 322.8156\n",
      "Epoch 262/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 355.7914 - val_loss: 316.6117\n",
      "Epoch 263/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 349.8983 - val_loss: 310.7331\n",
      "Epoch 264/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 344.3109 - val_loss: 305.1081\n",
      "Epoch 265/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 339.0427 - val_loss: 299.6304\n",
      "Epoch 266/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 334.4375 - val_loss: 294.0468\n",
      "Epoch 267/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 328.9657 - val_loss: 289.0693\n",
      "Epoch 268/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 324.4010 - val_loss: 284.1151\n",
      "Epoch 269/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 319.8400 - val_loss: 279.3286\n",
      "Epoch 270/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 315.4621 - val_loss: 274.6564\n",
      "Epoch 271/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 311.0861 - val_loss: 270.2521\n",
      "Epoch 272/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 306.8761 - val_loss: 266.0995\n",
      "Epoch 273/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 303.1600 - val_loss: 261.9544\n",
      "Epoch 274/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 299.3613 - val_loss: 257.9528\n",
      "Epoch 275/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 295.6725 - val_loss: 254.1389\n",
      "Epoch 276/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 292.3541 - val_loss: 250.3217\n",
      "Epoch 277/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 288.8688 - val_loss: 246.7347\n",
      "Epoch 278/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 285.6092 - val_loss: 243.3400\n",
      "Epoch 279/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 282.5721 - val_loss: 240.0643\n",
      "Epoch 280/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 279.5696 - val_loss: 236.9876\n",
      "Epoch 281/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 276.6863 - val_loss: 234.0803\n",
      "Epoch 282/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 274.2675 - val_loss: 231.1025\n",
      "Epoch 283/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 271.4911 - val_loss: 228.4036\n",
      "Epoch 284/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 269.1192 - val_loss: 225.7779\n",
      "Epoch 285/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 266.7420 - val_loss: 223.3127\n",
      "Epoch 286/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 264.6006 - val_loss: 220.8522\n",
      "Epoch 287/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 262.3050 - val_loss: 218.6065\n",
      "Epoch 288/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 260.5296 - val_loss: 216.3036\n",
      "Epoch 289/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 258.5389 - val_loss: 214.1422\n",
      "Epoch 290/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 256.5164 - val_loss: 212.1944\n",
      "Epoch 291/750\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 254.9077 - val_loss: 210.2282\n",
      "Epoch 292/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 253.1948 - val_loss: 208.3799\n",
      "Epoch 293/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 251.5301 - val_loss: 206.6932\n",
      "Epoch 294/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 250.0518 - val_loss: 205.0601\n",
      "Epoch 295/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 248.7413 - val_loss: 203.4314\n",
      "Epoch 296/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 247.3341 - val_loss: 201.8901\n",
      "Epoch 297/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 246.0109 - val_loss: 200.4335\n",
      "Epoch 298/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 244.8950 - val_loss: 199.0065\n",
      "Epoch 299/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 243.5506 - val_loss: 197.7684\n",
      "Epoch 300/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 242.6701 - val_loss: 196.4528\n",
      "Epoch 301/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 241.4590 - val_loss: 195.3178\n",
      "Epoch 302/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 240.5503 - val_loss: 194.2160\n",
      "Epoch 303/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 239.6997 - val_loss: 193.1546\n",
      "Epoch 304/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 238.7883 - val_loss: 192.1646\n",
      "Epoch 305/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 237.9612 - val_loss: 191.2515\n",
      "Epoch 306/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 237.1597 - val_loss: 190.3879\n",
      "Epoch 307/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 236.6248 - val_loss: 189.4520\n",
      "Epoch 308/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 235.7188 - val_loss: 188.6995\n",
      "Epoch 309/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 235.1202 - val_loss: 187.9398\n",
      "Epoch 310/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 234.5404 - val_loss: 187.2117\n",
      "Epoch 311/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 234.0113 - val_loss: 186.4975\n",
      "Epoch 312/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 233.5118 - val_loss: 185.8188\n",
      "Epoch 313/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 232.8896 - val_loss: 185.2475\n",
      "Epoch 314/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 232.3963 - val_loss: 184.7031\n",
      "Epoch 315/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 232.0503 - val_loss: 184.1045\n",
      "Epoch 316/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.5402 - val_loss: 183.6131\n",
      "Epoch 317/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 231.1803 - val_loss: 183.1045\n",
      "Epoch 318/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.7824 - val_loss: 182.6354\n",
      "Epoch 319/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.5660 - val_loss: 182.0863\n",
      "Epoch 320/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.0481 - val_loss: 181.6991\n",
      "Epoch 321/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.7164 - val_loss: 181.3409\n",
      "Epoch 322/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.4885 - val_loss: 180.9276\n",
      "Epoch 323/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.2957 - val_loss: 180.4863\n",
      "Epoch 324/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.8682 - val_loss: 180.1784\n",
      "Epoch 325/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.6253 - val_loss: 179.8730\n",
      "Epoch 326/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.4312 - val_loss: 179.5569\n",
      "Epoch 327/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.2483 - val_loss: 179.2314\n",
      "Epoch 328/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.9686 - val_loss: 178.9651\n",
      "Epoch 329/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.7274 - val_loss: 178.7299\n",
      "Epoch 330/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.6299 - val_loss: 178.4144\n",
      "Epoch 331/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.3919 - val_loss: 178.1633\n",
      "Epoch 332/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.2033 - val_loss: 177.9504\n",
      "Epoch 333/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.0704 - val_loss: 177.7343\n",
      "Epoch 334/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.9043 - val_loss: 177.5452\n",
      "Epoch 335/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.7997 - val_loss: 177.3323\n",
      "Epoch 336/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.6519 - val_loss: 177.1759\n",
      "Epoch 337/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.5109 - val_loss: 177.0180\n",
      "Epoch 338/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.4122 - val_loss: 176.8470\n",
      "Epoch 339/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.3323 - val_loss: 176.6703\n",
      "Epoch 340/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.2029 - val_loss: 176.5146\n",
      "Epoch 341/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.0616 - val_loss: 176.4066\n",
      "Epoch 342/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.9772 - val_loss: 176.2953\n",
      "Epoch 343/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.9164 - val_loss: 176.1397\n",
      "Epoch 344/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.8226 - val_loss: 175.9941\n",
      "Epoch 345/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.7534 - val_loss: 175.8582\n",
      "Epoch 346/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.6181 - val_loss: 175.7877\n",
      "Epoch 347/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.5579 - val_loss: 175.6924\n",
      "Epoch 348/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.4896 - val_loss: 175.5972\n",
      "Epoch 349/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.4095 - val_loss: 175.4981\n",
      "Epoch 350/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.3632 - val_loss: 175.3806\n",
      "Epoch 351/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.2906 - val_loss: 175.2763\n",
      "Epoch 352/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.2276 - val_loss: 175.2000\n",
      "Epoch 353/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.1736 - val_loss: 175.1023\n",
      "Epoch 354/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.0900 - val_loss: 175.0518\n",
      "Epoch 355/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.0457 - val_loss: 174.9700\n",
      "Epoch 356/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.0328 - val_loss: 174.8664\n",
      "Epoch 357/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.9420 - val_loss: 174.7903\n",
      "Epoch 358/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.8859 - val_loss: 174.7438\n",
      "Epoch 359/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.8401 - val_loss: 174.7098\n",
      "Epoch 360/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.8045 - val_loss: 174.6615\n",
      "Epoch 361/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.7518 - val_loss: 174.6134\n",
      "Epoch 362/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.7451 - val_loss: 174.5650\n",
      "Epoch 363/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.6694 - val_loss: 174.5403\n",
      "Epoch 364/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.6427 - val_loss: 174.5053\n",
      "Epoch 365/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5869 - val_loss: 174.4785\n",
      "Epoch 366/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5501 - val_loss: 174.4246\n",
      "Epoch 367/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.5035 - val_loss: 174.3972\n",
      "Epoch 368/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.4690 - val_loss: 174.3585\n",
      "Epoch 369/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.4474 - val_loss: 174.2988\n",
      "Epoch 370/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3844 - val_loss: 174.2857\n",
      "Epoch 371/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.3661 - val_loss: 174.2274\n",
      "Epoch 372/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3037 - val_loss: 174.1993\n",
      "Epoch 373/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2799 - val_loss: 174.1533\n",
      "Epoch 374/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2366 - val_loss: 174.1200\n",
      "Epoch 375/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1978 - val_loss: 174.1075\n",
      "Epoch 376/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1727 - val_loss: 174.0571\n",
      "Epoch 377/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1361 - val_loss: 174.0518\n",
      "Epoch 378/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.1026 - val_loss: 174.0347\n",
      "Epoch 379/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0734 - val_loss: 173.9520\n",
      "Epoch 380/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.0383 - val_loss: 173.9264\n",
      "Epoch 381/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9943 - val_loss: 173.8845\n",
      "Epoch 382/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.9605 - val_loss: 173.8828\n",
      "Epoch 383/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9392 - val_loss: 173.8643\n",
      "Epoch 384/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.9125 - val_loss: 173.8761\n",
      "Epoch 385/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.8678 - val_loss: 173.8091\n",
      "Epoch 386/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.8341 - val_loss: 173.7678\n",
      "Epoch 387/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.8157 - val_loss: 173.7461\n",
      "Epoch 388/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.7848 - val_loss: 173.6978\n",
      "Epoch 389/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.7417 - val_loss: 173.6963\n",
      "Epoch 390/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7113 - val_loss: 173.6983\n",
      "Epoch 391/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.6953 - val_loss: 173.6870\n",
      "Epoch 392/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.6538 - val_loss: 173.6601\n",
      "Epoch 393/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.6264 - val_loss: 173.6359\n",
      "Epoch 394/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6197 - val_loss: 173.6429\n",
      "Epoch 395/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.6033 - val_loss: 173.6009\n",
      "Epoch 396/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.5367 - val_loss: 173.5749\n",
      "Epoch 397/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.5139 - val_loss: 173.5724\n",
      "Epoch 398/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4924 - val_loss: 173.5557\n",
      "Epoch 399/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4580 - val_loss: 173.5789\n",
      "Epoch 400/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4261 - val_loss: 173.5536\n",
      "Epoch 401/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4477 - val_loss: 173.5789\n",
      "Epoch 402/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4437 - val_loss: 173.5258\n",
      "Epoch 403/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3552 - val_loss: 173.5140\n",
      "Epoch 404/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3147 - val_loss: 173.4891\n",
      "Epoch 405/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3129 - val_loss: 173.4658\n",
      "Epoch 406/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2737 - val_loss: 173.4754\n",
      "Epoch 407/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.2722 - val_loss: 173.4789\n",
      "Epoch 408/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.2285 - val_loss: 173.4662\n",
      "Epoch 409/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.2039 - val_loss: 173.4554\n",
      "Epoch 410/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1780 - val_loss: 173.4404\n",
      "Epoch 411/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1602 - val_loss: 173.4159\n",
      "Epoch 412/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1361 - val_loss: 173.4140\n",
      "Epoch 413/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1098 - val_loss: 173.4074\n",
      "Epoch 414/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0918 - val_loss: 173.4279\n",
      "Epoch 415/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0673 - val_loss: 173.3876\n",
      "Epoch 416/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0332 - val_loss: 173.3881\n",
      "Epoch 417/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0259 - val_loss: 173.3703\n",
      "Epoch 418/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0298 - val_loss: 173.4244\n",
      "Epoch 419/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9808 - val_loss: 173.4121\n",
      "Epoch 420/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9805 - val_loss: 173.4332\n",
      "Epoch 421/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9687 - val_loss: 173.3747\n",
      "Epoch 422/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9104 - val_loss: 173.3608\n",
      "Epoch 423/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8936 - val_loss: 173.3651\n",
      "Epoch 424/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8732 - val_loss: 173.3426\n",
      "Epoch 425/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8495 - val_loss: 173.3218\n",
      "Epoch 426/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8408 - val_loss: 173.2945\n",
      "Epoch 427/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8277 - val_loss: 173.3491\n",
      "Epoch 428/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7833 - val_loss: 173.3329\n",
      "Epoch 429/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8692 - val_loss: 173.2923\n",
      "Epoch 430/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7520 - val_loss: 173.3173\n",
      "Epoch 431/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7255 - val_loss: 173.3496\n",
      "Epoch 432/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7060 - val_loss: 173.3419\n",
      "Epoch 433/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6925 - val_loss: 173.3546\n",
      "Epoch 434/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6861 - val_loss: 173.3650\n",
      "Epoch 435/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6565 - val_loss: 173.3968\n",
      "Epoch 436/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6413 - val_loss: 173.4074\n",
      "Epoch 437/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6284 - val_loss: 173.3897\n",
      "Epoch 438/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6545 - val_loss: 173.4460\n",
      "Epoch 439/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6091 - val_loss: 173.4273\n",
      "Epoch 440/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5706 - val_loss: 173.4331\n",
      "Epoch 441/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5639 - val_loss: 173.4500\n",
      "Epoch 442/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5373 - val_loss: 173.4523\n",
      "Epoch 443/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5188 - val_loss: 173.4253\n",
      "Epoch 444/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5195 - val_loss: 173.3922\n",
      "Epoch 445/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4919 - val_loss: 173.4042\n",
      "Epoch 446/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4736 - val_loss: 173.4022\n",
      "Epoch 447/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4923 - val_loss: 173.4284\n",
      "Epoch 448/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4520 - val_loss: 173.4300\n",
      "Epoch 449/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4389 - val_loss: 173.4143\n",
      "Epoch 450/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4240 - val_loss: 173.4056\n",
      "Epoch 451/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4058 - val_loss: 173.3876\n",
      "Epoch 452/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3915 - val_loss: 173.3805\n",
      "Epoch 453/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3645 - val_loss: 173.3898\n",
      "Epoch 454/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3632 - val_loss: 173.4144\n",
      "Epoch 455/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3599 - val_loss: 173.3936\n",
      "Epoch 456/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3529 - val_loss: 173.4243\n",
      "Epoch 457/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3249 - val_loss: 173.4252\n",
      "Epoch 458/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3095 - val_loss: 173.4334\n",
      "Epoch 459/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2928 - val_loss: 173.4452\n",
      "Epoch 460/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2890 - val_loss: 173.4705\n",
      "Epoch 461/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2839 - val_loss: 173.5152\n",
      "Epoch 462/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2872 - val_loss: 173.5567\n",
      "Epoch 463/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2470 - val_loss: 173.5436\n",
      "Epoch 464/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2713 - val_loss: 173.5102\n",
      "Epoch 465/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2261 - val_loss: 173.5223\n",
      "Epoch 466/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2249 - val_loss: 173.5520\n",
      "Epoch 467/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1938 - val_loss: 173.5406\n",
      "Epoch 468/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2198 - val_loss: 173.5399\n",
      "Epoch 469/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1834 - val_loss: 173.5039\n",
      "Epoch 470/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1812 - val_loss: 173.4707\n",
      "Epoch 471/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1548 - val_loss: 173.4835\n",
      "Epoch 472/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1412 - val_loss: 173.4787\n",
      "Epoch 473/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1337 - val_loss: 173.4608\n",
      "Epoch 474/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1812 - val_loss: 173.5294\n",
      "Epoch 475/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1180 - val_loss: 173.5204\n",
      "Epoch 476/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1125 - val_loss: 173.5190\n",
      "Epoch 477/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0856 - val_loss: 173.5423\n",
      "Epoch 478/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0805 - val_loss: 173.5617\n",
      "Epoch 479/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0974 - val_loss: 173.5442\n",
      "Epoch 480/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0520 - val_loss: 173.5732\n",
      "Epoch 481/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0689 - val_loss: 173.6267\n",
      "Epoch 482/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0421 - val_loss: 173.6356\n",
      "Epoch 483/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0276 - val_loss: 173.6644\n",
      "Epoch 484/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0306 - val_loss: 173.6701\n",
      "Epoch 485/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0991 - val_loss: 173.6266\n",
      "Epoch 486/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9966 - val_loss: 173.6514\n",
      "Epoch 487/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9919 - val_loss: 173.6747\n",
      "Epoch 488/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9876 - val_loss: 173.6758\n",
      "Epoch 489/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9724 - val_loss: 173.6852\n",
      "Epoch 490/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9822 - val_loss: 173.6662\n",
      "Epoch 491/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9556 - val_loss: 173.6786\n",
      "Epoch 492/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9423 - val_loss: 173.6824\n",
      "Epoch 493/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9501 - val_loss: 173.6825\n",
      "Epoch 494/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9319 - val_loss: 173.7128\n",
      "Epoch 495/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9161 - val_loss: 173.7170\n",
      "Epoch 496/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9137 - val_loss: 173.7182\n",
      "Epoch 497/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9109 - val_loss: 173.7076\n",
      "Epoch 498/750\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 221.9135 - val_loss: 173.7392\n",
      "Epoch 499/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9102 - val_loss: 173.7032\n",
      "Epoch 500/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8818 - val_loss: 173.7328\n",
      "Epoch 501/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8759 - val_loss: 173.7178\n",
      "Epoch 502/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8772 - val_loss: 173.7167\n",
      "Epoch 503/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8620 - val_loss: 173.7492\n",
      "Epoch 504/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8562 - val_loss: 173.7703\n",
      "Epoch 505/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8603 - val_loss: 173.7444\n",
      "Epoch 506/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8364 - val_loss: 173.7545\n",
      "Epoch 507/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8392 - val_loss: 173.7594\n",
      "Epoch 508/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8229 - val_loss: 173.7695\n",
      "Epoch 509/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8404 - val_loss: 173.7999\n",
      "Epoch 510/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8244 - val_loss: 173.8037\n",
      "Epoch 511/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8113 - val_loss: 173.7528\n",
      "Epoch 512/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8012 - val_loss: 173.7287\n",
      "Epoch 513/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8073 - val_loss: 173.7628\n",
      "Epoch 514/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8007 - val_loss: 173.7488\n",
      "Epoch 515/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7782 - val_loss: 173.7340\n",
      "Epoch 516/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7684 - val_loss: 173.7474\n",
      "Epoch 517/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7653 - val_loss: 173.7751\n",
      "Epoch 518/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7744 - val_loss: 173.7513\n",
      "Epoch 519/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7607 - val_loss: 173.7942\n",
      "Epoch 520/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7395 - val_loss: 173.8127\n",
      "Epoch 521/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7312 - val_loss: 173.8276\n",
      "Epoch 522/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7465 - val_loss: 173.8550\n",
      "Epoch 523/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7959 - val_loss: 173.8006\n",
      "Epoch 524/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7497 - val_loss: 173.7943\n",
      "Epoch 525/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7101 - val_loss: 173.8228\n",
      "Epoch 526/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7132 - val_loss: 173.8308\n",
      "Epoch 527/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7017 - val_loss: 173.8578\n",
      "Epoch 528/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7136 - val_loss: 173.9026\n",
      "Epoch 529/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6967 - val_loss: 173.9240\n",
      "Epoch 530/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6908 - val_loss: 173.9013\n",
      "Epoch 531/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6891 - val_loss: 173.8922\n",
      "Epoch 532/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6737 - val_loss: 173.8927\n",
      "Epoch 533/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6910 - val_loss: 173.9121\n",
      "Epoch 534/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6672 - val_loss: 173.9288\n",
      "Epoch 535/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6603 - val_loss: 173.9185\n",
      "Epoch 536/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6568 - val_loss: 173.9103\n",
      "Epoch 537/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6722 - val_loss: 173.9370\n",
      "Epoch 538/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6521 - val_loss: 173.9303\n",
      "Epoch 539/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6650 - val_loss: 173.8894\n",
      "Epoch 540/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6443 - val_loss: 173.9263\n",
      "Epoch 541/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6271 - val_loss: 173.9590\n",
      "Epoch 542/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6272 - val_loss: 173.9594\n",
      "Epoch 543/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6061 - val_loss: 173.9877\n",
      "Epoch 544/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6216 - val_loss: 174.0383\n",
      "Epoch 545/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6283 - val_loss: 174.0964\n",
      "Epoch 546/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6091 - val_loss: 174.0972\n",
      "Epoch 547/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6057 - val_loss: 174.1215\n",
      "Epoch 548/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6136 - val_loss: 174.1017\n",
      "Epoch 549/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5897 - val_loss: 174.1331\n",
      "Epoch 550/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6910 - val_loss: 174.2090\n",
      "Epoch 551/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5887 - val_loss: 174.1899\n",
      "Epoch 552/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5957 - val_loss: 174.2103\n",
      "Epoch 553/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5922 - val_loss: 174.2380\n",
      "Epoch 554/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5830 - val_loss: 174.1915\n",
      "Epoch 555/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5690 - val_loss: 174.1839\n",
      "Epoch 556/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5629 - val_loss: 174.1912\n",
      "Epoch 557/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5658 - val_loss: 174.1621\n",
      "Epoch 558/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5573 - val_loss: 174.1697\n",
      "Epoch 559/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5699 - val_loss: 174.1878\n",
      "Epoch 560/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5526 - val_loss: 174.2244\n",
      "Epoch 561/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5546 - val_loss: 174.2357\n",
      "Epoch 562/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5615 - val_loss: 174.2805\n",
      "Epoch 563/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5354 - val_loss: 174.2656\n",
      "Epoch 564/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5393 - val_loss: 174.2437\n",
      "Epoch 565/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5910 - val_loss: 174.2730\n",
      "Epoch 566/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5801 - val_loss: 174.2195\n",
      "Epoch 567/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5272 - val_loss: 174.2213\n",
      "Epoch 568/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5504 - val_loss: 174.2003\n",
      "Epoch 569/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5564 - val_loss: 174.2430\n",
      "Epoch 570/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5118 - val_loss: 174.2359\n",
      "Epoch 571/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5216 - val_loss: 174.2287\n",
      "Epoch 572/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5172 - val_loss: 174.2205\n",
      "Epoch 573/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5065 - val_loss: 174.2110\n",
      "Epoch 574/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5065 - val_loss: 174.2186\n",
      "Epoch 575/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5050 - val_loss: 174.2232\n",
      "Epoch 576/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5166 - val_loss: 174.2239\n",
      "Epoch 577/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5041 - val_loss: 174.2295\n",
      "Epoch 578/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4999 - val_loss: 174.2635\n",
      "Epoch 579/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5081 - val_loss: 174.2427\n",
      "Epoch 580/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4938 - val_loss: 174.2800\n",
      "Epoch 581/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5068 - val_loss: 174.3123\n",
      "Epoch 582/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4963 - val_loss: 174.3371\n",
      "Epoch 583/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5066 - val_loss: 174.3484\n",
      "Epoch 584/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4809 - val_loss: 174.3626\n",
      "Epoch 585/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4855 - val_loss: 174.3627\n",
      "Epoch 586/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4799 - val_loss: 174.3723\n",
      "Epoch 587/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4691 - val_loss: 174.3526\n",
      "Epoch 588/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4759 - val_loss: 174.3822\n",
      "Epoch 589/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4724 - val_loss: 174.3703\n",
      "Epoch 590/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4782 - val_loss: 174.3472\n",
      "Epoch 591/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4622 - val_loss: 174.3592\n",
      "Epoch 592/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4529 - val_loss: 174.3906\n",
      "Epoch 593/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4611 - val_loss: 174.4114\n",
      "Epoch 594/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4680 - val_loss: 174.3866\n",
      "Epoch 595/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4791 - val_loss: 174.4363\n",
      "Epoch 596/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4427 - val_loss: 174.4159\n",
      "Epoch 597/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4531 - val_loss: 174.3823\n",
      "Epoch 598/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4449 - val_loss: 174.4128\n",
      "Epoch 599/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4399 - val_loss: 174.4430\n",
      "Epoch 600/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4475 - val_loss: 174.4266\n",
      "Epoch 601/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4496 - val_loss: 174.4109\n",
      "Epoch 602/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4299 - val_loss: 174.4185\n",
      "Epoch 603/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4349 - val_loss: 174.4235\n",
      "Epoch 604/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4271 - val_loss: 174.4476\n",
      "Epoch 605/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4341 - val_loss: 174.4586\n",
      "Epoch 606/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4262 - val_loss: 174.4529\n",
      "Epoch 607/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4168 - val_loss: 174.4660\n",
      "Epoch 608/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4162 - val_loss: 174.5028\n",
      "Epoch 609/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4216 - val_loss: 174.5434\n",
      "Epoch 610/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4291 - val_loss: 174.5760\n",
      "Epoch 611/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4089 - val_loss: 174.5947\n",
      "Epoch 612/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4233 - val_loss: 174.6186\n",
      "Epoch 613/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4110 - val_loss: 174.6319\n",
      "Epoch 614/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4066 - val_loss: 174.6201\n",
      "Epoch 615/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3926 - val_loss: 174.6404\n",
      "Epoch 616/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4043 - val_loss: 174.6474\n",
      "Epoch 617/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3959 - val_loss: 174.6688\n",
      "Epoch 618/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4049 - val_loss: 174.7037\n",
      "Epoch 619/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3954 - val_loss: 174.7057\n",
      "Epoch 620/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3862 - val_loss: 174.7021\n",
      "Epoch 621/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.3939 - val_loss: 174.7056\n",
      "Epoch 622/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3810 - val_loss: 174.7295\n",
      "Epoch 623/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3821 - val_loss: 174.7378\n",
      "Epoch 624/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3799 - val_loss: 174.7390\n",
      "Epoch 625/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3953 - val_loss: 174.7735\n",
      "Epoch 626/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3780 - val_loss: 174.7572\n",
      "Epoch 627/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3711 - val_loss: 174.7404\n",
      "Epoch 628/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3779 - val_loss: 174.7667\n",
      "Epoch 629/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3696 - val_loss: 174.7803\n",
      "Epoch 630/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3758 - val_loss: 174.7714\n",
      "Epoch 631/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3773 - val_loss: 174.7673\n",
      "Epoch 632/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3701 - val_loss: 174.7894\n",
      "Epoch 633/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3638 - val_loss: 174.8294\n",
      "Epoch 634/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3689 - val_loss: 174.8218\n",
      "Epoch 635/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3582 - val_loss: 174.8447\n",
      "Epoch 636/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3655 - val_loss: 174.8560\n",
      "Epoch 637/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3571 - val_loss: 174.9011\n",
      "Epoch 638/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3590 - val_loss: 174.9396\n",
      "Epoch 639/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3758 - val_loss: 174.9657\n",
      "Epoch 640/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3666 - val_loss: 174.9413\n",
      "Epoch 641/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3584 - val_loss: 174.9764\n",
      "Epoch 642/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3665 - val_loss: 174.9448\n",
      "Epoch 643/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.3463 - val_loss: 174.9516\n",
      "Epoch 644/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3467 - val_loss: 174.9915\n",
      "Epoch 645/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3352 - val_loss: 175.0232\n",
      "Epoch 646/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3521 - val_loss: 174.9925\n",
      "Epoch 647/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3423 - val_loss: 175.0112\n",
      "Epoch 648/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3352 - val_loss: 175.0287\n",
      "Epoch 649/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3285 - val_loss: 175.0332\n",
      "Epoch 650/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4014 - val_loss: 175.0850\n",
      "Epoch 651/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3314 - val_loss: 175.0632\n",
      "Epoch 652/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3346 - val_loss: 175.0821\n",
      "Epoch 653/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3426 - val_loss: 175.1098\n",
      "Epoch 654/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3345 - val_loss: 175.1332\n",
      "Epoch 655/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3180 - val_loss: 175.1070\n",
      "Epoch 656/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3188 - val_loss: 175.1003\n",
      "Epoch 657/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3161 - val_loss: 175.1038\n",
      "Epoch 658/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3172 - val_loss: 175.1074\n",
      "Epoch 659/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3186 - val_loss: 175.0894\n",
      "Epoch 660/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3142 - val_loss: 175.1119\n",
      "Epoch 661/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3214 - val_loss: 175.0931\n",
      "Epoch 662/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3089 - val_loss: 175.1025\n",
      "Epoch 663/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3203 - val_loss: 175.1012\n",
      "Epoch 664/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3200 - val_loss: 175.1410\n",
      "Epoch 665/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3189 - val_loss: 175.1120\n",
      "Epoch 666/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3291 - val_loss: 175.1629\n",
      "Epoch 667/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3186 - val_loss: 175.1638\n",
      "Epoch 668/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3107 - val_loss: 175.1775\n",
      "Epoch 669/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2980 - val_loss: 175.1935\n",
      "Epoch 670/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2949 - val_loss: 175.1949\n",
      "Epoch 671/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3201 - val_loss: 175.2307\n",
      "Epoch 672/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2981 - val_loss: 175.2355\n",
      "Epoch 673/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3098 - val_loss: 175.2194\n",
      "Epoch 674/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3111 - val_loss: 175.2022\n",
      "Epoch 675/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2881 - val_loss: 175.2175\n",
      "Epoch 676/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2891 - val_loss: 175.2429\n",
      "Epoch 677/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2887 - val_loss: 175.2762\n",
      "Epoch 678/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3006 - val_loss: 175.2759\n",
      "Epoch 679/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2807 - val_loss: 175.3122\n",
      "Epoch 680/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2871 - val_loss: 175.3277\n",
      "Epoch 681/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2864 - val_loss: 175.3323\n",
      "Epoch 682/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2820 - val_loss: 175.3553\n",
      "Epoch 683/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2982 - val_loss: 175.3908\n",
      "Epoch 684/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2828 - val_loss: 175.3677\n",
      "Epoch 685/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2867 - val_loss: 175.3664\n",
      "Epoch 686/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2776 - val_loss: 175.3783\n",
      "Epoch 687/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2774 - val_loss: 175.3557\n",
      "Epoch 688/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2786 - val_loss: 175.3476\n",
      "Epoch 689/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2909 - val_loss: 175.3618\n",
      "Epoch 690/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2734 - val_loss: 175.3926\n",
      "Epoch 691/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2891 - val_loss: 175.4273\n",
      "Epoch 692/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2734 - val_loss: 175.4070\n",
      "Epoch 693/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2753 - val_loss: 175.3880\n",
      "Epoch 694/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2679 - val_loss: 175.4031\n",
      "Epoch 695/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2651 - val_loss: 175.3999\n",
      "Epoch 696/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2826 - val_loss: 175.4316\n",
      "Epoch 697/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2746 - val_loss: 175.4540\n",
      "Epoch 698/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2656 - val_loss: 175.4334\n",
      "Epoch 699/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2585 - val_loss: 175.4331\n",
      "Epoch 700/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2800 - val_loss: 175.4461\n",
      "Epoch 701/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2682 - val_loss: 175.4471\n",
      "Epoch 702/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2717 - val_loss: 175.4469\n",
      "Epoch 703/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2518 - val_loss: 175.4260\n",
      "Epoch 704/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3027 - val_loss: 175.3743\n",
      "Epoch 705/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2636 - val_loss: 175.3900\n",
      "Epoch 706/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2746 - val_loss: 175.4180\n",
      "Epoch 707/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2572 - val_loss: 175.4405\n",
      "Epoch 708/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2521 - val_loss: 175.4532\n",
      "Epoch 709/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2469 - val_loss: 175.4500\n",
      "Epoch 710/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2691 - val_loss: 175.4653\n",
      "Epoch 711/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2445 - val_loss: 175.4642\n",
      "Epoch 712/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2542 - val_loss: 175.4817\n",
      "Epoch 713/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2481 - val_loss: 175.4875\n",
      "Epoch 714/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2569 - val_loss: 175.5147\n",
      "Epoch 715/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2490 - val_loss: 175.4977\n",
      "Epoch 716/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2609 - val_loss: 175.4928\n",
      "Epoch 717/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2484 - val_loss: 175.5200\n",
      "Epoch 718/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2422 - val_loss: 175.5298\n",
      "Epoch 719/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2636 - val_loss: 175.5650\n",
      "Epoch 720/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2428 - val_loss: 175.5585\n",
      "Epoch 721/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2520 - val_loss: 175.5224\n",
      "Epoch 722/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2505 - val_loss: 175.5228\n",
      "Epoch 723/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2738 - val_loss: 175.5783\n",
      "Epoch 724/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2447 - val_loss: 175.5816\n",
      "Epoch 725/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2288 - val_loss: 175.5970\n",
      "Epoch 726/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2459 - val_loss: 175.6325\n",
      "Epoch 727/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.2296 - val_loss: 175.6268\n",
      "Epoch 728/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2364 - val_loss: 175.6503\n",
      "Epoch 729/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2392 - val_loss: 175.6561\n",
      "Epoch 730/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2369 - val_loss: 175.6639\n",
      "Epoch 731/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2186 - val_loss: 175.6390\n",
      "Epoch 732/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2416 - val_loss: 175.6527\n",
      "Epoch 733/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2270 - val_loss: 175.6071\n",
      "Epoch 734/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2287 - val_loss: 175.6136\n",
      "Epoch 735/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2231 - val_loss: 175.6371\n",
      "Epoch 736/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2191 - val_loss: 175.6156\n",
      "Epoch 737/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2168 - val_loss: 175.6366\n",
      "Epoch 738/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2134 - val_loss: 175.6351\n",
      "Epoch 739/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2248 - val_loss: 175.6100\n",
      "Epoch 740/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2278 - val_loss: 175.6155\n",
      "Epoch 741/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2274 - val_loss: 175.6516\n",
      "Epoch 742/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2329 - val_loss: 175.6387\n",
      "Epoch 743/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2197 - val_loss: 175.6616\n",
      "Epoch 744/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2073 - val_loss: 175.6889\n",
      "Epoch 745/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2151 - val_loss: 175.7061\n",
      "Epoch 746/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2061 - val_loss: 175.7150\n",
      "Epoch 747/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2153 - val_loss: 175.7439\n",
      "Epoch 748/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2437 - val_loss: 175.7684\n",
      "Epoch 749/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2022 - val_loss: 175.7778\n",
      "Epoch 750/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2702 - val_loss: 175.7176\n",
      ">Saved ./models/comp4948_a2_model_1.h5\n",
      "Epoch 1/750\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 4538.8706 - val_loss: 4535.5688\n",
      "Epoch 2/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4532.7109 - val_loss: 4529.5425\n",
      "Epoch 3/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4526.7627 - val_loss: 4523.6045\n",
      "Epoch 4/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4520.7666 - val_loss: 4517.7500\n",
      "Epoch 5/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4515.0073 - val_loss: 4511.9067\n",
      "Epoch 6/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4509.1396 - val_loss: 4506.0269\n",
      "Epoch 7/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4503.2124 - val_loss: 4500.0786\n",
      "Epoch 8/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4497.2778 - val_loss: 4494.1641\n",
      "Epoch 9/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4491.3667 - val_loss: 4488.1479\n",
      "Epoch 10/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4485.2349 - val_loss: 4482.0435\n",
      "Epoch 11/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4479.1387 - val_loss: 4475.7690\n",
      "Epoch 12/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4472.8325 - val_loss: 4469.4258\n",
      "Epoch 13/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4466.3911 - val_loss: 4462.9346\n",
      "Epoch 14/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4459.8311 - val_loss: 4456.1904\n",
      "Epoch 15/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4453.1396 - val_loss: 4449.6714\n",
      "Epoch 16/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4446.5459 - val_loss: 4442.9766\n",
      "Epoch 17/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4439.7793 - val_loss: 4436.0552\n",
      "Epoch 18/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4432.8032 - val_loss: 4429.0127\n",
      "Epoch 19/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4425.6890 - val_loss: 4421.7651\n",
      "Epoch 20/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4418.3745 - val_loss: 4414.2847\n",
      "Epoch 21/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4411.0327 - val_loss: 4407.0737\n",
      "Epoch 22/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4403.6558 - val_loss: 4399.6753\n",
      "Epoch 23/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4396.2798 - val_loss: 4392.0825\n",
      "Epoch 24/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4388.5474 - val_loss: 4384.2896\n",
      "Epoch 25/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4380.7349 - val_loss: 4376.2612\n",
      "Epoch 26/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4372.5620 - val_loss: 4368.0371\n",
      "Epoch 27/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4364.2188 - val_loss: 4359.6108\n",
      "Epoch 28/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4355.8120 - val_loss: 4351.1270\n",
      "Epoch 29/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4347.3188 - val_loss: 4342.5576\n",
      "Epoch 30/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4338.6411 - val_loss: 4333.8354\n",
      "Epoch 31/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4329.8188 - val_loss: 4324.9229\n",
      "Epoch 32/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4321.1157 - val_loss: 4316.2334\n",
      "Epoch 33/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4312.4033 - val_loss: 4307.2695\n",
      "Epoch 34/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4303.3223 - val_loss: 4298.0996\n",
      "Epoch 35/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4294.1284 - val_loss: 4288.7031\n",
      "Epoch 36/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4284.5820 - val_loss: 4279.1548\n",
      "Epoch 37/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4274.9243 - val_loss: 4269.4102\n",
      "Epoch 38/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4265.1396 - val_loss: 4259.5254\n",
      "Epoch 39/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4255.1719 - val_loss: 4249.3447\n",
      "Epoch 40/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4245.0620 - val_loss: 4239.0566\n",
      "Epoch 41/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4234.6724 - val_loss: 4228.5938\n",
      "Epoch 42/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4224.0625 - val_loss: 4217.9922\n",
      "Epoch 43/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4213.4893 - val_loss: 4207.3628\n",
      "Epoch 44/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4202.8062 - val_loss: 4196.7808\n",
      "Epoch 45/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4192.3672 - val_loss: 4185.8799\n",
      "Epoch 46/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4181.1895 - val_loss: 4174.8604\n",
      "Epoch 47/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4170.1597 - val_loss: 4163.6245\n",
      "Epoch 48/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4158.8682 - val_loss: 4152.3535\n",
      "Epoch 49/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4147.5200 - val_loss: 4141.2183\n",
      "Epoch 50/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4136.5298 - val_loss: 4130.0078\n",
      "Epoch 51/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4125.2969 - val_loss: 4118.4897\n",
      "Epoch 52/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4113.5430 - val_loss: 4106.7295\n",
      "Epoch 53/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4101.6772 - val_loss: 4094.5945\n",
      "Epoch 54/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4089.5967 - val_loss: 4082.2344\n",
      "Epoch 55/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4077.2937 - val_loss: 4069.8352\n",
      "Epoch 56/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4064.7253 - val_loss: 4057.5193\n",
      "Epoch 57/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4052.5593 - val_loss: 4044.7913\n",
      "Epoch 58/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4039.6680 - val_loss: 4031.9697\n",
      "Epoch 59/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4026.9819 - val_loss: 4019.2617\n",
      "Epoch 60/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4014.0410 - val_loss: 4006.3799\n",
      "Epoch 61/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4001.2773 - val_loss: 3993.2771\n",
      "Epoch 62/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3988.0579 - val_loss: 3979.8608\n",
      "Epoch 63/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3974.4182 - val_loss: 3966.4163\n",
      "Epoch 64/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3961.0625 - val_loss: 3952.8367\n",
      "Epoch 65/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3947.4360 - val_loss: 3938.9148\n",
      "Epoch 66/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3933.5081 - val_loss: 3924.9775\n",
      "Epoch 67/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3919.5200 - val_loss: 3910.8040\n",
      "Epoch 68/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3905.2627 - val_loss: 3896.4619\n",
      "Epoch 69/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3891.0229 - val_loss: 3882.0342\n",
      "Epoch 70/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3876.4229 - val_loss: 3867.5852\n",
      "Epoch 71/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3861.9880 - val_loss: 3853.0076\n",
      "Epoch 72/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3847.5051 - val_loss: 3838.4990\n",
      "Epoch 73/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3832.9858 - val_loss: 3823.7830\n",
      "Epoch 74/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3818.0779 - val_loss: 3808.7944\n",
      "Epoch 75/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3802.9209 - val_loss: 3793.5459\n",
      "Epoch 76/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3787.5173 - val_loss: 3777.9556\n",
      "Epoch 77/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3771.9570 - val_loss: 3762.0437\n",
      "Epoch 78/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3755.8750 - val_loss: 3746.3467\n",
      "Epoch 79/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3740.5627 - val_loss: 3730.9109\n",
      "Epoch 80/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3724.9033 - val_loss: 3715.2351\n",
      "Epoch 81/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3709.0779 - val_loss: 3699.3047\n",
      "Epoch 82/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3692.9783 - val_loss: 3683.1533\n",
      "Epoch 83/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3677.0210 - val_loss: 3667.1467\n",
      "Epoch 84/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3661.0029 - val_loss: 3650.8110\n",
      "Epoch 85/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3644.4556 - val_loss: 3634.1853\n",
      "Epoch 86/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3627.8027 - val_loss: 3617.3635\n",
      "Epoch 87/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3610.7993 - val_loss: 3600.3296\n",
      "Epoch 88/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3593.8699 - val_loss: 3583.2109\n",
      "Epoch 89/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3576.6182 - val_loss: 3565.9070\n",
      "Epoch 90/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3559.4956 - val_loss: 3548.6108\n",
      "Epoch 91/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3542.0984 - val_loss: 3531.0952\n",
      "Epoch 92/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3524.7144 - val_loss: 3513.7336\n",
      "Epoch 93/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3507.3257 - val_loss: 3496.1338\n",
      "Epoch 94/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3489.5439 - val_loss: 3478.2585\n",
      "Epoch 95/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3471.6265 - val_loss: 3460.0103\n",
      "Epoch 96/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3453.4377 - val_loss: 3441.7773\n",
      "Epoch 97/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3435.3577 - val_loss: 3423.5505\n",
      "Epoch 98/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3416.9990 - val_loss: 3405.1086\n",
      "Epoch 99/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3398.4575 - val_loss: 3387.0120\n",
      "Epoch 100/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3380.4712 - val_loss: 3368.8625\n",
      "Epoch 101/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3362.3455 - val_loss: 3350.3467\n",
      "Epoch 102/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3343.7500 - val_loss: 3331.5959\n",
      "Epoch 103/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3325.1116 - val_loss: 3312.6565\n",
      "Epoch 104/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3306.3958 - val_loss: 3293.8303\n",
      "Epoch 105/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3287.3157 - val_loss: 3274.8694\n",
      "Epoch 106/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3268.2388 - val_loss: 3255.5198\n",
      "Epoch 107/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3248.9128 - val_loss: 3235.9338\n",
      "Epoch 108/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3229.4106 - val_loss: 3216.5283\n",
      "Epoch 109/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3210.4087 - val_loss: 3197.9414\n",
      "Epoch 110/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3192.0864 - val_loss: 3179.1616\n",
      "Epoch 111/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3173.2219 - val_loss: 3160.1328\n",
      "Epoch 112/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3153.9258 - val_loss: 3140.8960\n",
      "Epoch 113/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3134.7397 - val_loss: 3121.2783\n",
      "Epoch 114/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3115.0500 - val_loss: 3101.3757\n",
      "Epoch 115/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3094.9346 - val_loss: 3081.2795\n",
      "Epoch 116/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3074.9314 - val_loss: 3060.8523\n",
      "Epoch 117/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3054.4688 - val_loss: 3040.2542\n",
      "Epoch 118/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3033.7900 - val_loss: 3019.3149\n",
      "Epoch 119/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3012.6543 - val_loss: 2998.4241\n",
      "Epoch 120/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2992.1511 - val_loss: 2977.4131\n",
      "Epoch 121/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2971.1353 - val_loss: 2956.1997\n",
      "Epoch 122/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2950.2056 - val_loss: 2935.4541\n",
      "Epoch 123/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2929.7571 - val_loss: 2915.2629\n",
      "Epoch 124/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2909.6360 - val_loss: 2895.0173\n",
      "Epoch 125/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2889.3821 - val_loss: 2874.4331\n",
      "Epoch 126/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2868.5884 - val_loss: 2853.6887\n",
      "Epoch 127/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2848.4380 - val_loss: 2832.9609\n",
      "Epoch 128/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2827.3452 - val_loss: 2812.3110\n",
      "Epoch 129/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2806.7588 - val_loss: 2791.2712\n",
      "Epoch 130/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2785.9548 - val_loss: 2769.8767\n",
      "Epoch 131/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2764.3391 - val_loss: 2748.6094\n",
      "Epoch 132/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2743.1218 - val_loss: 2727.2625\n",
      "Epoch 133/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2721.7097 - val_loss: 2705.6934\n",
      "Epoch 134/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2700.1821 - val_loss: 2683.8733\n",
      "Epoch 135/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2678.6865 - val_loss: 2662.2517\n",
      "Epoch 136/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2657.1108 - val_loss: 2640.4978\n",
      "Epoch 137/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2635.1721 - val_loss: 2618.7937\n",
      "Epoch 138/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2613.5527 - val_loss: 2597.0012\n",
      "Epoch 139/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2592.0532 - val_loss: 2575.1758\n",
      "Epoch 140/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2570.1226 - val_loss: 2553.3186\n",
      "Epoch 141/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2548.2114 - val_loss: 2531.3691\n",
      "Epoch 142/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2526.7009 - val_loss: 2509.4958\n",
      "Epoch 143/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2504.9734 - val_loss: 2487.4221\n",
      "Epoch 144/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2482.8005 - val_loss: 2465.3110\n",
      "Epoch 145/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2460.5737 - val_loss: 2443.0515\n",
      "Epoch 146/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2438.6235 - val_loss: 2420.3564\n",
      "Epoch 147/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2415.7502 - val_loss: 2397.7949\n",
      "Epoch 148/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2393.6716 - val_loss: 2375.2412\n",
      "Epoch 149/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2371.2212 - val_loss: 2352.5212\n",
      "Epoch 150/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2348.4639 - val_loss: 2329.6360\n",
      "Epoch 151/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2325.4841 - val_loss: 2306.6255\n",
      "Epoch 152/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2302.6067 - val_loss: 2283.7625\n",
      "Epoch 153/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2279.8086 - val_loss: 2261.1060\n",
      "Epoch 154/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2257.6758 - val_loss: 2238.4487\n",
      "Epoch 155/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2235.2834 - val_loss: 2215.7754\n",
      "Epoch 156/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2212.9395 - val_loss: 2192.9697\n",
      "Epoch 157/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2190.1033 - val_loss: 2170.1853\n",
      "Epoch 158/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2167.5686 - val_loss: 2147.2661\n",
      "Epoch 159/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2144.8953 - val_loss: 2124.3730\n",
      "Epoch 160/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2122.1326 - val_loss: 2101.3467\n",
      "Epoch 161/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2099.2212 - val_loss: 2078.2314\n",
      "Epoch 162/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2076.3347 - val_loss: 2055.0286\n",
      "Epoch 163/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2053.4194 - val_loss: 2031.8478\n",
      "Epoch 164/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2030.3173 - val_loss: 2008.5919\n",
      "Epoch 165/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2007.3828 - val_loss: 1985.7383\n",
      "Epoch 166/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1985.2090 - val_loss: 1962.9895\n",
      "Epoch 167/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1962.3156 - val_loss: 1940.3594\n",
      "Epoch 168/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1940.3361 - val_loss: 1917.7020\n",
      "Epoch 169/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1917.6991 - val_loss: 1895.2758\n",
      "Epoch 170/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1895.5730 - val_loss: 1872.6946\n",
      "Epoch 171/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1872.9215 - val_loss: 1850.3344\n",
      "Epoch 172/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1850.9022 - val_loss: 1827.7727\n",
      "Epoch 173/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1829.0090 - val_loss: 1805.3246\n",
      "Epoch 174/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1806.5367 - val_loss: 1783.0980\n",
      "Epoch 175/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1784.2856 - val_loss: 1760.9274\n",
      "Epoch 176/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1762.0963 - val_loss: 1738.7008\n",
      "Epoch 177/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1740.2943 - val_loss: 1716.2058\n",
      "Epoch 178/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1718.0190 - val_loss: 1693.6774\n",
      "Epoch 179/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1695.5703 - val_loss: 1671.2031\n",
      "Epoch 180/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1673.3574 - val_loss: 1648.6796\n",
      "Epoch 181/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1651.1626 - val_loss: 1626.2065\n",
      "Epoch 182/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1628.9569 - val_loss: 1604.4269\n",
      "Epoch 183/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1607.0588 - val_loss: 1582.6831\n",
      "Epoch 184/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1585.5382 - val_loss: 1561.0216\n",
      "Epoch 185/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1563.9260 - val_loss: 1539.3972\n",
      "Epoch 186/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1542.6295 - val_loss: 1517.6750\n",
      "Epoch 187/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1520.9409 - val_loss: 1496.2605\n",
      "Epoch 188/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1500.0371 - val_loss: 1474.8887\n",
      "Epoch 189/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1479.0471 - val_loss: 1453.4449\n",
      "Epoch 190/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1457.4906 - val_loss: 1432.2618\n",
      "Epoch 191/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1436.8016 - val_loss: 1410.8322\n",
      "Epoch 192/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1415.6318 - val_loss: 1389.5492\n",
      "Epoch 193/750\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1394.6720 - val_loss: 1368.2958\n",
      "Epoch 194/750\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 1373.6932 - val_loss: 1347.1578\n",
      "Epoch 195/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1352.3961 - val_loss: 1326.2997\n",
      "Epoch 196/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1332.0704 - val_loss: 1305.5032\n",
      "Epoch 197/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 1311.5981 - val_loss: 1284.8075\n",
      "Epoch 198/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1290.6975 - val_loss: 1264.8020\n",
      "Epoch 199/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1271.6757 - val_loss: 1245.0417\n",
      "Epoch 200/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1252.3356 - val_loss: 1225.3828\n",
      "Epoch 201/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1233.1180 - val_loss: 1205.7917\n",
      "Epoch 202/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1213.9443 - val_loss: 1186.3495\n",
      "Epoch 203/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1194.5056 - val_loss: 1167.2509\n",
      "Epoch 204/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1175.6859 - val_loss: 1148.1367\n",
      "Epoch 205/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1157.0309 - val_loss: 1129.0931\n",
      "Epoch 206/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1138.4590 - val_loss: 1110.0822\n",
      "Epoch 207/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1119.8452 - val_loss: 1091.1669\n",
      "Epoch 208/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1101.1163 - val_loss: 1072.5206\n",
      "Epoch 209/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1082.7883 - val_loss: 1053.9429\n",
      "Epoch 210/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1064.7572 - val_loss: 1035.3135\n",
      "Epoch 211/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1046.4796 - val_loss: 1016.9758\n",
      "Epoch 212/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1028.4884 - val_loss: 998.7766\n",
      "Epoch 213/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1010.8748 - val_loss: 980.5793\n",
      "Epoch 214/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 992.6377 - val_loss: 962.8401\n",
      "Epoch 215/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 975.2520 - val_loss: 945.0980\n",
      "Epoch 216/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 958.0364 - val_loss: 927.2894\n",
      "Epoch 217/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 940.3780 - val_loss: 909.9396\n",
      "Epoch 218/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 923.3268 - val_loss: 892.7598\n",
      "Epoch 219/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 906.6891 - val_loss: 875.4949\n",
      "Epoch 220/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 889.8274 - val_loss: 858.4732\n",
      "Epoch 221/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 873.4252 - val_loss: 841.5510\n",
      "Epoch 222/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 856.7017 - val_loss: 825.0165\n",
      "Epoch 223/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 840.5775 - val_loss: 808.5696\n",
      "Epoch 224/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 824.6027 - val_loss: 792.3176\n",
      "Epoch 225/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 808.8873 - val_loss: 776.2347\n",
      "Epoch 226/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 793.0454 - val_loss: 760.5199\n",
      "Epoch 227/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 777.8665 - val_loss: 744.8398\n",
      "Epoch 228/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 762.3236 - val_loss: 729.5529\n",
      "Epoch 229/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 747.5015 - val_loss: 714.3475\n",
      "Epoch 230/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 732.9662 - val_loss: 699.1914\n",
      "Epoch 231/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 718.5392 - val_loss: 684.1349\n",
      "Epoch 232/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 703.9380 - val_loss: 669.5367\n",
      "Epoch 233/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 689.6456 - val_loss: 655.3754\n",
      "Epoch 234/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 675.9319 - val_loss: 641.4291\n",
      "Epoch 235/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 662.2199 - val_loss: 627.7707\n",
      "Epoch 236/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 649.1459 - val_loss: 614.1803\n",
      "Epoch 237/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 635.9821 - val_loss: 600.8110\n",
      "Epoch 238/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 623.1193 - val_loss: 587.6226\n",
      "Epoch 239/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 610.1604 - val_loss: 574.8645\n",
      "Epoch 240/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 598.1129 - val_loss: 562.1455\n",
      "Epoch 241/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 585.5283 - val_loss: 549.9992\n",
      "Epoch 242/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 573.9128 - val_loss: 537.8790\n",
      "Epoch 243/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 562.3160 - val_loss: 526.0045\n",
      "Epoch 244/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 550.9014 - val_loss: 514.4238\n",
      "Epoch 245/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 539.8668 - val_loss: 503.0446\n",
      "Epoch 246/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 528.8698 - val_loss: 491.9870\n",
      "Epoch 247/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 518.4492 - val_loss: 481.0139\n",
      "Epoch 248/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 507.5898 - val_loss: 470.5460\n",
      "Epoch 249/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 497.2480 - val_loss: 460.3834\n",
      "Epoch 250/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 487.7270 - val_loss: 450.1713\n",
      "Epoch 251/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 478.3105 - val_loss: 440.0683\n",
      "Epoch 252/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 468.6566 - val_loss: 430.4395\n",
      "Epoch 253/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 459.2782 - val_loss: 421.1861\n",
      "Epoch 254/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 450.3212 - val_loss: 412.1320\n",
      "Epoch 255/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 441.8849 - val_loss: 403.1459\n",
      "Epoch 256/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 433.3310 - val_loss: 394.4750\n",
      "Epoch 257/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 424.8140 - val_loss: 386.1960\n",
      "Epoch 258/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 417.3005 - val_loss: 377.8414\n",
      "Epoch 259/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 409.1031 - val_loss: 370.0237\n",
      "Epoch 260/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 401.9699 - val_loss: 362.2272\n",
      "Epoch 261/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 394.3792 - val_loss: 354.8480\n",
      "Epoch 262/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 387.6245 - val_loss: 347.5146\n",
      "Epoch 263/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 380.3495 - val_loss: 340.6274\n",
      "Epoch 264/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 374.3942 - val_loss: 333.5952\n",
      "Epoch 265/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 367.6115 - val_loss: 327.0073\n",
      "Epoch 266/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 361.3549 - val_loss: 320.7280\n",
      "Epoch 267/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 355.3412 - val_loss: 314.6523\n",
      "Epoch 268/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 349.7386 - val_loss: 308.6104\n",
      "Epoch 269/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 344.1867 - val_loss: 302.7477\n",
      "Epoch 270/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 338.6107 - val_loss: 297.2386\n",
      "Epoch 271/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 333.3002 - val_loss: 291.9688\n",
      "Epoch 272/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 328.4978 - val_loss: 286.7592\n",
      "Epoch 273/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 323.7650 - val_loss: 281.7257\n",
      "Epoch 274/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 319.0315 - val_loss: 276.9423\n",
      "Epoch 275/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 314.4073 - val_loss: 272.4210\n",
      "Epoch 276/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 310.2708 - val_loss: 267.9358\n",
      "Epoch 277/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 306.1724 - val_loss: 263.5914\n",
      "Epoch 278/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 302.3255 - val_loss: 259.3677\n",
      "Epoch 279/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 298.4230 - val_loss: 255.3686\n",
      "Epoch 280/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 294.6267 - val_loss: 251.6216\n",
      "Epoch 281/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 291.3327 - val_loss: 247.8544\n",
      "Epoch 282/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 287.8113 - val_loss: 244.3401\n",
      "Epoch 283/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 284.7845 - val_loss: 240.8651\n",
      "Epoch 284/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 281.6320 - val_loss: 237.6243\n",
      "Epoch 285/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 278.6319 - val_loss: 234.5828\n",
      "Epoch 286/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 275.9728 - val_loss: 231.5833\n",
      "Epoch 287/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 273.1859 - val_loss: 228.7901\n",
      "Epoch 288/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 270.6004 - val_loss: 226.1440\n",
      "Epoch 289/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 268.3625 - val_loss: 223.5560\n",
      "Epoch 290/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 266.0197 - val_loss: 221.1378\n",
      "Epoch 291/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 263.8834 - val_loss: 218.7984\n",
      "Epoch 292/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 261.7996 - val_loss: 216.6094\n",
      "Epoch 293/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 259.8082 - val_loss: 214.5006\n",
      "Epoch 294/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 257.8599 - val_loss: 212.5138\n",
      "Epoch 295/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 256.0709 - val_loss: 210.5955\n",
      "Epoch 296/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 254.5072 - val_loss: 208.6521\n",
      "Epoch 297/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 252.8918 - val_loss: 206.8021\n",
      "Epoch 298/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 251.1895 - val_loss: 205.1464\n",
      "Epoch 299/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 249.6803 - val_loss: 203.5820\n",
      "Epoch 300/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 248.5797 - val_loss: 201.9123\n",
      "Epoch 301/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 247.1956 - val_loss: 200.3760\n",
      "Epoch 302/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 245.7183 - val_loss: 199.0810\n",
      "Epoch 303/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 244.5910 - val_loss: 197.7935\n",
      "Epoch 304/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 243.4449 - val_loss: 196.5828\n",
      "Epoch 305/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 242.4741 - val_loss: 195.3544\n",
      "Epoch 306/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 241.5942 - val_loss: 194.1602\n",
      "Epoch 307/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 240.3880 - val_loss: 193.1844\n",
      "Epoch 308/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 239.5449 - val_loss: 192.1961\n",
      "Epoch 309/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 238.8685 - val_loss: 191.1523\n",
      "Epoch 310/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 237.9291 - val_loss: 190.2619\n",
      "Epoch 311/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 237.2104 - val_loss: 189.3809\n",
      "Epoch 312/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 236.4732 - val_loss: 188.5680\n",
      "Epoch 313/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 235.9548 - val_loss: 187.7054\n",
      "Epoch 314/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 235.1069 - val_loss: 187.0355\n",
      "Epoch 315/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 234.5609 - val_loss: 186.3596\n",
      "Epoch 316/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 234.0798 - val_loss: 185.6616\n",
      "Epoch 317/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 233.4778 - val_loss: 185.0543\n",
      "Epoch 318/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 232.9889 - val_loss: 184.4782\n",
      "Epoch 319/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232.5586 - val_loss: 183.8738\n",
      "Epoch 320/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 232.0525 - val_loss: 183.3505\n",
      "Epoch 321/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 231.7464 - val_loss: 182.7818\n",
      "Epoch 322/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 231.1890 - val_loss: 182.3738\n",
      "Epoch 323/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.9373 - val_loss: 181.8704\n",
      "Epoch 324/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.5647 - val_loss: 181.4131\n",
      "Epoch 325/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.1803 - val_loss: 180.9732\n",
      "Epoch 326/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.8501 - val_loss: 180.5600\n",
      "Epoch 327/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.5603 - val_loss: 180.1782\n",
      "Epoch 328/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.2672 - val_loss: 179.8461\n",
      "Epoch 329/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.9840 - val_loss: 179.5574\n",
      "Epoch 330/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.7886 - val_loss: 179.2378\n",
      "Epoch 331/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.5835 - val_loss: 178.9399\n",
      "Epoch 332/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.3317 - val_loss: 178.6989\n",
      "Epoch 333/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.1481 - val_loss: 178.4698\n",
      "Epoch 334/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.9820 - val_loss: 178.2262\n",
      "Epoch 335/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.8182 - val_loss: 177.9656\n",
      "Epoch 336/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.6280 - val_loss: 177.7446\n",
      "Epoch 337/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.4508 - val_loss: 177.5595\n",
      "Epoch 338/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.3025 - val_loss: 177.3771\n",
      "Epoch 339/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.1418 - val_loss: 177.2197\n",
      "Epoch 340/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.0439 - val_loss: 176.9934\n",
      "Epoch 341/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.8908 - val_loss: 176.8182\n",
      "Epoch 342/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.7651 - val_loss: 176.6552\n",
      "Epoch 343/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.6463 - val_loss: 176.4834\n",
      "Epoch 344/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.5240 - val_loss: 176.3164\n",
      "Epoch 345/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.4360 - val_loss: 176.1513\n",
      "Epoch 346/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.3099 - val_loss: 176.0169\n",
      "Epoch 347/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.2343 - val_loss: 175.8908\n",
      "Epoch 348/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.1151 - val_loss: 175.8000\n",
      "Epoch 349/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.0457 - val_loss: 175.6795\n",
      "Epoch 350/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.9507 - val_loss: 175.5739\n",
      "Epoch 351/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.8756 - val_loss: 175.4495\n",
      "Epoch 352/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.8468 - val_loss: 175.3166\n",
      "Epoch 353/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.7286 - val_loss: 175.2332\n",
      "Epoch 354/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.6579 - val_loss: 175.1420\n",
      "Epoch 355/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.5813 - val_loss: 175.0854\n",
      "Epoch 356/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.5537 - val_loss: 174.9731\n",
      "Epoch 357/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.4902 - val_loss: 174.8910\n",
      "Epoch 358/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.4064 - val_loss: 174.8605\n",
      "Epoch 359/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.3737 - val_loss: 174.8044\n",
      "Epoch 360/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.3010 - val_loss: 174.7365\n",
      "Epoch 361/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.2552 - val_loss: 174.6683\n",
      "Epoch 362/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.1884 - val_loss: 174.6168\n",
      "Epoch 363/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.1441 - val_loss: 174.5812\n",
      "Epoch 364/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.0866 - val_loss: 174.5245\n",
      "Epoch 365/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.0253 - val_loss: 174.4773\n",
      "Epoch 366/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.9698 - val_loss: 174.4147\n",
      "Epoch 367/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.9521 - val_loss: 174.3286\n",
      "Epoch 368/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.8769 - val_loss: 174.2883\n",
      "Epoch 369/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.8251 - val_loss: 174.2365\n",
      "Epoch 370/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.8047 - val_loss: 174.1769\n",
      "Epoch 371/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.7366 - val_loss: 174.1682\n",
      "Epoch 372/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.6851 - val_loss: 174.1188\n",
      "Epoch 373/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.6604 - val_loss: 174.1041\n",
      "Epoch 374/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.6012 - val_loss: 174.0671\n",
      "Epoch 375/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5666 - val_loss: 174.0266\n",
      "Epoch 376/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5176 - val_loss: 173.9916\n",
      "Epoch 377/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.4788 - val_loss: 173.9595\n",
      "Epoch 378/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.4334 - val_loss: 173.9310\n",
      "Epoch 379/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3997 - val_loss: 173.9036\n",
      "Epoch 380/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3463 - val_loss: 173.8813\n",
      "Epoch 381/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3258 - val_loss: 173.8399\n",
      "Epoch 382/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.2787 - val_loss: 173.8317\n",
      "Epoch 383/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2673 - val_loss: 173.8264\n",
      "Epoch 384/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.1973 - val_loss: 173.7934\n",
      "Epoch 385/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.1690 - val_loss: 173.7428\n",
      "Epoch 386/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1361 - val_loss: 173.7342\n",
      "Epoch 387/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.0928 - val_loss: 173.6912\n",
      "Epoch 388/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0625 - val_loss: 173.6747\n",
      "Epoch 389/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.0236 - val_loss: 173.6702\n",
      "Epoch 390/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.9822 - val_loss: 173.6465\n",
      "Epoch 391/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9475 - val_loss: 173.6237\n",
      "Epoch 392/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9199 - val_loss: 173.5999\n",
      "Epoch 393/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8789 - val_loss: 173.5690\n",
      "Epoch 394/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8498 - val_loss: 173.5343\n",
      "Epoch 395/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8107 - val_loss: 173.5308\n",
      "Epoch 396/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7762 - val_loss: 173.5272\n",
      "Epoch 397/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7851 - val_loss: 173.5028\n",
      "Epoch 398/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.7030 - val_loss: 173.5092\n",
      "Epoch 399/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6881 - val_loss: 173.4924\n",
      "Epoch 400/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6531 - val_loss: 173.4905\n",
      "Epoch 401/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6149 - val_loss: 173.4968\n",
      "Epoch 402/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6007 - val_loss: 173.5060\n",
      "Epoch 403/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.5537 - val_loss: 173.4867\n",
      "Epoch 404/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.5264 - val_loss: 173.4605\n",
      "Epoch 405/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4861 - val_loss: 173.4538\n",
      "Epoch 406/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4646 - val_loss: 173.4289\n",
      "Epoch 407/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4329 - val_loss: 173.4253\n",
      "Epoch 408/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4068 - val_loss: 173.4047\n",
      "Epoch 409/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3865 - val_loss: 173.3864\n",
      "Epoch 410/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3397 - val_loss: 173.3783\n",
      "Epoch 411/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3384 - val_loss: 173.3880\n",
      "Epoch 412/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2778 - val_loss: 173.4037\n",
      "Epoch 413/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2625 - val_loss: 173.4181\n",
      "Epoch 414/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2305 - val_loss: 173.4123\n",
      "Epoch 415/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2117 - val_loss: 173.3913\n",
      "Epoch 416/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1634 - val_loss: 173.3842\n",
      "Epoch 417/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1358 - val_loss: 173.3693\n",
      "Epoch 418/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1196 - val_loss: 173.3653\n",
      "Epoch 419/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 223.0855 - val_loss: 173.3621\n",
      "Epoch 420/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 223.0521 - val_loss: 173.3631\n",
      "Epoch 421/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0729 - val_loss: 173.3901\n",
      "Epoch 422/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0219 - val_loss: 173.4002\n",
      "Epoch 423/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9889 - val_loss: 173.4086\n",
      "Epoch 424/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9579 - val_loss: 173.3636\n",
      "Epoch 425/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9314 - val_loss: 173.3401\n",
      "Epoch 426/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9233 - val_loss: 173.3292\n",
      "Epoch 427/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8883 - val_loss: 173.3215\n",
      "Epoch 428/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8640 - val_loss: 173.3024\n",
      "Epoch 429/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8439 - val_loss: 173.2814\n",
      "Epoch 430/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8794 - val_loss: 173.2257\n",
      "Epoch 431/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7931 - val_loss: 173.2360\n",
      "Epoch 432/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7921 - val_loss: 173.2313\n",
      "Epoch 433/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7652 - val_loss: 173.2554\n",
      "Epoch 434/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7472 - val_loss: 173.2332\n",
      "Epoch 435/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7294 - val_loss: 173.2381\n",
      "Epoch 436/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7139 - val_loss: 173.2347\n",
      "Epoch 437/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6736 - val_loss: 173.2291\n",
      "Epoch 438/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6518 - val_loss: 173.2429\n",
      "Epoch 439/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6623 - val_loss: 173.2460\n",
      "Epoch 440/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.6522 - val_loss: 173.2352\n",
      "Epoch 441/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5963 - val_loss: 173.2234\n",
      "Epoch 442/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6258 - val_loss: 173.2616\n",
      "Epoch 443/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5551 - val_loss: 173.2567\n",
      "Epoch 444/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5370 - val_loss: 173.2603\n",
      "Epoch 445/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5282 - val_loss: 173.2654\n",
      "Epoch 446/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5022 - val_loss: 173.2607\n",
      "Epoch 447/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5126 - val_loss: 173.2329\n",
      "Epoch 448/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4725 - val_loss: 173.2479\n",
      "Epoch 449/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4648 - val_loss: 173.2691\n",
      "Epoch 450/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4367 - val_loss: 173.2551\n",
      "Epoch 451/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4230 - val_loss: 173.2660\n",
      "Epoch 452/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3956 - val_loss: 173.2586\n",
      "Epoch 453/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3837 - val_loss: 173.2300\n",
      "Epoch 454/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3889 - val_loss: 173.2063\n",
      "Epoch 455/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3544 - val_loss: 173.2333\n",
      "Epoch 456/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3655 - val_loss: 173.2501\n",
      "Epoch 457/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3326 - val_loss: 173.2295\n",
      "Epoch 458/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3499 - val_loss: 173.1860\n",
      "Epoch 459/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3265 - val_loss: 173.2282\n",
      "Epoch 460/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2836 - val_loss: 173.2355\n",
      "Epoch 461/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2848 - val_loss: 173.2220\n",
      "Epoch 462/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2603 - val_loss: 173.2202\n",
      "Epoch 463/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2379 - val_loss: 173.2316\n",
      "Epoch 464/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2407 - val_loss: 173.2322\n",
      "Epoch 465/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2332 - val_loss: 173.2563\n",
      "Epoch 466/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2195 - val_loss: 173.2530\n",
      "Epoch 467/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1783 - val_loss: 173.2671\n",
      "Epoch 468/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1685 - val_loss: 173.2888\n",
      "Epoch 469/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1459 - val_loss: 173.2790\n",
      "Epoch 470/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1508 - val_loss: 173.2621\n",
      "Epoch 471/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1354 - val_loss: 173.2757\n",
      "Epoch 472/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1134 - val_loss: 173.3212\n",
      "Epoch 473/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1070 - val_loss: 173.3264\n",
      "Epoch 474/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0817 - val_loss: 173.3641\n",
      "Epoch 475/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1481 - val_loss: 173.3255\n",
      "Epoch 476/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0634 - val_loss: 173.3767\n",
      "Epoch 477/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0675 - val_loss: 173.4114\n",
      "Epoch 478/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0309 - val_loss: 173.4471\n",
      "Epoch 479/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0578 - val_loss: 173.4729\n",
      "Epoch 480/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0385 - val_loss: 173.4406\n",
      "Epoch 481/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0287 - val_loss: 173.4396\n",
      "Epoch 482/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0024 - val_loss: 173.4937\n",
      "Epoch 483/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9925 - val_loss: 173.5045\n",
      "Epoch 484/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9789 - val_loss: 173.5441\n",
      "Epoch 485/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0359 - val_loss: 173.6117\n",
      "Epoch 486/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9534 - val_loss: 173.6115\n",
      "Epoch 487/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9651 - val_loss: 173.6088\n",
      "Epoch 488/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9679 - val_loss: 173.6323\n",
      "Epoch 489/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9237 - val_loss: 173.6212\n",
      "Epoch 490/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9302 - val_loss: 173.5973\n",
      "Epoch 491/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9330 - val_loss: 173.5822\n",
      "Epoch 492/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.9029 - val_loss: 173.6123\n",
      "Epoch 493/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9023 - val_loss: 173.6050\n",
      "Epoch 494/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9008 - val_loss: 173.5963\n",
      "Epoch 495/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.8905 - val_loss: 173.6399\n",
      "Epoch 496/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8757 - val_loss: 173.6411\n",
      "Epoch 497/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8592 - val_loss: 173.6447\n",
      "Epoch 498/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8506 - val_loss: 173.6654\n",
      "Epoch 499/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8603 - val_loss: 173.6864\n",
      "Epoch 500/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8392 - val_loss: 173.6779\n",
      "Epoch 501/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.8322 - val_loss: 173.6930\n",
      "Epoch 502/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.8464 - val_loss: 173.7117\n",
      "Epoch 503/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8194 - val_loss: 173.6818\n",
      "Epoch 504/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8174 - val_loss: 173.7168\n",
      "Epoch 505/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7956 - val_loss: 173.7076\n",
      "Epoch 506/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7855 - val_loss: 173.6985\n",
      "Epoch 507/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.7917 - val_loss: 173.6940\n",
      "Epoch 508/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7823 - val_loss: 173.7347\n",
      "Epoch 509/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7720 - val_loss: 173.7572\n",
      "Epoch 510/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7813 - val_loss: 173.7534\n",
      "Epoch 511/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7713 - val_loss: 173.7601\n",
      "Epoch 512/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7787 - val_loss: 173.7642\n",
      "Epoch 513/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7623 - val_loss: 173.7937\n",
      "Epoch 514/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7639 - val_loss: 173.7833\n",
      "Epoch 515/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7384 - val_loss: 173.7952\n",
      "Epoch 516/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7555 - val_loss: 173.8524\n",
      "Epoch 517/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7053 - val_loss: 173.8428\n",
      "Epoch 518/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7187 - val_loss: 173.8466\n",
      "Epoch 519/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7209 - val_loss: 173.8077\n",
      "Epoch 520/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7064 - val_loss: 173.8361\n",
      "Epoch 521/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7045 - val_loss: 173.8344\n",
      "Epoch 522/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6748 - val_loss: 173.8418\n",
      "Epoch 523/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7239 - val_loss: 173.8900\n",
      "Epoch 524/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6612 - val_loss: 173.8756\n",
      "Epoch 525/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6619 - val_loss: 173.8520\n",
      "Epoch 526/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6585 - val_loss: 173.8621\n",
      "Epoch 527/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6549 - val_loss: 173.8845\n",
      "Epoch 528/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6471 - val_loss: 173.8958\n",
      "Epoch 529/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6409 - val_loss: 173.9009\n",
      "Epoch 530/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6326 - val_loss: 173.8892\n",
      "Epoch 531/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6292 - val_loss: 173.8944\n",
      "Epoch 532/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.6135 - val_loss: 173.8940\n",
      "Epoch 533/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.6207 - val_loss: 173.8891\n",
      "Epoch 534/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6219 - val_loss: 173.8882\n",
      "Epoch 535/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6226 - val_loss: 173.8651\n",
      "Epoch 536/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6407 - val_loss: 173.8558\n",
      "Epoch 537/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6773 - val_loss: 173.9103\n",
      "Epoch 538/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5959 - val_loss: 173.9076\n",
      "Epoch 539/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6109 - val_loss: 173.9355\n",
      "Epoch 540/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5833 - val_loss: 173.9414\n",
      "Epoch 541/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5762 - val_loss: 173.9394\n",
      "Epoch 542/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5734 - val_loss: 173.9575\n",
      "Epoch 543/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5926 - val_loss: 174.0107\n",
      "Epoch 544/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5591 - val_loss: 174.0175\n",
      "Epoch 545/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5765 - val_loss: 174.0461\n",
      "Epoch 546/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5616 - val_loss: 174.0625\n",
      "Epoch 547/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5573 - val_loss: 174.0433\n",
      "Epoch 548/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5524 - val_loss: 174.0527\n",
      "Epoch 549/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5573 - val_loss: 174.0508\n",
      "Epoch 550/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5395 - val_loss: 174.0887\n",
      "Epoch 551/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5413 - val_loss: 174.1272\n",
      "Epoch 552/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5296 - val_loss: 174.1332\n",
      "Epoch 553/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5287 - val_loss: 174.1105\n",
      "Epoch 554/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5294 - val_loss: 174.1106\n",
      "Epoch 555/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5248 - val_loss: 174.1417\n",
      "Epoch 556/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5291 - val_loss: 174.1335\n",
      "Epoch 557/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5099 - val_loss: 174.1620\n",
      "Epoch 558/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5165 - val_loss: 174.1872\n",
      "Epoch 559/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5177 - val_loss: 174.2203\n",
      "Epoch 560/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5176 - val_loss: 174.2443\n",
      "Epoch 561/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5083 - val_loss: 174.2152\n",
      "Epoch 562/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5052 - val_loss: 174.2437\n",
      "Epoch 563/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5003 - val_loss: 174.2571\n",
      "Epoch 564/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4934 - val_loss: 174.2544\n",
      "Epoch 565/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4963 - val_loss: 174.2572\n",
      "Epoch 566/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5311 - val_loss: 174.3190\n",
      "Epoch 567/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4808 - val_loss: 174.3302\n",
      "Epoch 568/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4889 - val_loss: 174.3186\n",
      "Epoch 569/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4885 - val_loss: 174.3090\n",
      "Epoch 570/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4809 - val_loss: 174.3488\n",
      "Epoch 571/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4617 - val_loss: 174.3699\n",
      "Epoch 572/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4579 - val_loss: 174.3909\n",
      "Epoch 573/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4660 - val_loss: 174.3979\n",
      "Epoch 574/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4501 - val_loss: 174.4250\n",
      "Epoch 575/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4733 - val_loss: 174.5043\n",
      "Epoch 576/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4566 - val_loss: 174.5244\n",
      "Epoch 577/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4596 - val_loss: 174.5020\n",
      "Epoch 578/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4486 - val_loss: 174.5195\n",
      "Epoch 579/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4601 - val_loss: 174.5432\n",
      "Epoch 580/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4439 - val_loss: 174.5635\n",
      "Epoch 581/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4450 - val_loss: 174.5186\n",
      "Epoch 582/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4730 - val_loss: 174.5592\n",
      "Epoch 583/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4751 - val_loss: 174.5244\n",
      "Epoch 584/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4348 - val_loss: 174.5510\n",
      "Epoch 585/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4425 - val_loss: 174.5085\n",
      "Epoch 586/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4210 - val_loss: 174.5420\n",
      "Epoch 587/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4207 - val_loss: 174.5496\n",
      "Epoch 588/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4469 - val_loss: 174.6075\n",
      "Epoch 589/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4125 - val_loss: 174.6117\n",
      "Epoch 590/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4286 - val_loss: 174.5893\n",
      "Epoch 591/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4113 - val_loss: 174.6310\n",
      "Epoch 592/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4210 - val_loss: 174.6542\n",
      "Epoch 593/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3955 - val_loss: 174.6422\n",
      "Epoch 594/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4138 - val_loss: 174.6479\n",
      "Epoch 595/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3851 - val_loss: 174.6106\n",
      "Epoch 596/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4043 - val_loss: 174.5724\n",
      "Epoch 597/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3965 - val_loss: 174.5629\n",
      "Epoch 598/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3954 - val_loss: 174.6100\n",
      "Epoch 599/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3924 - val_loss: 174.6397\n",
      "Epoch 600/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3895 - val_loss: 174.6258\n",
      "Epoch 601/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3810 - val_loss: 174.6214\n",
      "Epoch 602/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3713 - val_loss: 174.6394\n",
      "Epoch 603/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3850 - val_loss: 174.6286\n",
      "Epoch 604/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3841 - val_loss: 174.6784\n",
      "Epoch 605/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3913 - val_loss: 174.7101\n",
      "Epoch 606/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3689 - val_loss: 174.7446\n",
      "Epoch 607/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3959 - val_loss: 174.7793\n",
      "Epoch 608/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3702 - val_loss: 174.7475\n",
      "Epoch 609/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3549 - val_loss: 174.7451\n",
      "Epoch 610/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3665 - val_loss: 174.7768\n",
      "Epoch 611/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3499 - val_loss: 174.7802\n",
      "Epoch 612/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3622 - val_loss: 174.7755\n",
      "Epoch 613/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3482 - val_loss: 174.8110\n",
      "Epoch 614/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3557 - val_loss: 174.8052\n",
      "Epoch 615/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3512 - val_loss: 174.8137\n",
      "Epoch 616/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3485 - val_loss: 174.8321\n",
      "Epoch 617/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3561 - val_loss: 174.8356\n",
      "Epoch 618/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3907 - val_loss: 174.9082\n",
      "Epoch 619/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3418 - val_loss: 174.8752\n",
      "Epoch 620/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3459 - val_loss: 174.8528\n",
      "Epoch 621/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3651 - val_loss: 174.9247\n",
      "Epoch 622/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3331 - val_loss: 174.9550\n",
      "Epoch 623/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3732 - val_loss: 174.9330\n",
      "Epoch 624/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3275 - val_loss: 174.9675\n",
      "Epoch 625/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3576 - val_loss: 174.9413\n",
      "Epoch 626/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3185 - val_loss: 174.9734\n",
      "Epoch 627/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3183 - val_loss: 174.9860\n",
      "Epoch 628/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3158 - val_loss: 174.9859\n",
      "Epoch 629/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3366 - val_loss: 175.0371\n",
      "Epoch 630/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3117 - val_loss: 175.0700\n",
      "Epoch 631/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3259 - val_loss: 175.0805\n",
      "Epoch 632/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3119 - val_loss: 175.1112\n",
      "Epoch 633/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3174 - val_loss: 175.1165\n",
      "Epoch 634/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3083 - val_loss: 175.0963\n",
      "Epoch 635/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3311 - val_loss: 175.0576\n",
      "Epoch 636/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3127 - val_loss: 175.0922\n",
      "Epoch 637/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3016 - val_loss: 175.1055\n",
      "Epoch 638/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3153 - val_loss: 175.1101\n",
      "Epoch 639/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3149 - val_loss: 175.0906\n",
      "Epoch 640/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2982 - val_loss: 175.1094\n",
      "Epoch 641/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3047 - val_loss: 175.1039\n",
      "Epoch 642/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3040 - val_loss: 175.0938\n",
      "Epoch 643/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2978 - val_loss: 175.1260\n",
      "Epoch 644/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3172 - val_loss: 175.1479\n",
      "Epoch 645/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2914 - val_loss: 175.1682\n",
      "Epoch 646/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3127 - val_loss: 175.1426\n",
      "Epoch 647/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2900 - val_loss: 175.1608\n",
      "Epoch 648/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2891 - val_loss: 175.1734\n",
      "Epoch 649/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.2997 - val_loss: 175.1915\n",
      "Epoch 650/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3345 - val_loss: 175.2412\n",
      "Epoch 651/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2782 - val_loss: 175.2431\n",
      "Epoch 652/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2825 - val_loss: 175.2418\n",
      "Epoch 653/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2899 - val_loss: 175.2569\n",
      "Epoch 654/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2974 - val_loss: 175.2938\n",
      "Epoch 655/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2875 - val_loss: 175.2494\n",
      "Epoch 656/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2775 - val_loss: 175.2345\n",
      "Epoch 657/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2743 - val_loss: 175.2330\n",
      "Epoch 658/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2757 - val_loss: 175.2505\n",
      "Epoch 659/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2964 - val_loss: 175.2428\n",
      "Epoch 660/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2696 - val_loss: 175.2728\n",
      "Epoch 661/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3025 - val_loss: 175.2460\n",
      "Epoch 662/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3004 - val_loss: 175.2823\n",
      "Epoch 663/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2992 - val_loss: 175.2713\n",
      "Epoch 664/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2631 - val_loss: 175.2796\n",
      "Epoch 665/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2887 - val_loss: 175.2622\n",
      "Epoch 666/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3075 - val_loss: 175.3277\n",
      "Epoch 667/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2736 - val_loss: 175.3628\n",
      "Epoch 668/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2614 - val_loss: 175.3386\n",
      "Epoch 669/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2534 - val_loss: 175.3251\n",
      "Epoch 670/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2619 - val_loss: 175.3206\n",
      "Epoch 671/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2685 - val_loss: 175.3016\n",
      "Epoch 672/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2728 - val_loss: 175.3037\n",
      "Epoch 673/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2567 - val_loss: 175.3333\n",
      "Epoch 674/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2588 - val_loss: 175.3485\n",
      "Epoch 675/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2825 - val_loss: 175.3793\n",
      "Epoch 676/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2504 - val_loss: 175.3435\n",
      "Epoch 677/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2518 - val_loss: 175.3583\n",
      "Epoch 678/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2600 - val_loss: 175.3611\n",
      "Epoch 679/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2572 - val_loss: 175.3864\n",
      "Epoch 680/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2679 - val_loss: 175.4068\n",
      "Epoch 681/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2670 - val_loss: 175.3840\n",
      "Epoch 682/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2627 - val_loss: 175.4265\n",
      "Epoch 683/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2391 - val_loss: 175.4304\n",
      "Epoch 684/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2415 - val_loss: 175.4358\n",
      "Epoch 685/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2441 - val_loss: 175.4229\n",
      "Epoch 686/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2422 - val_loss: 175.4270\n",
      "Epoch 687/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2517 - val_loss: 175.4238\n",
      "Epoch 688/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2404 - val_loss: 175.4512\n",
      "Epoch 689/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2639 - val_loss: 175.4831\n",
      "Epoch 690/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2380 - val_loss: 175.4922\n",
      "Epoch 691/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2399 - val_loss: 175.5016\n",
      "Epoch 692/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2727 - val_loss: 175.4503\n",
      "Epoch 693/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2344 - val_loss: 175.4747\n",
      "Epoch 694/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2446 - val_loss: 175.4541\n",
      "Epoch 695/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2310 - val_loss: 175.4785\n",
      "Epoch 696/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2532 - val_loss: 175.5269\n",
      "Epoch 697/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2527 - val_loss: 175.5206\n",
      "Epoch 698/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2417 - val_loss: 175.5598\n",
      "Epoch 699/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2629 - val_loss: 175.5195\n",
      "Epoch 700/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2321 - val_loss: 175.5290\n",
      "Epoch 701/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2475 - val_loss: 175.5699\n",
      "Epoch 702/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2348 - val_loss: 175.5763\n",
      "Epoch 703/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2276 - val_loss: 175.5719\n",
      "Epoch 704/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2253 - val_loss: 175.5609\n",
      "Epoch 705/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2309 - val_loss: 175.5964\n",
      "Epoch 706/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2279 - val_loss: 175.6095\n",
      "Epoch 707/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2283 - val_loss: 175.5862\n",
      "Epoch 708/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2201 - val_loss: 175.6123\n",
      "Epoch 709/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2297 - val_loss: 175.6332\n",
      "Epoch 710/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2163 - val_loss: 175.6360\n",
      "Epoch 711/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2231 - val_loss: 175.6514\n",
      "Epoch 712/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2185 - val_loss: 175.6505\n",
      "Epoch 713/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2196 - val_loss: 175.6366\n",
      "Epoch 714/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2349 - val_loss: 175.6058\n",
      "Epoch 715/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2117 - val_loss: 175.6302\n",
      "Epoch 716/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2111 - val_loss: 175.6489\n",
      "Epoch 717/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2121 - val_loss: 175.6725\n",
      "Epoch 718/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2218 - val_loss: 175.6842\n",
      "Epoch 719/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2139 - val_loss: 175.7111\n",
      "Epoch 720/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2147 - val_loss: 175.7427\n",
      "Epoch 721/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2033 - val_loss: 175.7671\n",
      "Epoch 722/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2135 - val_loss: 175.7980\n",
      "Epoch 723/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1970 - val_loss: 175.8244\n",
      "Epoch 724/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2220 - val_loss: 175.8600\n",
      "Epoch 725/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2129 - val_loss: 175.9137\n",
      "Epoch 726/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2089 - val_loss: 175.9105\n",
      "Epoch 727/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2124 - val_loss: 175.9109\n",
      "Epoch 728/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2057 - val_loss: 175.9141\n",
      "Epoch 729/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2082 - val_loss: 175.9404\n",
      "Epoch 730/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2022 - val_loss: 175.9302\n",
      "Epoch 731/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2029 - val_loss: 175.8907\n",
      "Epoch 732/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1977 - val_loss: 175.8736\n",
      "Epoch 733/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2046 - val_loss: 175.8536\n",
      "Epoch 734/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1982 - val_loss: 175.8454\n",
      "Epoch 735/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1976 - val_loss: 175.8364\n",
      "Epoch 736/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2026 - val_loss: 175.8384\n",
      "Epoch 737/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2424 - val_loss: 175.8921\n",
      "Epoch 738/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1959 - val_loss: 175.8883\n",
      "Epoch 739/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2274 - val_loss: 175.9523\n",
      "Epoch 740/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2060 - val_loss: 175.9678\n",
      "Epoch 741/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1929 - val_loss: 175.9638\n",
      "Epoch 742/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1968 - val_loss: 175.9403\n",
      "Epoch 743/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1827 - val_loss: 175.9732\n",
      "Epoch 744/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2245 - val_loss: 176.0223\n",
      "Epoch 745/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2080 - val_loss: 176.0490\n",
      "Epoch 746/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2169 - val_loss: 175.9837\n",
      "Epoch 747/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1972 - val_loss: 175.9747\n",
      "Epoch 748/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1850 - val_loss: 175.9940\n",
      "Epoch 749/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1980 - val_loss: 176.0023\n",
      "Epoch 750/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1799 - val_loss: 175.9805\n",
      ">Saved ./models/comp4948_a2_model_2.h5\n",
      "Epoch 1/750\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 4530.8110 - val_loss: 4527.4609\n",
      "Epoch 2/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4524.4526 - val_loss: 4521.2808\n",
      "Epoch 3/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4518.4600 - val_loss: 4515.2627\n",
      "Epoch 4/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4512.4590 - val_loss: 4509.2339\n",
      "Epoch 5/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4506.4170 - val_loss: 4503.1738\n",
      "Epoch 6/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4500.3501 - val_loss: 4497.0894\n",
      "Epoch 7/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4494.2612 - val_loss: 4490.9551\n",
      "Epoch 8/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4488.1064 - val_loss: 4484.7808\n",
      "Epoch 9/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4481.9126 - val_loss: 4478.4932\n",
      "Epoch 10/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4475.6230 - val_loss: 4472.0884\n",
      "Epoch 11/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4469.1396 - val_loss: 4465.5762\n",
      "Epoch 12/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4462.6338 - val_loss: 4458.9219\n",
      "Epoch 13/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4455.8901 - val_loss: 4452.1582\n",
      "Epoch 14/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4449.1411 - val_loss: 4445.3940\n",
      "Epoch 15/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4442.3555 - val_loss: 4438.4814\n",
      "Epoch 16/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4435.3887 - val_loss: 4431.3965\n",
      "Epoch 17/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4428.2769 - val_loss: 4424.0781\n",
      "Epoch 18/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4420.8291 - val_loss: 4416.6831\n",
      "Epoch 19/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4413.5488 - val_loss: 4409.2944\n",
      "Epoch 20/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4406.0869 - val_loss: 4401.7046\n",
      "Epoch 21/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4398.4863 - val_loss: 4393.9653\n",
      "Epoch 22/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4390.6143 - val_loss: 4386.0552\n",
      "Epoch 23/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4382.7607 - val_loss: 4377.9858\n",
      "Epoch 24/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4374.6123 - val_loss: 4369.7271\n",
      "Epoch 25/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4366.3818 - val_loss: 4361.5005\n",
      "Epoch 26/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4358.0781 - val_loss: 4353.1699\n",
      "Epoch 27/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4349.6299 - val_loss: 4344.6694\n",
      "Epoch 28/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4341.2729 - val_loss: 4336.2808\n",
      "Epoch 29/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4332.7363 - val_loss: 4327.6465\n",
      "Epoch 30/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4324.0869 - val_loss: 4318.7280\n",
      "Epoch 31/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4315.1719 - val_loss: 4309.6846\n",
      "Epoch 32/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4306.0063 - val_loss: 4300.5581\n",
      "Epoch 33/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4296.9521 - val_loss: 4291.4053\n",
      "Epoch 34/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4287.6963 - val_loss: 4281.9907\n",
      "Epoch 35/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4278.1855 - val_loss: 4272.3545\n",
      "Epoch 36/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4268.6553 - val_loss: 4262.5566\n",
      "Epoch 37/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4258.8179 - val_loss: 4252.5020\n",
      "Epoch 38/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4248.6597 - val_loss: 4242.2842\n",
      "Epoch 39/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4238.3413 - val_loss: 4231.7812\n",
      "Epoch 40/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4227.7344 - val_loss: 4221.0864\n",
      "Epoch 41/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4217.0234 - val_loss: 4210.3442\n",
      "Epoch 42/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4206.3164 - val_loss: 4199.5312\n",
      "Epoch 43/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4195.7080 - val_loss: 4188.6743\n",
      "Epoch 44/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4184.6831 - val_loss: 4177.6411\n",
      "Epoch 45/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4173.5366 - val_loss: 4166.3330\n",
      "Epoch 46/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4162.1133 - val_loss: 4155.0083\n",
      "Epoch 47/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4150.8364 - val_loss: 4143.5166\n",
      "Epoch 48/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4139.1860 - val_loss: 4131.6836\n",
      "Epoch 49/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4127.5771 - val_loss: 4120.1377\n",
      "Epoch 50/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4116.0332 - val_loss: 4108.4731\n",
      "Epoch 51/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4104.4873 - val_loss: 4096.5161\n",
      "Epoch 52/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4092.3235 - val_loss: 4084.4666\n",
      "Epoch 53/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4080.3960 - val_loss: 4072.2861\n",
      "Epoch 54/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4068.2234 - val_loss: 4060.0334\n",
      "Epoch 55/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4055.9607 - val_loss: 4047.4319\n",
      "Epoch 56/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4043.0535 - val_loss: 4034.8455\n",
      "Epoch 57/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4030.8215 - val_loss: 4022.5181\n",
      "Epoch 58/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4018.4985 - val_loss: 4009.8267\n",
      "Epoch 59/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4005.6936 - val_loss: 3996.9717\n",
      "Epoch 60/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3992.7830 - val_loss: 3983.8936\n",
      "Epoch 61/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3979.5789 - val_loss: 3970.6052\n",
      "Epoch 62/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3966.3328 - val_loss: 3957.0156\n",
      "Epoch 63/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3952.6147 - val_loss: 3943.1790\n",
      "Epoch 64/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3938.5884 - val_loss: 3929.2717\n",
      "Epoch 65/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3925.1526 - val_loss: 3915.8196\n",
      "Epoch 66/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3911.5549 - val_loss: 3902.1287\n",
      "Epoch 67/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3897.7673 - val_loss: 3888.1641\n",
      "Epoch 68/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3883.7559 - val_loss: 3873.8416\n",
      "Epoch 69/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3869.6882 - val_loss: 3859.5771\n",
      "Epoch 70/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3855.2441 - val_loss: 3845.1052\n",
      "Epoch 71/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3840.6985 - val_loss: 3830.2788\n",
      "Epoch 72/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3825.8325 - val_loss: 3815.2314\n",
      "Epoch 73/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3810.7085 - val_loss: 3799.9141\n",
      "Epoch 74/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3795.2788 - val_loss: 3784.2432\n",
      "Epoch 75/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3779.6213 - val_loss: 3768.6030\n",
      "Epoch 76/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3764.1389 - val_loss: 3752.9900\n",
      "Epoch 77/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3748.3760 - val_loss: 3737.0312\n",
      "Epoch 78/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3732.3291 - val_loss: 3720.7615\n",
      "Epoch 79/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3715.8264 - val_loss: 3704.3281\n",
      "Epoch 80/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3699.4016 - val_loss: 3688.0256\n",
      "Epoch 81/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3683.4314 - val_loss: 3671.4368\n",
      "Epoch 82/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3666.8931 - val_loss: 3655.1033\n",
      "Epoch 83/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3650.4592 - val_loss: 3638.5930\n",
      "Epoch 84/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3633.8594 - val_loss: 3621.7219\n",
      "Epoch 85/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3617.0447 - val_loss: 3604.4780\n",
      "Epoch 86/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3599.6780 - val_loss: 3587.1143\n",
      "Epoch 87/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3582.5850 - val_loss: 3569.7146\n",
      "Epoch 88/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3565.4180 - val_loss: 3552.4495\n",
      "Epoch 89/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3547.9648 - val_loss: 3535.0774\n",
      "Epoch 90/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3530.6123 - val_loss: 3517.5571\n",
      "Epoch 91/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3513.4502 - val_loss: 3500.1938\n",
      "Epoch 92/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3495.8235 - val_loss: 3482.6868\n",
      "Epoch 93/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3478.5139 - val_loss: 3464.6084\n",
      "Epoch 94/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3460.3923 - val_loss: 3446.4102\n",
      "Epoch 95/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 3442.1919 - val_loss: 3428.1624\n",
      "Epoch 96/750\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 3424.1755 - val_loss: 3410.0493\n",
      "Epoch 97/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3405.9539 - val_loss: 3391.7119\n",
      "Epoch 98/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3387.5515 - val_loss: 3373.0967\n",
      "Epoch 99/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3368.9905 - val_loss: 3354.6135\n",
      "Epoch 100/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 3350.7217 - val_loss: 3335.9021\n",
      "Epoch 101/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3331.9270 - val_loss: 3317.0532\n",
      "Epoch 102/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3312.9382 - val_loss: 3298.0146\n",
      "Epoch 103/750\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3294.0366 - val_loss: 3278.5632\n",
      "Epoch 104/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3274.4578 - val_loss: 3258.9673\n",
      "Epoch 105/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3255.0901 - val_loss: 3238.9646\n",
      "Epoch 106/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3235.1436 - val_loss: 3218.6819\n",
      "Epoch 107/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 3215.1836 - val_loss: 3198.8630\n",
      "Epoch 108/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3195.4221 - val_loss: 3178.9873\n",
      "Epoch 109/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3175.5139 - val_loss: 3158.8721\n",
      "Epoch 110/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3155.4307 - val_loss: 3138.7888\n",
      "Epoch 111/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3135.2881 - val_loss: 3118.5713\n",
      "Epoch 112/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3114.9436 - val_loss: 3098.0798\n",
      "Epoch 113/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3094.9546 - val_loss: 3077.3577\n",
      "Epoch 114/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3074.0251 - val_loss: 3056.7173\n",
      "Epoch 115/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3053.5720 - val_loss: 3036.1423\n",
      "Epoch 116/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3032.9373 - val_loss: 3015.5732\n",
      "Epoch 117/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3012.4480 - val_loss: 2994.5530\n",
      "Epoch 118/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2991.4905 - val_loss: 2973.6523\n",
      "Epoch 119/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2970.6963 - val_loss: 2952.9617\n",
      "Epoch 120/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2949.9922 - val_loss: 2932.0530\n",
      "Epoch 121/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2929.1873 - val_loss: 2910.9834\n",
      "Epoch 122/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2908.1536 - val_loss: 2889.5522\n",
      "Epoch 123/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2886.4673 - val_loss: 2868.0635\n",
      "Epoch 124/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2865.4587 - val_loss: 2846.9944\n",
      "Epoch 125/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2843.9417 - val_loss: 2825.9695\n",
      "Epoch 126/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2823.6963 - val_loss: 2805.0120\n",
      "Epoch 127/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2802.7373 - val_loss: 2784.1277\n",
      "Epoch 128/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2781.6184 - val_loss: 2763.2029\n",
      "Epoch 129/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2760.5864 - val_loss: 2741.9729\n",
      "Epoch 130/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2739.4731 - val_loss: 2720.3525\n",
      "Epoch 131/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2717.8027 - val_loss: 2698.5974\n",
      "Epoch 132/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2695.9890 - val_loss: 2676.7378\n",
      "Epoch 133/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2674.1958 - val_loss: 2654.6616\n",
      "Epoch 134/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2652.1794 - val_loss: 2632.4106\n",
      "Epoch 135/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2629.9446 - val_loss: 2610.2146\n",
      "Epoch 136/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2608.1494 - val_loss: 2587.9265\n",
      "Epoch 137/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2585.7327 - val_loss: 2565.5508\n",
      "Epoch 138/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2563.5046 - val_loss: 2542.9648\n",
      "Epoch 139/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2541.2124 - val_loss: 2520.5667\n",
      "Epoch 140/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2518.7063 - val_loss: 2498.1863\n",
      "Epoch 141/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2496.3728 - val_loss: 2475.5703\n",
      "Epoch 142/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2473.6799 - val_loss: 2452.8523\n",
      "Epoch 143/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2451.1646 - val_loss: 2430.1321\n",
      "Epoch 144/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2428.7344 - val_loss: 2407.4001\n",
      "Epoch 145/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2406.1824 - val_loss: 2385.1670\n",
      "Epoch 146/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2384.7612 - val_loss: 2363.5027\n",
      "Epoch 147/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2362.7915 - val_loss: 2341.9890\n",
      "Epoch 148/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2341.5579 - val_loss: 2320.1001\n",
      "Epoch 149/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2319.8066 - val_loss: 2298.0300\n",
      "Epoch 150/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2297.6570 - val_loss: 2275.8713\n",
      "Epoch 151/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2275.9729 - val_loss: 2253.9099\n",
      "Epoch 152/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2253.9775 - val_loss: 2231.8469\n",
      "Epoch 153/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2231.9626 - val_loss: 2209.6414\n",
      "Epoch 154/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2209.8787 - val_loss: 2187.4392\n",
      "Epoch 155/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2187.7842 - val_loss: 2165.3884\n",
      "Epoch 156/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2166.0479 - val_loss: 2143.1233\n",
      "Epoch 157/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2143.7710 - val_loss: 2120.7412\n",
      "Epoch 158/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2121.6921 - val_loss: 2098.2383\n",
      "Epoch 159/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2099.0161 - val_loss: 2075.7632\n",
      "Epoch 160/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2076.9519 - val_loss: 2053.0908\n",
      "Epoch 161/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2054.2700 - val_loss: 2030.4304\n",
      "Epoch 162/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2031.6208 - val_loss: 2007.5919\n",
      "Epoch 163/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2008.7136 - val_loss: 1984.7095\n",
      "Epoch 164/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1986.3741 - val_loss: 1961.7313\n",
      "Epoch 165/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1963.6219 - val_loss: 1939.0841\n",
      "Epoch 166/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1941.2206 - val_loss: 1916.3075\n",
      "Epoch 167/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1918.6678 - val_loss: 1893.4301\n",
      "Epoch 168/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1896.1232 - val_loss: 1870.5806\n",
      "Epoch 169/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1873.7729 - val_loss: 1847.6620\n",
      "Epoch 170/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1851.1199 - val_loss: 1824.7805\n",
      "Epoch 171/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1828.2047 - val_loss: 1801.9449\n",
      "Epoch 172/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1805.4316 - val_loss: 1779.0493\n",
      "Epoch 173/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1782.8516 - val_loss: 1755.8563\n",
      "Epoch 174/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1759.6243 - val_loss: 1733.2894\n",
      "Epoch 175/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1737.8654 - val_loss: 1710.8348\n",
      "Epoch 176/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1715.3107 - val_loss: 1688.6195\n",
      "Epoch 177/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1694.0793 - val_loss: 1666.2501\n",
      "Epoch 178/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1671.8772 - val_loss: 1644.3322\n",
      "Epoch 179/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1650.1726 - val_loss: 1622.5746\n",
      "Epoch 180/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1628.7655 - val_loss: 1600.6855\n",
      "Epoch 181/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1607.4352 - val_loss: 1579.5490\n",
      "Epoch 182/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1586.4860 - val_loss: 1558.7749\n",
      "Epoch 183/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1565.6095 - val_loss: 1538.0300\n",
      "Epoch 184/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1545.2142 - val_loss: 1517.0565\n",
      "Epoch 185/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1524.6259 - val_loss: 1496.0101\n",
      "Epoch 186/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1503.8640 - val_loss: 1474.9911\n",
      "Epoch 187/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1482.8588 - val_loss: 1454.0939\n",
      "Epoch 188/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1462.0953 - val_loss: 1433.1973\n",
      "Epoch 189/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1441.9861 - val_loss: 1412.1759\n",
      "Epoch 190/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1421.1497 - val_loss: 1391.5354\n",
      "Epoch 191/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1400.7195 - val_loss: 1371.0078\n",
      "Epoch 192/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1380.6090 - val_loss: 1350.3851\n",
      "Epoch 193/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1360.1813 - val_loss: 1329.8766\n",
      "Epoch 194/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1339.9799 - val_loss: 1309.4226\n",
      "Epoch 195/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1319.9171 - val_loss: 1289.1013\n",
      "Epoch 196/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1299.9143 - val_loss: 1268.8540\n",
      "Epoch 197/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1279.6376 - val_loss: 1248.8584\n",
      "Epoch 198/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1259.8351 - val_loss: 1228.8419\n",
      "Epoch 199/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1240.2632 - val_loss: 1208.7440\n",
      "Epoch 200/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1220.2916 - val_loss: 1188.8704\n",
      "Epoch 201/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1201.2388 - val_loss: 1169.3204\n",
      "Epoch 202/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1181.9059 - val_loss: 1150.0609\n",
      "Epoch 203/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1162.6019 - val_loss: 1130.9965\n",
      "Epoch 204/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1144.1053 - val_loss: 1111.8572\n",
      "Epoch 205/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1125.0763 - val_loss: 1093.0730\n",
      "Epoch 206/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1106.7545 - val_loss: 1074.2095\n",
      "Epoch 207/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1088.3599 - val_loss: 1055.3998\n",
      "Epoch 208/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1069.4285 - val_loss: 1037.0181\n",
      "Epoch 209/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1051.3894 - val_loss: 1018.6027\n",
      "Epoch 210/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1033.1132 - val_loss: 1000.3832\n",
      "Epoch 211/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1015.1081 - val_loss: 982.1968\n",
      "Epoch 212/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 997.4860 - val_loss: 963.9744\n",
      "Epoch 213/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 979.8203 - val_loss: 945.8528\n",
      "Epoch 214/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 961.5717 - val_loss: 928.3160\n",
      "Epoch 215/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 944.4857 - val_loss: 910.6699\n",
      "Epoch 216/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 927.0336 - val_loss: 893.3210\n",
      "Epoch 217/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 910.2810 - val_loss: 875.9149\n",
      "Epoch 218/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 892.9835 - val_loss: 858.9016\n",
      "Epoch 219/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 876.0760 - val_loss: 842.1276\n",
      "Epoch 220/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 860.0468 - val_loss: 825.1322\n",
      "Epoch 221/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 843.3497 - val_loss: 808.5149\n",
      "Epoch 222/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 827.0400 - val_loss: 792.0758\n",
      "Epoch 223/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 810.8135 - val_loss: 775.9323\n",
      "Epoch 224/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 795.1025 - val_loss: 759.9108\n",
      "Epoch 225/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 779.5197 - val_loss: 744.0552\n",
      "Epoch 226/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 764.1738 - val_loss: 728.3856\n",
      "Epoch 227/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 748.4363 - val_loss: 713.2372\n",
      "Epoch 228/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 734.1647 - val_loss: 697.8527\n",
      "Epoch 229/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 719.0100 - val_loss: 682.9869\n",
      "Epoch 230/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 704.3889 - val_loss: 668.4418\n",
      "Epoch 231/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 690.6213 - val_loss: 653.8412\n",
      "Epoch 232/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 676.1544 - val_loss: 639.7582\n",
      "Epoch 233/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 662.4765 - val_loss: 625.8627\n",
      "Epoch 234/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 649.1901 - val_loss: 612.0527\n",
      "Epoch 235/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 635.5969 - val_loss: 598.6605\n",
      "Epoch 236/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 622.6498 - val_loss: 585.4288\n",
      "Epoch 237/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 609.8230 - val_loss: 572.4414\n",
      "Epoch 238/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 597.1702 - val_loss: 559.8095\n",
      "Epoch 239/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 585.0728 - val_loss: 547.3101\n",
      "Epoch 240/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 572.9976 - val_loss: 535.1029\n",
      "Epoch 241/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 561.2753 - val_loss: 523.2071\n",
      "Epoch 242/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 550.0195 - val_loss: 511.5327\n",
      "Epoch 243/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 538.7158 - val_loss: 500.1482\n",
      "Epoch 244/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 527.6223 - val_loss: 489.1250\n",
      "Epoch 245/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 517.0869 - val_loss: 478.2501\n",
      "Epoch 246/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 506.4911 - val_loss: 467.7101\n",
      "Epoch 247/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 496.1247 - val_loss: 457.4771\n",
      "Epoch 248/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 486.5244 - val_loss: 447.2682\n",
      "Epoch 249/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 476.8482 - val_loss: 437.2847\n",
      "Epoch 250/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 467.0875 - val_loss: 427.7844\n",
      "Epoch 251/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 457.9187 - val_loss: 418.4750\n",
      "Epoch 252/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 449.0281 - val_loss: 409.3261\n",
      "Epoch 253/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 440.5855 - val_loss: 400.4102\n",
      "Epoch 254/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 432.0796 - val_loss: 391.8993\n",
      "Epoch 255/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 423.6321 - val_loss: 383.8911\n",
      "Epoch 256/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 416.5243 - val_loss: 375.6621\n",
      "Epoch 257/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 408.5089 - val_loss: 367.9567\n",
      "Epoch 258/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 401.2115 - val_loss: 360.4453\n",
      "Epoch 259/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 393.9657 - val_loss: 353.1690\n",
      "Epoch 260/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 387.1004 - val_loss: 346.0437\n",
      "Epoch 261/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 380.4524 - val_loss: 339.0996\n",
      "Epoch 262/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 373.7535 - val_loss: 332.5108\n",
      "Epoch 263/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 367.4856 - val_loss: 326.1213\n",
      "Epoch 264/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 361.3230 - val_loss: 319.9278\n",
      "Epoch 265/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 355.8359 - val_loss: 313.6707\n",
      "Epoch 266/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 349.8894 - val_loss: 307.7780\n",
      "Epoch 267/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 344.2219 - val_loss: 302.1764\n",
      "Epoch 268/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 338.9599 - val_loss: 296.7052\n",
      "Epoch 269/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 334.1395 - val_loss: 291.1985\n",
      "Epoch 270/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 328.7463 - val_loss: 286.1648\n",
      "Epoch 271/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 324.2496 - val_loss: 281.1391\n",
      "Epoch 272/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 319.3054 - val_loss: 276.5356\n",
      "Epoch 273/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 315.0504 - val_loss: 272.0103\n",
      "Epoch 274/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 310.9875 - val_loss: 267.5211\n",
      "Epoch 275/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 306.9065 - val_loss: 263.2456\n",
      "Epoch 276/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 302.9761 - val_loss: 259.1477\n",
      "Epoch 277/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 299.1725 - val_loss: 255.2872\n",
      "Epoch 278/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 295.7250 - val_loss: 251.5001\n",
      "Epoch 279/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 292.0627 - val_loss: 248.0233\n",
      "Epoch 280/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 288.8773 - val_loss: 244.6075\n",
      "Epoch 281/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 285.7478 - val_loss: 241.3425\n",
      "Epoch 282/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 282.9342 - val_loss: 238.0617\n",
      "Epoch 283/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 279.9465 - val_loss: 235.0122\n",
      "Epoch 284/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 277.0532 - val_loss: 232.1716\n",
      "Epoch 285/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 274.5632 - val_loss: 229.3470\n",
      "Epoch 286/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 271.8349 - val_loss: 226.7599\n",
      "Epoch 287/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 269.5901 - val_loss: 224.1432\n",
      "Epoch 288/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 267.3541 - val_loss: 221.5966\n",
      "Epoch 289/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 264.9692 - val_loss: 219.2766\n",
      "Epoch 290/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 262.8597 - val_loss: 217.0213\n",
      "Epoch 291/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 260.9225 - val_loss: 214.8066\n",
      "Epoch 292/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 258.8994 - val_loss: 212.7479\n",
      "Epoch 293/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 256.9606 - val_loss: 210.8393\n",
      "Epoch 294/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 255.5556 - val_loss: 208.8188\n",
      "Epoch 295/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 253.8015 - val_loss: 206.9883\n",
      "Epoch 296/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 252.1679 - val_loss: 205.3120\n",
      "Epoch 297/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 250.6339 - val_loss: 203.7521\n",
      "Epoch 298/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 249.1722 - val_loss: 202.3298\n",
      "Epoch 299/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 247.9842 - val_loss: 200.8607\n",
      "Epoch 300/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 246.6147 - val_loss: 199.5369\n",
      "Epoch 301/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 245.5124 - val_loss: 198.1750\n",
      "Epoch 302/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 244.4807 - val_loss: 196.8276\n",
      "Epoch 303/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 243.2851 - val_loss: 195.6785\n",
      "Epoch 304/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 242.2664 - val_loss: 194.5838\n",
      "Epoch 305/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 241.3386 - val_loss: 193.5092\n",
      "Epoch 306/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 240.4342 - val_loss: 192.4833\n",
      "Epoch 307/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 239.5689 - val_loss: 191.4983\n",
      "Epoch 308/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 238.7282 - val_loss: 190.5838\n",
      "Epoch 309/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 238.1132 - val_loss: 189.6441\n",
      "Epoch 310/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 237.2601 - val_loss: 188.8368\n",
      "Epoch 311/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 236.4962 - val_loss: 188.1252\n",
      "Epoch 312/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 236.0460 - val_loss: 187.3079\n",
      "Epoch 313/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 235.3491 - val_loss: 186.5841\n",
      "Epoch 314/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 234.7755 - val_loss: 185.9136\n",
      "Epoch 315/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 234.2412 - val_loss: 185.2736\n",
      "Epoch 316/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 233.7423 - val_loss: 184.6882\n",
      "Epoch 317/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 233.2656 - val_loss: 184.1293\n",
      "Epoch 318/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 232.8007 - val_loss: 183.6140\n",
      "Epoch 319/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 232.3719 - val_loss: 183.1351\n",
      "Epoch 320/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 231.9556 - val_loss: 182.6859\n",
      "Epoch 321/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.6844 - val_loss: 182.1807\n",
      "Epoch 322/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.3000 - val_loss: 181.7302\n",
      "Epoch 323/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 230.9124 - val_loss: 181.3565\n",
      "Epoch 324/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 230.6003 - val_loss: 180.9826\n",
      "Epoch 325/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 230.3685 - val_loss: 180.6145\n",
      "Epoch 326/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 230.0257 - val_loss: 180.2939\n",
      "Epoch 327/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.7211 - val_loss: 179.9846\n",
      "Epoch 328/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.5941 - val_loss: 179.5918\n",
      "Epoch 329/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.2836 - val_loss: 179.2616\n",
      "Epoch 330/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.0590 - val_loss: 178.9842\n",
      "Epoch 331/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.8648 - val_loss: 178.6938\n",
      "Epoch 332/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.6130 - val_loss: 178.4707\n",
      "Epoch 333/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.4579 - val_loss: 178.2490\n",
      "Epoch 334/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.2981 - val_loss: 178.0254\n",
      "Epoch 335/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.1053 - val_loss: 177.8348\n",
      "Epoch 336/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.9636 - val_loss: 177.6472\n",
      "Epoch 337/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.8365 - val_loss: 177.4304\n",
      "Epoch 338/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.7596 - val_loss: 177.2205\n",
      "Epoch 339/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.5249 - val_loss: 177.0986\n",
      "Epoch 340/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 227.4140 - val_loss: 176.9766\n",
      "Epoch 341/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 227.3417 - val_loss: 176.8044\n",
      "Epoch 342/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.2012 - val_loss: 176.6758\n",
      "Epoch 343/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.0884 - val_loss: 176.5353\n",
      "Epoch 344/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.9735 - val_loss: 176.4269\n",
      "Epoch 345/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.8983 - val_loss: 176.2730\n",
      "Epoch 346/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.8045 - val_loss: 176.1387\n",
      "Epoch 347/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.6936 - val_loss: 176.0308\n",
      "Epoch 348/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.5997 - val_loss: 175.9163\n",
      "Epoch 349/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.5237 - val_loss: 175.7967\n",
      "Epoch 350/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.4239 - val_loss: 175.7179\n",
      "Epoch 351/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.3663 - val_loss: 175.6147\n",
      "Epoch 352/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.2713 - val_loss: 175.5371\n",
      "Epoch 353/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.2069 - val_loss: 175.4460\n",
      "Epoch 354/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.1270 - val_loss: 175.3776\n",
      "Epoch 355/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.0778 - val_loss: 175.2802\n",
      "Epoch 356/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.0092 - val_loss: 175.2016\n",
      "Epoch 357/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.9381 - val_loss: 175.1529\n",
      "Epoch 358/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.8668 - val_loss: 175.0841\n",
      "Epoch 359/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.8000 - val_loss: 175.0083\n",
      "Epoch 360/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.7271 - val_loss: 174.9559\n",
      "Epoch 361/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.6727 - val_loss: 174.8734\n",
      "Epoch 362/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.5923 - val_loss: 174.7974\n",
      "Epoch 363/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.5598 - val_loss: 174.7128\n",
      "Epoch 364/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.4761 - val_loss: 174.6505\n",
      "Epoch 365/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.4223 - val_loss: 174.5843\n",
      "Epoch 366/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.4129 - val_loss: 174.4821\n",
      "Epoch 367/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.3036 - val_loss: 174.4315\n",
      "Epoch 368/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.2807 - val_loss: 174.3978\n",
      "Epoch 369/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.2168 - val_loss: 174.3483\n",
      "Epoch 370/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.1654 - val_loss: 174.3166\n",
      "Epoch 371/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.1239 - val_loss: 174.3095\n",
      "Epoch 372/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.0826 - val_loss: 174.2337\n",
      "Epoch 373/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.0305 - val_loss: 174.1931\n",
      "Epoch 374/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.9527 - val_loss: 174.1673\n",
      "Epoch 375/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.9489 - val_loss: 174.0914\n",
      "Epoch 376/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.8972 - val_loss: 174.0971\n",
      "Epoch 377/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.8308 - val_loss: 174.0740\n",
      "Epoch 378/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.7940 - val_loss: 174.0338\n",
      "Epoch 379/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.7425 - val_loss: 173.9926\n",
      "Epoch 380/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.7296 - val_loss: 173.9299\n",
      "Epoch 381/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.6500 - val_loss: 173.9068\n",
      "Epoch 382/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.6422 - val_loss: 173.8596\n",
      "Epoch 383/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.5806 - val_loss: 173.8387\n",
      "Epoch 384/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5280 - val_loss: 173.8257\n",
      "Epoch 385/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.4942 - val_loss: 173.8129\n",
      "Epoch 386/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.4576 - val_loss: 173.7945\n",
      "Epoch 387/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.4074 - val_loss: 173.7836\n",
      "Epoch 388/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3905 - val_loss: 173.7689\n",
      "Epoch 389/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3293 - val_loss: 173.7384\n",
      "Epoch 390/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3559 - val_loss: 173.7005\n",
      "Epoch 391/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.2718 - val_loss: 173.6996\n",
      "Epoch 392/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.2329 - val_loss: 173.6868\n",
      "Epoch 393/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.1923 - val_loss: 173.6510\n",
      "Epoch 394/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.1562 - val_loss: 173.6326\n",
      "Epoch 395/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 224.1189 - val_loss: 173.6151\n",
      "Epoch 396/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0814 - val_loss: 173.5956\n",
      "Epoch 397/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.0842 - val_loss: 173.5587\n",
      "Epoch 398/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.0134 - val_loss: 173.5620\n",
      "Epoch 399/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9719 - val_loss: 173.5399\n",
      "Epoch 400/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9539 - val_loss: 173.5137\n",
      "Epoch 401/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.9194 - val_loss: 173.5076\n",
      "Epoch 402/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.8849 - val_loss: 173.4885\n",
      "Epoch 403/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8648 - val_loss: 173.4975\n",
      "Epoch 404/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8179 - val_loss: 173.4861\n",
      "Epoch 405/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8087 - val_loss: 173.5022\n",
      "Epoch 406/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.7459 - val_loss: 173.5031\n",
      "Epoch 407/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.7362 - val_loss: 173.4834\n",
      "Epoch 408/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.6843 - val_loss: 173.4826\n",
      "Epoch 409/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.6560 - val_loss: 173.4669\n",
      "Epoch 410/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6332 - val_loss: 173.4535\n",
      "Epoch 411/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.6062 - val_loss: 173.4814\n",
      "Epoch 412/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.5851 - val_loss: 173.4918\n",
      "Epoch 413/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.5342 - val_loss: 173.4799\n",
      "Epoch 414/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5174 - val_loss: 173.4441\n",
      "Epoch 415/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4630 - val_loss: 173.4261\n",
      "Epoch 416/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4337 - val_loss: 173.4064\n",
      "Epoch 417/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4129 - val_loss: 173.4057\n",
      "Epoch 418/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3738 - val_loss: 173.3890\n",
      "Epoch 419/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3456 - val_loss: 173.3956\n",
      "Epoch 420/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3684 - val_loss: 173.3468\n",
      "Epoch 421/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3132 - val_loss: 173.3347\n",
      "Epoch 422/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.2763 - val_loss: 173.3738\n",
      "Epoch 423/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2257 - val_loss: 173.3839\n",
      "Epoch 424/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.2145 - val_loss: 173.3784\n",
      "Epoch 425/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1914 - val_loss: 173.4094\n",
      "Epoch 426/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1595 - val_loss: 173.4420\n",
      "Epoch 427/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1220 - val_loss: 173.4645\n",
      "Epoch 428/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0971 - val_loss: 173.4648\n",
      "Epoch 429/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0729 - val_loss: 173.4777\n",
      "Epoch 430/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0656 - val_loss: 173.4662\n",
      "Epoch 431/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0844 - val_loss: 173.4118\n",
      "Epoch 432/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0173 - val_loss: 173.4347\n",
      "Epoch 433/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9926 - val_loss: 173.4090\n",
      "Epoch 434/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9579 - val_loss: 173.4331\n",
      "Epoch 435/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9622 - val_loss: 173.4516\n",
      "Epoch 436/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9580 - val_loss: 173.3845\n",
      "Epoch 437/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8887 - val_loss: 173.3591\n",
      "Epoch 438/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8687 - val_loss: 173.3538\n",
      "Epoch 439/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8507 - val_loss: 173.3163\n",
      "Epoch 440/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8123 - val_loss: 173.3164\n",
      "Epoch 441/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7930 - val_loss: 173.3203\n",
      "Epoch 442/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8359 - val_loss: 173.2850\n",
      "Epoch 443/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7520 - val_loss: 173.3041\n",
      "Epoch 444/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.7360 - val_loss: 173.3205\n",
      "Epoch 445/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7531 - val_loss: 173.3511\n",
      "Epoch 446/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.7023 - val_loss: 173.3229\n",
      "Epoch 447/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6748 - val_loss: 173.3060\n",
      "Epoch 448/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6567 - val_loss: 173.3230\n",
      "Epoch 449/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6503 - val_loss: 173.3107\n",
      "Epoch 450/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6220 - val_loss: 173.2946\n",
      "Epoch 451/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6135 - val_loss: 173.3515\n",
      "Epoch 452/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5840 - val_loss: 173.3848\n",
      "Epoch 453/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5752 - val_loss: 173.3641\n",
      "Epoch 454/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5771 - val_loss: 173.3497\n",
      "Epoch 455/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5663 - val_loss: 173.3897\n",
      "Epoch 456/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5052 - val_loss: 173.3815\n",
      "Epoch 457/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5195 - val_loss: 173.3594\n",
      "Epoch 458/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4892 - val_loss: 173.3704\n",
      "Epoch 459/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4695 - val_loss: 173.3703\n",
      "Epoch 460/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4496 - val_loss: 173.3687\n",
      "Epoch 461/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4210 - val_loss: 173.3667\n",
      "Epoch 462/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4456 - val_loss: 173.4128\n",
      "Epoch 463/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3943 - val_loss: 173.3973\n",
      "Epoch 464/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3959 - val_loss: 173.4186\n",
      "Epoch 465/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3604 - val_loss: 173.4065\n",
      "Epoch 466/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3567 - val_loss: 173.4116\n",
      "Epoch 467/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3282 - val_loss: 173.3926\n",
      "Epoch 468/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3236 - val_loss: 173.4088\n",
      "Epoch 469/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3018 - val_loss: 173.4341\n",
      "Epoch 470/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3203 - val_loss: 173.3842\n",
      "Epoch 471/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2885 - val_loss: 173.4117\n",
      "Epoch 472/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2883 - val_loss: 173.4097\n",
      "Epoch 473/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2508 - val_loss: 173.4305\n",
      "Epoch 474/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2298 - val_loss: 173.4360\n",
      "Epoch 475/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2896 - val_loss: 173.3943\n",
      "Epoch 476/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2175 - val_loss: 173.4255\n",
      "Epoch 477/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2026 - val_loss: 173.4258\n",
      "Epoch 478/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1815 - val_loss: 173.4738\n",
      "Epoch 479/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1647 - val_loss: 173.4957\n",
      "Epoch 480/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1619 - val_loss: 173.5387\n",
      "Epoch 481/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1616 - val_loss: 173.5503\n",
      "Epoch 482/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1294 - val_loss: 173.5382\n",
      "Epoch 483/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1149 - val_loss: 173.5281\n",
      "Epoch 484/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1159 - val_loss: 173.5394\n",
      "Epoch 485/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1080 - val_loss: 173.5407\n",
      "Epoch 486/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0977 - val_loss: 173.5162\n",
      "Epoch 487/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0773 - val_loss: 173.5095\n",
      "Epoch 488/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0752 - val_loss: 173.5430\n",
      "Epoch 489/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0621 - val_loss: 173.5683\n",
      "Epoch 490/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0373 - val_loss: 173.5415\n",
      "Epoch 491/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0186 - val_loss: 173.5447\n",
      "Epoch 492/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0199 - val_loss: 173.5687\n",
      "Epoch 493/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0207 - val_loss: 173.5984\n",
      "Epoch 494/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9902 - val_loss: 173.5666\n",
      "Epoch 495/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9977 - val_loss: 173.5310\n",
      "Epoch 496/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9674 - val_loss: 173.5422\n",
      "Epoch 497/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9629 - val_loss: 173.5615\n",
      "Epoch 498/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9717 - val_loss: 173.6144\n",
      "Epoch 499/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9344 - val_loss: 173.6308\n",
      "Epoch 500/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9727 - val_loss: 173.6656\n",
      "Epoch 501/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9971 - val_loss: 173.6254\n",
      "Epoch 502/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9361 - val_loss: 173.6560\n",
      "Epoch 503/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9295 - val_loss: 173.6391\n",
      "Epoch 504/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9072 - val_loss: 173.6596\n",
      "Epoch 505/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8839 - val_loss: 173.6566\n",
      "Epoch 506/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9131 - val_loss: 173.6766\n",
      "Epoch 507/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8752 - val_loss: 173.6856\n",
      "Epoch 508/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8611 - val_loss: 173.6730\n",
      "Epoch 509/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8654 - val_loss: 173.6673\n",
      "Epoch 510/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8623 - val_loss: 173.6703\n",
      "Epoch 511/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8613 - val_loss: 173.6584\n",
      "Epoch 512/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8413 - val_loss: 173.6796\n",
      "Epoch 513/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8343 - val_loss: 173.6688\n",
      "Epoch 514/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8233 - val_loss: 173.6804\n",
      "Epoch 515/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8319 - val_loss: 173.6826\n",
      "Epoch 516/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7992 - val_loss: 173.6935\n",
      "Epoch 517/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8056 - val_loss: 173.7199\n",
      "Epoch 518/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8143 - val_loss: 173.7051\n",
      "Epoch 519/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7789 - val_loss: 173.7168\n",
      "Epoch 520/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7878 - val_loss: 173.7482\n",
      "Epoch 521/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7861 - val_loss: 173.7851\n",
      "Epoch 522/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7744 - val_loss: 173.8058\n",
      "Epoch 523/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7635 - val_loss: 173.8208\n",
      "Epoch 524/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7427 - val_loss: 173.8221\n",
      "Epoch 525/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7459 - val_loss: 173.8476\n",
      "Epoch 526/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7377 - val_loss: 173.8377\n",
      "Epoch 527/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7327 - val_loss: 173.8382\n",
      "Epoch 528/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7183 - val_loss: 173.8369\n",
      "Epoch 529/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7305 - val_loss: 173.8358\n",
      "Epoch 530/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7031 - val_loss: 173.8570\n",
      "Epoch 531/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7003 - val_loss: 173.8886\n",
      "Epoch 532/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6867 - val_loss: 173.9153\n",
      "Epoch 533/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6842 - val_loss: 173.9558\n",
      "Epoch 534/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6739 - val_loss: 173.9661\n",
      "Epoch 535/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6859 - val_loss: 173.9713\n",
      "Epoch 536/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6668 - val_loss: 174.0317\n",
      "Epoch 537/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7016 - val_loss: 174.0900\n",
      "Epoch 538/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6652 - val_loss: 174.0857\n",
      "Epoch 539/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6732 - val_loss: 174.0699\n",
      "Epoch 540/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6540 - val_loss: 174.0644\n",
      "Epoch 541/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7045 - val_loss: 174.1215\n",
      "Epoch 542/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6329 - val_loss: 174.1165\n",
      "Epoch 543/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6494 - val_loss: 174.1155\n",
      "Epoch 544/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6294 - val_loss: 174.0963\n",
      "Epoch 545/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6311 - val_loss: 174.1333\n",
      "Epoch 546/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6289 - val_loss: 174.1475\n",
      "Epoch 547/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6293 - val_loss: 174.1703\n",
      "Epoch 548/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6267 - val_loss: 174.1489\n",
      "Epoch 549/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5988 - val_loss: 174.1429\n",
      "Epoch 550/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6027 - val_loss: 174.1335\n",
      "Epoch 551/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5936 - val_loss: 174.1363\n",
      "Epoch 552/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5919 - val_loss: 174.1461\n",
      "Epoch 553/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5944 - val_loss: 174.1504\n",
      "Epoch 554/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6010 - val_loss: 174.1326\n",
      "Epoch 555/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5765 - val_loss: 174.1446\n",
      "Epoch 556/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5763 - val_loss: 174.1969\n",
      "Epoch 557/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5708 - val_loss: 174.2265\n",
      "Epoch 558/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5859 - val_loss: 174.2492\n",
      "Epoch 559/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5748 - val_loss: 174.2618\n",
      "Epoch 560/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5836 - val_loss: 174.2224\n",
      "Epoch 561/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5709 - val_loss: 174.2344\n",
      "Epoch 562/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5498 - val_loss: 174.2199\n",
      "Epoch 563/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5477 - val_loss: 174.2135\n",
      "Epoch 564/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5514 - val_loss: 174.2059\n",
      "Epoch 565/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5796 - val_loss: 174.2717\n",
      "Epoch 566/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5494 - val_loss: 174.2466\n",
      "Epoch 567/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5484 - val_loss: 174.2344\n",
      "Epoch 568/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5288 - val_loss: 174.2749\n",
      "Epoch 569/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5469 - val_loss: 174.3427\n",
      "Epoch 570/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5258 - val_loss: 174.3252\n",
      "Epoch 571/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.5072 - val_loss: 174.3515\n",
      "Epoch 572/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 221.5198 - val_loss: 174.3822\n",
      "Epoch 573/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.5216 - val_loss: 174.3970\n",
      "Epoch 574/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5268 - val_loss: 174.4526\n",
      "Epoch 575/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5204 - val_loss: 174.4109\n",
      "Epoch 576/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4969 - val_loss: 174.4278\n",
      "Epoch 577/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4901 - val_loss: 174.4489\n",
      "Epoch 578/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4920 - val_loss: 174.4887\n",
      "Epoch 579/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4928 - val_loss: 174.5161\n",
      "Epoch 580/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4674 - val_loss: 174.5013\n",
      "Epoch 581/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4634 - val_loss: 174.4809\n",
      "Epoch 582/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4823 - val_loss: 174.4837\n",
      "Epoch 583/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4829 - val_loss: 174.4688\n",
      "Epoch 584/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4765 - val_loss: 174.4604\n",
      "Epoch 585/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4676 - val_loss: 174.4589\n",
      "Epoch 586/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4581 - val_loss: 174.4836\n",
      "Epoch 587/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4608 - val_loss: 174.5066\n",
      "Epoch 588/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4641 - val_loss: 174.5278\n",
      "Epoch 589/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4693 - val_loss: 174.5695\n",
      "Epoch 590/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4888 - val_loss: 174.5288\n",
      "Epoch 591/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4554 - val_loss: 174.5728\n",
      "Epoch 592/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4511 - val_loss: 174.5859\n",
      "Epoch 593/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4451 - val_loss: 174.6088\n",
      "Epoch 594/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4616 - val_loss: 174.6453\n",
      "Epoch 595/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4390 - val_loss: 174.6273\n",
      "Epoch 596/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4407 - val_loss: 174.6081\n",
      "Epoch 597/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4361 - val_loss: 174.6122\n",
      "Epoch 598/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4351 - val_loss: 174.6081\n",
      "Epoch 599/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4256 - val_loss: 174.6313\n",
      "Epoch 600/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4543 - val_loss: 174.6613\n",
      "Epoch 601/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4180 - val_loss: 174.6753\n",
      "Epoch 602/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4208 - val_loss: 174.6712\n",
      "Epoch 603/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4260 - val_loss: 174.6675\n",
      "Epoch 604/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4050 - val_loss: 174.7005\n",
      "Epoch 605/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4147 - val_loss: 174.6924\n",
      "Epoch 606/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4052 - val_loss: 174.7117\n",
      "Epoch 607/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4003 - val_loss: 174.7275\n",
      "Epoch 608/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4026 - val_loss: 174.7234\n",
      "Epoch 609/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4149 - val_loss: 174.7865\n",
      "Epoch 610/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4235 - val_loss: 174.7618\n",
      "Epoch 611/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3887 - val_loss: 174.7589\n",
      "Epoch 612/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3893 - val_loss: 174.7735\n",
      "Epoch 613/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3820 - val_loss: 174.8332\n",
      "Epoch 614/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3923 - val_loss: 174.8826\n",
      "Epoch 615/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3890 - val_loss: 174.8651\n",
      "Epoch 616/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3923 - val_loss: 174.8631\n",
      "Epoch 617/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4048 - val_loss: 174.8420\n",
      "Epoch 618/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3767 - val_loss: 174.8458\n",
      "Epoch 619/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3764 - val_loss: 174.8443\n",
      "Epoch 620/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3800 - val_loss: 174.8813\n",
      "Epoch 621/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3661 - val_loss: 174.8951\n",
      "Epoch 622/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3678 - val_loss: 174.8904\n",
      "Epoch 623/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3822 - val_loss: 174.9362\n",
      "Epoch 624/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3662 - val_loss: 174.9134\n",
      "Epoch 625/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3640 - val_loss: 174.9404\n",
      "Epoch 626/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3617 - val_loss: 174.9356\n",
      "Epoch 627/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3565 - val_loss: 174.9519\n",
      "Epoch 628/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3625 - val_loss: 174.9410\n",
      "Epoch 629/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3585 - val_loss: 174.9238\n",
      "Epoch 630/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3424 - val_loss: 174.9415\n",
      "Epoch 631/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3448 - val_loss: 174.9533\n",
      "Epoch 632/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3694 - val_loss: 175.0190\n",
      "Epoch 633/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3659 - val_loss: 175.0235\n",
      "Epoch 634/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3645 - val_loss: 175.0568\n",
      "Epoch 635/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3474 - val_loss: 175.0320\n",
      "Epoch 636/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3416 - val_loss: 175.0449\n",
      "Epoch 637/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3525 - val_loss: 175.0560\n",
      "Epoch 638/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3591 - val_loss: 175.0769\n",
      "Epoch 639/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3420 - val_loss: 175.0472\n",
      "Epoch 640/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3412 - val_loss: 175.0349\n",
      "Epoch 641/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3471 - val_loss: 175.0206\n",
      "Epoch 642/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3249 - val_loss: 175.0183\n",
      "Epoch 643/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3497 - val_loss: 174.9907\n",
      "Epoch 644/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3349 - val_loss: 175.0335\n",
      "Epoch 645/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3268 - val_loss: 175.0667\n",
      "Epoch 646/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3369 - val_loss: 175.0477\n",
      "Epoch 647/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3176 - val_loss: 175.0564\n",
      "Epoch 648/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3237 - val_loss: 175.0717\n",
      "Epoch 649/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3292 - val_loss: 175.0997\n",
      "Epoch 650/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3260 - val_loss: 175.0862\n",
      "Epoch 651/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3145 - val_loss: 175.0769\n",
      "Epoch 652/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3225 - val_loss: 175.0961\n",
      "Epoch 653/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3105 - val_loss: 175.0849\n",
      "Epoch 654/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3380 - val_loss: 175.0713\n",
      "Epoch 655/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3230 - val_loss: 175.1082\n",
      "Epoch 656/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3196 - val_loss: 175.1082\n",
      "Epoch 657/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3208 - val_loss: 175.1022\n",
      "Epoch 658/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3130 - val_loss: 175.1402\n",
      "Epoch 659/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3101 - val_loss: 175.1178\n",
      "Epoch 660/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3211 - val_loss: 175.1552\n",
      "Epoch 661/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3001 - val_loss: 175.1457\n",
      "Epoch 662/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3016 - val_loss: 175.1198\n",
      "Epoch 663/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3405 - val_loss: 175.0925\n",
      "Epoch 664/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2929 - val_loss: 175.1044\n",
      "Epoch 665/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3046 - val_loss: 175.1439\n",
      "Epoch 666/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2940 - val_loss: 175.1761\n",
      "Epoch 667/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2981 - val_loss: 175.1915\n",
      "Epoch 668/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2874 - val_loss: 175.1964\n",
      "Epoch 669/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2942 - val_loss: 175.2100\n",
      "Epoch 670/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2870 - val_loss: 175.2178\n",
      "Epoch 671/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2877 - val_loss: 175.2386\n",
      "Epoch 672/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3316 - val_loss: 175.2097\n",
      "Epoch 673/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2955 - val_loss: 175.2613\n",
      "Epoch 674/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2851 - val_loss: 175.2595\n",
      "Epoch 675/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2892 - val_loss: 175.2901\n",
      "Epoch 676/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2845 - val_loss: 175.2870\n",
      "Epoch 677/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2829 - val_loss: 175.3044\n",
      "Epoch 678/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2785 - val_loss: 175.3264\n",
      "Epoch 679/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2765 - val_loss: 175.3670\n",
      "Epoch 680/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2813 - val_loss: 175.3764\n",
      "Epoch 681/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2764 - val_loss: 175.3757\n",
      "Epoch 682/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2939 - val_loss: 175.3580\n",
      "Epoch 683/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2800 - val_loss: 175.3793\n",
      "Epoch 684/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2864 - val_loss: 175.4165\n",
      "Epoch 685/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2901 - val_loss: 175.4146\n",
      "Epoch 686/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2845 - val_loss: 175.4461\n",
      "Epoch 687/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2884 - val_loss: 175.4524\n",
      "Epoch 688/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2714 - val_loss: 175.4579\n",
      "Epoch 689/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3097 - val_loss: 175.4131\n",
      "Epoch 690/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2545 - val_loss: 175.4339\n",
      "Epoch 691/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2603 - val_loss: 175.4608\n",
      "Epoch 692/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2584 - val_loss: 175.4822\n",
      "Epoch 693/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2614 - val_loss: 175.4793\n",
      "Epoch 694/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2567 - val_loss: 175.4859\n",
      "Epoch 695/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2785 - val_loss: 175.5149\n",
      "Epoch 696/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2673 - val_loss: 175.4776\n",
      "Epoch 697/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2507 - val_loss: 175.4942\n",
      "Epoch 698/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2578 - val_loss: 175.4933\n",
      "Epoch 699/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2590 - val_loss: 175.5013\n",
      "Epoch 700/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2574 - val_loss: 175.5290\n",
      "Epoch 701/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2677 - val_loss: 175.5367\n",
      "Epoch 702/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2460 - val_loss: 175.5169\n",
      "Epoch 703/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2699 - val_loss: 175.4924\n",
      "Epoch 704/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2699 - val_loss: 175.5344\n",
      "Epoch 705/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2499 - val_loss: 175.5546\n",
      "Epoch 706/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2980 - val_loss: 175.6051\n",
      "Epoch 707/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2725 - val_loss: 175.5522\n",
      "Epoch 708/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3367 - val_loss: 175.6273\n",
      "Epoch 709/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2396 - val_loss: 175.5940\n",
      "Epoch 710/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2365 - val_loss: 175.5723\n",
      "Epoch 711/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2717 - val_loss: 175.5462\n",
      "Epoch 712/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2922 - val_loss: 175.5903\n",
      "Epoch 713/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2402 - val_loss: 175.6198\n",
      "Epoch 714/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2923 - val_loss: 175.5740\n",
      "Epoch 715/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2561 - val_loss: 175.6122\n",
      "Epoch 716/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2502 - val_loss: 175.6126\n",
      "Epoch 717/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2272 - val_loss: 175.6501\n",
      "Epoch 718/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2579 - val_loss: 175.6982\n",
      "Epoch 719/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2542 - val_loss: 175.6700\n",
      "Epoch 720/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2375 - val_loss: 175.6857\n",
      "Epoch 721/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2292 - val_loss: 175.7032\n",
      "Epoch 722/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2371 - val_loss: 175.6767\n",
      "Epoch 723/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2266 - val_loss: 175.6960\n",
      "Epoch 724/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2266 - val_loss: 175.7029\n",
      "Epoch 725/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2414 - val_loss: 175.6776\n",
      "Epoch 726/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2306 - val_loss: 175.6871\n",
      "Epoch 727/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2347 - val_loss: 175.6836\n",
      "Epoch 728/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2868 - val_loss: 175.7603\n",
      "Epoch 729/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2269 - val_loss: 175.7685\n",
      "Epoch 730/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2212 - val_loss: 175.7528\n",
      "Epoch 731/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2209 - val_loss: 175.7620\n",
      "Epoch 732/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2155 - val_loss: 175.7826\n",
      "Epoch 733/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2158 - val_loss: 175.7854\n",
      "Epoch 734/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2119 - val_loss: 175.7747\n",
      "Epoch 735/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2234 - val_loss: 175.8074\n",
      "Epoch 736/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2542 - val_loss: 175.7812\n",
      "Epoch 737/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2375 - val_loss: 175.8223\n",
      "Epoch 738/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2130 - val_loss: 175.8258\n",
      "Epoch 739/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2245 - val_loss: 175.7986\n",
      "Epoch 740/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2009 - val_loss: 175.7904\n",
      "Epoch 741/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2114 - val_loss: 175.7916\n",
      "Epoch 742/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2093 - val_loss: 175.7983\n",
      "Epoch 743/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2155 - val_loss: 175.7852\n",
      "Epoch 744/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2099 - val_loss: 175.7889\n",
      "Epoch 745/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2109 - val_loss: 175.7843\n",
      "Epoch 746/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2446 - val_loss: 175.7522\n",
      "Epoch 747/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2048 - val_loss: 175.7817\n",
      "Epoch 748/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2102 - val_loss: 175.8192\n",
      "Epoch 749/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2108 - val_loss: 175.8063\n",
      "Epoch 750/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2036 - val_loss: 175.8404\n",
      ">Saved ./models/comp4948_a2_model_3.h5\n",
      "Epoch 1/750\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 4523.1948 - val_loss: 4519.7480\n",
      "Epoch 2/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4516.9502 - val_loss: 4513.6519\n",
      "Epoch 3/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4510.8032 - val_loss: 4507.5073\n",
      "Epoch 4/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4504.6494 - val_loss: 4501.3599\n",
      "Epoch 5/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4498.5103 - val_loss: 4495.0981\n",
      "Epoch 6/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4492.2100 - val_loss: 4488.7168\n",
      "Epoch 7/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4485.7817 - val_loss: 4482.2871\n",
      "Epoch 8/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4479.3398 - val_loss: 4475.9062\n",
      "Epoch 9/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4473.1343 - val_loss: 4469.6406\n",
      "Epoch 10/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4466.8462 - val_loss: 4463.2231\n",
      "Epoch 11/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4460.3262 - val_loss: 4456.6860\n",
      "Epoch 12/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4453.8125 - val_loss: 4449.9727\n",
      "Epoch 13/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4447.0015 - val_loss: 4443.1367\n",
      "Epoch 14/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4440.1011 - val_loss: 4436.0449\n",
      "Epoch 15/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4432.9097 - val_loss: 4428.7690\n",
      "Epoch 16/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4425.5947 - val_loss: 4421.4209\n",
      "Epoch 17/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4418.3208 - val_loss: 4414.1128\n",
      "Epoch 18/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4410.9268 - val_loss: 4406.6504\n",
      "Epoch 19/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4403.4297 - val_loss: 4398.9497\n",
      "Epoch 20/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4395.6260 - val_loss: 4391.0791\n",
      "Epoch 21/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4387.7905 - val_loss: 4383.1113\n",
      "Epoch 22/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4379.8179 - val_loss: 4374.9854\n",
      "Epoch 23/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4371.6455 - val_loss: 4366.8164\n",
      "Epoch 24/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4363.3428 - val_loss: 4358.4771\n",
      "Epoch 25/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4355.2153 - val_loss: 4350.2886\n",
      "Epoch 26/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4347.0376 - val_loss: 4341.7583\n",
      "Epoch 27/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4338.3428 - val_loss: 4333.1011\n",
      "Epoch 28/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4329.6621 - val_loss: 4324.4199\n",
      "Epoch 29/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4321.0156 - val_loss: 4315.5781\n",
      "Epoch 30/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4312.0688 - val_loss: 4306.5171\n",
      "Epoch 31/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4303.1548 - val_loss: 4297.4292\n",
      "Epoch 32/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4293.9580 - val_loss: 4288.1440\n",
      "Epoch 33/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4284.5518 - val_loss: 4278.5786\n",
      "Epoch 34/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4274.8931 - val_loss: 4268.7417\n",
      "Epoch 35/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4265.0547 - val_loss: 4258.7974\n",
      "Epoch 36/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4255.0674 - val_loss: 4248.5435\n",
      "Epoch 37/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4244.5874 - val_loss: 4238.3018\n",
      "Epoch 38/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4234.6724 - val_loss: 4228.1787\n",
      "Epoch 39/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4224.3940 - val_loss: 4217.8042\n",
      "Epoch 40/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4214.0073 - val_loss: 4207.1055\n",
      "Epoch 41/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4203.2012 - val_loss: 4196.2373\n",
      "Epoch 42/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4192.2144 - val_loss: 4185.1577\n",
      "Epoch 43/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4181.1299 - val_loss: 4173.7310\n",
      "Epoch 44/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4169.5737 - val_loss: 4162.1001\n",
      "Epoch 45/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4157.9648 - val_loss: 4150.3730\n",
      "Epoch 46/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4146.2104 - val_loss: 4138.4082\n",
      "Epoch 47/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4134.1875 - val_loss: 4126.2988\n",
      "Epoch 48/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4122.4272 - val_loss: 4114.3901\n",
      "Epoch 49/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4110.5093 - val_loss: 4102.4429\n",
      "Epoch 50/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4098.3228 - val_loss: 4090.3616\n",
      "Epoch 51/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4086.3499 - val_loss: 4078.0212\n",
      "Epoch 52/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4074.0571 - val_loss: 4065.5586\n",
      "Epoch 53/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4061.4099 - val_loss: 4052.9133\n",
      "Epoch 54/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4048.6057 - val_loss: 4040.0554\n",
      "Epoch 55/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4035.8618 - val_loss: 4026.9744\n",
      "Epoch 56/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4022.7136 - val_loss: 4013.6938\n",
      "Epoch 57/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4009.4451 - val_loss: 4000.0042\n",
      "Epoch 58/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3995.6565 - val_loss: 3986.0901\n",
      "Epoch 59/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3981.8008 - val_loss: 3972.3586\n",
      "Epoch 60/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3968.1272 - val_loss: 3958.5139\n",
      "Epoch 61/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3954.2695 - val_loss: 3944.3850\n",
      "Epoch 62/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3940.0654 - val_loss: 3930.1768\n",
      "Epoch 63/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3926.2268 - val_loss: 3916.1165\n",
      "Epoch 64/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3912.0225 - val_loss: 3901.8350\n",
      "Epoch 65/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3897.6992 - val_loss: 3887.4119\n",
      "Epoch 66/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3883.4858 - val_loss: 3873.0767\n",
      "Epoch 67/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3869.0698 - val_loss: 3858.4443\n",
      "Epoch 68/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3854.1279 - val_loss: 3843.7393\n",
      "Epoch 69/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3839.5820 - val_loss: 3829.0413\n",
      "Epoch 70/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3825.0459 - val_loss: 3814.1516\n",
      "Epoch 71/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3809.9978 - val_loss: 3799.1201\n",
      "Epoch 72/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3794.8782 - val_loss: 3783.7131\n",
      "Epoch 73/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3779.4702 - val_loss: 3767.8796\n",
      "Epoch 74/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3763.5720 - val_loss: 3751.8870\n",
      "Epoch 75/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3747.6084 - val_loss: 3735.6523\n",
      "Epoch 76/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3731.3291 - val_loss: 3719.1748\n",
      "Epoch 77/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3715.0022 - val_loss: 3702.7500\n",
      "Epoch 78/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3698.4143 - val_loss: 3686.0940\n",
      "Epoch 79/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3681.8831 - val_loss: 3669.6868\n",
      "Epoch 80/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3665.6829 - val_loss: 3653.1882\n",
      "Epoch 81/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3649.0840 - val_loss: 3636.4026\n",
      "Epoch 82/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3632.2136 - val_loss: 3619.3523\n",
      "Epoch 83/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3614.9250 - val_loss: 3602.2842\n",
      "Epoch 84/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3598.0200 - val_loss: 3585.0154\n",
      "Epoch 85/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3580.8110 - val_loss: 3567.2830\n",
      "Epoch 86/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3562.8823 - val_loss: 3549.6580\n",
      "Epoch 87/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3545.4590 - val_loss: 3531.9858\n",
      "Epoch 88/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3527.8093 - val_loss: 3514.0029\n",
      "Epoch 89/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3509.6877 - val_loss: 3495.9553\n",
      "Epoch 90/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3491.7502 - val_loss: 3477.8821\n",
      "Epoch 91/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3474.1416 - val_loss: 3459.7366\n",
      "Epoch 92/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3455.8728 - val_loss: 3441.5664\n",
      "Epoch 93/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3437.8372 - val_loss: 3422.9485\n",
      "Epoch 94/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3419.1169 - val_loss: 3404.0798\n",
      "Epoch 95/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3400.2078 - val_loss: 3385.1140\n",
      "Epoch 96/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3381.4690 - val_loss: 3366.2095\n",
      "Epoch 97/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3362.6272 - val_loss: 3347.1631\n",
      "Epoch 98/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3343.5869 - val_loss: 3328.2102\n",
      "Epoch 99/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3324.5164 - val_loss: 3309.0977\n",
      "Epoch 100/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3305.3750 - val_loss: 3289.7061\n",
      "Epoch 101/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3286.0173 - val_loss: 3269.9509\n",
      "Epoch 102/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3266.0618 - val_loss: 3250.1812\n",
      "Epoch 103/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3246.7983 - val_loss: 3230.7007\n",
      "Epoch 104/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3227.4189 - val_loss: 3211.0413\n",
      "Epoch 105/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3207.7173 - val_loss: 3191.1143\n",
      "Epoch 106/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3188.0430 - val_loss: 3170.8579\n",
      "Epoch 107/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3168.0571 - val_loss: 3150.6399\n",
      "Epoch 108/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3147.7529 - val_loss: 3130.3811\n",
      "Epoch 109/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3127.5557 - val_loss: 3110.0432\n",
      "Epoch 110/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3107.1836 - val_loss: 3089.5295\n",
      "Epoch 111/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3086.7173 - val_loss: 3068.9126\n",
      "Epoch 112/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3066.1741 - val_loss: 3047.9177\n",
      "Epoch 113/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3044.9548 - val_loss: 3026.8579\n",
      "Epoch 114/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3024.4539 - val_loss: 3006.0781\n",
      "Epoch 115/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3003.4578 - val_loss: 2985.1304\n",
      "Epoch 116/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2982.4692 - val_loss: 2963.9622\n",
      "Epoch 117/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2961.2598 - val_loss: 2942.7190\n",
      "Epoch 118/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2940.5928 - val_loss: 2922.0972\n",
      "Epoch 119/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2920.0842 - val_loss: 2901.4541\n",
      "Epoch 120/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2899.4272 - val_loss: 2880.7576\n",
      "Epoch 121/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2878.9377 - val_loss: 2859.7227\n",
      "Epoch 122/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2857.8330 - val_loss: 2838.5139\n",
      "Epoch 123/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2836.7668 - val_loss: 2816.9492\n",
      "Epoch 124/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2815.1177 - val_loss: 2795.1638\n",
      "Epoch 125/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2793.4368 - val_loss: 2773.1533\n",
      "Epoch 126/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2771.8958 - val_loss: 2751.4448\n",
      "Epoch 127/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2750.3774 - val_loss: 2730.1025\n",
      "Epoch 128/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2729.0903 - val_loss: 2708.4377\n",
      "Epoch 129/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2707.4058 - val_loss: 2686.7903\n",
      "Epoch 130/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2686.1926 - val_loss: 2664.6858\n",
      "Epoch 131/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2664.0732 - val_loss: 2642.7451\n",
      "Epoch 132/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2642.1758 - val_loss: 2620.9312\n",
      "Epoch 133/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2620.5586 - val_loss: 2598.7620\n",
      "Epoch 134/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2598.4880 - val_loss: 2576.4392\n",
      "Epoch 135/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2576.1177 - val_loss: 2554.0557\n",
      "Epoch 136/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2553.8479 - val_loss: 2531.4993\n",
      "Epoch 137/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2531.4204 - val_loss: 2508.6350\n",
      "Epoch 138/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2508.5388 - val_loss: 2485.5645\n",
      "Epoch 139/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2485.5420 - val_loss: 2462.4858\n",
      "Epoch 140/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2462.8123 - val_loss: 2439.2502\n",
      "Epoch 141/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2439.5352 - val_loss: 2416.0076\n",
      "Epoch 142/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2416.6711 - val_loss: 2392.7979\n",
      "Epoch 143/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2393.7698 - val_loss: 2370.1523\n",
      "Epoch 144/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2371.3718 - val_loss: 2347.6814\n",
      "Epoch 145/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2348.8875 - val_loss: 2325.0493\n",
      "Epoch 146/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2326.5586 - val_loss: 2302.6218\n",
      "Epoch 147/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2304.4963 - val_loss: 2280.3181\n",
      "Epoch 148/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2282.1404 - val_loss: 2258.0110\n",
      "Epoch 149/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2259.8235 - val_loss: 2235.5840\n",
      "Epoch 150/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2237.5081 - val_loss: 2212.8584\n",
      "Epoch 151/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2214.9548 - val_loss: 2190.0276\n",
      "Epoch 152/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2192.2878 - val_loss: 2166.9651\n",
      "Epoch 153/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2169.3037 - val_loss: 2143.7920\n",
      "Epoch 154/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2146.0615 - val_loss: 2120.6528\n",
      "Epoch 155/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2123.3042 - val_loss: 2097.3286\n",
      "Epoch 156/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2100.1526 - val_loss: 2074.2625\n",
      "Epoch 157/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2077.2417 - val_loss: 2051.1260\n",
      "Epoch 158/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2054.2617 - val_loss: 2028.0363\n",
      "Epoch 159/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2031.1884 - val_loss: 2005.0767\n",
      "Epoch 160/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2008.7914 - val_loss: 1982.4972\n",
      "Epoch 161/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1986.7150 - val_loss: 1960.3511\n",
      "Epoch 162/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1964.3828 - val_loss: 1938.3068\n",
      "Epoch 163/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1942.5607 - val_loss: 1916.0504\n",
      "Epoch 164/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1920.3389 - val_loss: 1893.9823\n",
      "Epoch 165/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1898.5883 - val_loss: 1871.7228\n",
      "Epoch 166/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1876.3840 - val_loss: 1849.4620\n",
      "Epoch 167/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1854.1749 - val_loss: 1827.0867\n",
      "Epoch 168/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1832.0353 - val_loss: 1804.7642\n",
      "Epoch 169/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1809.9305 - val_loss: 1782.5344\n",
      "Epoch 170/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1787.8605 - val_loss: 1760.2711\n",
      "Epoch 171/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1765.5573 - val_loss: 1738.1649\n",
      "Epoch 172/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1743.6379 - val_loss: 1716.2516\n",
      "Epoch 173/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1721.7800 - val_loss: 1694.1932\n",
      "Epoch 174/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1700.0563 - val_loss: 1671.9279\n",
      "Epoch 175/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1677.7529 - val_loss: 1649.8199\n",
      "Epoch 176/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1655.9033 - val_loss: 1627.6265\n",
      "Epoch 177/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1634.0442 - val_loss: 1605.3214\n",
      "Epoch 178/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1611.8292 - val_loss: 1583.1232\n",
      "Epoch 179/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1589.7345 - val_loss: 1561.0242\n",
      "Epoch 180/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1567.9517 - val_loss: 1539.2755\n",
      "Epoch 181/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1546.8582 - val_loss: 1517.8885\n",
      "Epoch 182/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1525.2867 - val_loss: 1496.7328\n",
      "Epoch 183/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1504.6075 - val_loss: 1475.2300\n",
      "Epoch 184/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1483.0267 - val_loss: 1453.9808\n",
      "Epoch 185/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1462.0413 - val_loss: 1432.5895\n",
      "Epoch 186/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1440.9625 - val_loss: 1411.1532\n",
      "Epoch 187/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1419.6948 - val_loss: 1390.0742\n",
      "Epoch 188/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1398.8983 - val_loss: 1369.0538\n",
      "Epoch 189/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1378.2600 - val_loss: 1347.9839\n",
      "Epoch 190/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1357.2147 - val_loss: 1327.0669\n",
      "Epoch 191/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1336.5741 - val_loss: 1306.1331\n",
      "Epoch 192/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1315.9493 - val_loss: 1285.2173\n",
      "Epoch 193/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1295.5050 - val_loss: 1264.3513\n",
      "Epoch 194/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1274.5645 - val_loss: 1243.8131\n",
      "Epoch 195/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1254.4454 - val_loss: 1223.1987\n",
      "Epoch 196/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1234.4219 - val_loss: 1202.5262\n",
      "Epoch 197/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1213.9652 - val_loss: 1182.2966\n",
      "Epoch 198/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1193.9843 - val_loss: 1162.2208\n",
      "Epoch 199/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1174.4288 - val_loss: 1142.4279\n",
      "Epoch 200/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1155.1482 - val_loss: 1122.8246\n",
      "Epoch 201/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1135.5885 - val_loss: 1103.5614\n",
      "Epoch 202/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1117.0204 - val_loss: 1084.0913\n",
      "Epoch 203/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1097.2996 - val_loss: 1065.3494\n",
      "Epoch 204/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1079.1489 - val_loss: 1046.6227\n",
      "Epoch 205/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1061.0785 - val_loss: 1027.8085\n",
      "Epoch 206/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1042.7594 - val_loss: 1009.1959\n",
      "Epoch 207/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1024.4603 - val_loss: 990.8713\n",
      "Epoch 208/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1006.5809 - val_loss: 972.6136\n",
      "Epoch 209/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 988.7574 - val_loss: 954.5494\n",
      "Epoch 210/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 971.2781 - val_loss: 936.5705\n",
      "Epoch 211/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 953.3337 - val_loss: 918.9849\n",
      "Epoch 212/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 936.3311 - val_loss: 901.3720\n",
      "Epoch 213/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 919.1288 - val_loss: 883.8936\n",
      "Epoch 214/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 902.0617 - val_loss: 866.5352\n",
      "Epoch 215/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 885.0538 - val_loss: 849.4359\n",
      "Epoch 216/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 868.5245 - val_loss: 832.4348\n",
      "Epoch 217/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 852.0478 - val_loss: 815.9688\n",
      "Epoch 218/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 835.8707 - val_loss: 799.8168\n",
      "Epoch 219/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 820.0004 - val_loss: 783.8703\n",
      "Epoch 220/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 804.5717 - val_loss: 767.9771\n",
      "Epoch 221/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 788.8066 - val_loss: 752.4189\n",
      "Epoch 222/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 773.9700 - val_loss: 736.7462\n",
      "Epoch 223/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 758.3956 - val_loss: 721.5844\n",
      "Epoch 224/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 743.8159 - val_loss: 706.5144\n",
      "Epoch 225/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 729.4289 - val_loss: 691.5268\n",
      "Epoch 226/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 714.7948 - val_loss: 676.9401\n",
      "Epoch 227/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 700.7600 - val_loss: 662.5254\n",
      "Epoch 228/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 686.8014 - val_loss: 648.3746\n",
      "Epoch 229/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 672.8856 - val_loss: 634.6849\n",
      "Epoch 230/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 659.7068 - val_loss: 621.0231\n",
      "Epoch 231/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 646.6256 - val_loss: 607.4791\n",
      "Epoch 232/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 633.4529 - val_loss: 594.2770\n",
      "Epoch 233/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 620.7455 - val_loss: 581.3280\n",
      "Epoch 234/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 608.3214 - val_loss: 568.5634\n",
      "Epoch 235/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 596.3658 - val_loss: 555.8779\n",
      "Epoch 236/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 583.7019 - val_loss: 543.8997\n",
      "Epoch 237/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 572.1500 - val_loss: 532.0058\n",
      "Epoch 238/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 560.5927 - val_loss: 520.3959\n",
      "Epoch 239/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 549.7352 - val_loss: 508.7409\n",
      "Epoch 240/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 538.2003 - val_loss: 497.6580\n",
      "Epoch 241/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 527.4825 - val_loss: 486.8045\n",
      "Epoch 242/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 517.0936 - val_loss: 476.1514\n",
      "Epoch 243/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 506.9157 - val_loss: 465.7352\n",
      "Epoch 244/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 496.6246 - val_loss: 455.7115\n",
      "Epoch 245/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 487.2782 - val_loss: 445.6655\n",
      "Epoch 246/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 477.8396 - val_loss: 435.8895\n",
      "Epoch 247/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 468.3170 - val_loss: 426.5314\n",
      "Epoch 248/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 459.3838 - val_loss: 417.4055\n",
      "Epoch 249/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 450.7166 - val_loss: 408.4600\n",
      "Epoch 250/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 442.0986 - val_loss: 399.8214\n",
      "Epoch 251/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 433.6391 - val_loss: 391.4692\n",
      "Epoch 252/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 425.7463 - val_loss: 383.2153\n",
      "Epoch 253/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 418.0441 - val_loss: 375.1128\n",
      "Epoch 254/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 410.2018 - val_loss: 367.3996\n",
      "Epoch 255/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 402.8745 - val_loss: 359.8117\n",
      "Epoch 256/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 395.3841 - val_loss: 352.5012\n",
      "Epoch 257/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 388.7813 - val_loss: 345.1259\n",
      "Epoch 258/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 381.8534 - val_loss: 338.1067\n",
      "Epoch 259/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 374.7087 - val_loss: 331.6076\n",
      "Epoch 260/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 368.7982 - val_loss: 324.9391\n",
      "Epoch 261/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 362.5797 - val_loss: 318.5137\n",
      "Epoch 262/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 356.5370 - val_loss: 312.3619\n",
      "Epoch 263/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 350.8724 - val_loss: 306.3613\n",
      "Epoch 264/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 344.8310 - val_loss: 300.8399\n",
      "Epoch 265/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 339.8163 - val_loss: 295.2412\n",
      "Epoch 266/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 334.6718 - val_loss: 289.8149\n",
      "Epoch 267/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 329.5305 - val_loss: 284.6943\n",
      "Epoch 268/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 324.6628 - val_loss: 279.8178\n",
      "Epoch 269/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 320.1015 - val_loss: 275.1165\n",
      "Epoch 270/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 315.8558 - val_loss: 270.4746\n",
      "Epoch 271/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 311.4173 - val_loss: 266.0608\n",
      "Epoch 272/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 307.2628 - val_loss: 261.7883\n",
      "Epoch 273/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 303.4931 - val_loss: 257.5355\n",
      "Epoch 274/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 299.5702 - val_loss: 253.5429\n",
      "Epoch 275/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 295.5240 - val_loss: 249.9663\n",
      "Epoch 276/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 292.3273 - val_loss: 246.2686\n",
      "Epoch 277/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 289.0385 - val_loss: 242.6989\n",
      "Epoch 278/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 285.7677 - val_loss: 239.3259\n",
      "Epoch 279/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 282.6872 - val_loss: 236.1128\n",
      "Epoch 280/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 279.7491 - val_loss: 233.1071\n",
      "Epoch 281/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 276.9126 - val_loss: 230.3007\n",
      "Epoch 282/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 274.3316 - val_loss: 227.5035\n",
      "Epoch 283/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 272.0150 - val_loss: 224.6773\n",
      "Epoch 284/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 269.3783 - val_loss: 222.1239\n",
      "Epoch 285/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 266.9784 - val_loss: 219.7414\n",
      "Epoch 286/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 264.7923 - val_loss: 217.4326\n",
      "Epoch 287/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 262.6517 - val_loss: 215.2245\n",
      "Epoch 288/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 260.7216 - val_loss: 213.0661\n",
      "Epoch 289/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 258.8264 - val_loss: 210.9736\n",
      "Epoch 290/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 256.9293 - val_loss: 209.0198\n",
      "Epoch 291/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 255.0967 - val_loss: 207.1993\n",
      "Epoch 292/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 253.6849 - val_loss: 205.3047\n",
      "Epoch 293/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 251.8586 - val_loss: 203.6858\n",
      "Epoch 294/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 250.5092 - val_loss: 202.0509\n",
      "Epoch 295/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 249.0998 - val_loss: 200.5210\n",
      "Epoch 296/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 247.7030 - val_loss: 199.1098\n",
      "Epoch 297/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 246.4468 - val_loss: 197.7901\n",
      "Epoch 298/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 245.3152 - val_loss: 196.5188\n",
      "Epoch 299/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 244.2623 - val_loss: 195.2699\n",
      "Epoch 300/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 243.1623 - val_loss: 194.1035\n",
      "Epoch 301/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 242.1189 - val_loss: 192.9885\n",
      "Epoch 302/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 241.1189 - val_loss: 191.9510\n",
      "Epoch 303/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 240.1996 - val_loss: 190.9662\n",
      "Epoch 304/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 239.4281 - val_loss: 189.9752\n",
      "Epoch 305/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 238.6092 - val_loss: 189.0582\n",
      "Epoch 306/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 237.9638 - val_loss: 188.1299\n",
      "Epoch 307/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 237.0708 - val_loss: 187.3791\n",
      "Epoch 308/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 236.3389 - val_loss: 186.6946\n",
      "Epoch 309/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 235.8635 - val_loss: 185.8862\n",
      "Epoch 310/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 235.0960 - val_loss: 185.2255\n",
      "Epoch 311/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 234.6717 - val_loss: 184.4919\n",
      "Epoch 312/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 233.9785 - val_loss: 183.8797\n",
      "Epoch 313/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 233.5908 - val_loss: 183.2251\n",
      "Epoch 314/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232.9645 - val_loss: 182.7246\n",
      "Epoch 315/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232.5590 - val_loss: 182.2252\n",
      "Epoch 316/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232.1242 - val_loss: 181.7397\n",
      "Epoch 317/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 231.7217 - val_loss: 181.2725\n",
      "Epoch 318/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 231.3588 - val_loss: 180.8367\n",
      "Epoch 319/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 231.0792 - val_loss: 180.3776\n",
      "Epoch 320/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.6710 - val_loss: 180.0069\n",
      "Epoch 321/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 230.4129 - val_loss: 179.6175\n",
      "Epoch 322/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.0710 - val_loss: 179.2889\n",
      "Epoch 323/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.7994 - val_loss: 178.9605\n",
      "Epoch 324/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.5667 - val_loss: 178.6436\n",
      "Epoch 325/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.3106 - val_loss: 178.3087\n",
      "Epoch 326/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.0593 - val_loss: 178.0312\n",
      "Epoch 327/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.8156 - val_loss: 177.7790\n",
      "Epoch 328/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.6153 - val_loss: 177.5418\n",
      "Epoch 329/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.4268 - val_loss: 177.3091\n",
      "Epoch 330/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.2173 - val_loss: 177.1091\n",
      "Epoch 331/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.1186 - val_loss: 176.8518\n",
      "Epoch 332/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.9117 - val_loss: 176.6407\n",
      "Epoch 333/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.7232 - val_loss: 176.4989\n",
      "Epoch 334/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.6160 - val_loss: 176.2828\n",
      "Epoch 335/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.4620 - val_loss: 176.1004\n",
      "Epoch 336/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.3151 - val_loss: 175.9311\n",
      "Epoch 337/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.1686 - val_loss: 175.7850\n",
      "Epoch 338/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.0392 - val_loss: 175.6440\n",
      "Epoch 339/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.9362 - val_loss: 175.4837\n",
      "Epoch 340/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.8115 - val_loss: 175.3358\n",
      "Epoch 341/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.7325 - val_loss: 175.1824\n",
      "Epoch 342/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.5895 - val_loss: 175.0797\n",
      "Epoch 343/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.4998 - val_loss: 174.9955\n",
      "Epoch 344/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.4146 - val_loss: 174.8664\n",
      "Epoch 345/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.3134 - val_loss: 174.7652\n",
      "Epoch 346/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.2514 - val_loss: 174.6610\n",
      "Epoch 347/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.1278 - val_loss: 174.5885\n",
      "Epoch 348/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.1043 - val_loss: 174.4617\n",
      "Epoch 349/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.9600 - val_loss: 174.3864\n",
      "Epoch 350/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.9137 - val_loss: 174.2743\n",
      "Epoch 351/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.8305 - val_loss: 174.1985\n",
      "Epoch 352/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.7349 - val_loss: 174.1309\n",
      "Epoch 353/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.6786 - val_loss: 174.0304\n",
      "Epoch 354/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.5890 - val_loss: 173.9754\n",
      "Epoch 355/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.5917 - val_loss: 173.8696\n",
      "Epoch 356/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.4708 - val_loss: 173.8140\n",
      "Epoch 357/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.3890 - val_loss: 173.7638\n",
      "Epoch 358/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.3490 - val_loss: 173.7236\n",
      "Epoch 359/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.2778 - val_loss: 173.6629\n",
      "Epoch 360/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.2424 - val_loss: 173.5637\n",
      "Epoch 361/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.1555 - val_loss: 173.5029\n",
      "Epoch 362/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.1089 - val_loss: 173.4898\n",
      "Epoch 363/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.0697 - val_loss: 173.4356\n",
      "Epoch 364/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.9851 - val_loss: 173.3980\n",
      "Epoch 365/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.9412 - val_loss: 173.3348\n",
      "Epoch 366/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.8767 - val_loss: 173.2821\n",
      "Epoch 367/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.8433 - val_loss: 173.2396\n",
      "Epoch 368/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.7984 - val_loss: 173.1801\n",
      "Epoch 369/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.7460 - val_loss: 173.1423\n",
      "Epoch 370/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.6969 - val_loss: 173.1032\n",
      "Epoch 371/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.6554 - val_loss: 173.0773\n",
      "Epoch 372/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.6179 - val_loss: 173.0275\n",
      "Epoch 373/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.5776 - val_loss: 172.9917\n",
      "Epoch 374/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.5326 - val_loss: 172.9312\n",
      "Epoch 375/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.4690 - val_loss: 172.9197\n",
      "Epoch 376/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.4695 - val_loss: 172.9082\n",
      "Epoch 377/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3888 - val_loss: 172.8894\n",
      "Epoch 378/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.3664 - val_loss: 172.8342\n",
      "Epoch 379/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3315 - val_loss: 172.8140\n",
      "Epoch 380/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.2987 - val_loss: 172.7858\n",
      "Epoch 381/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.2542 - val_loss: 172.7542\n",
      "Epoch 382/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.2364 - val_loss: 172.7224\n",
      "Epoch 383/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.1775 - val_loss: 172.7177\n",
      "Epoch 384/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1478 - val_loss: 172.7049\n",
      "Epoch 385/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.1268 - val_loss: 172.7270\n",
      "Epoch 386/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.0813 - val_loss: 172.7308\n",
      "Epoch 387/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.0693 - val_loss: 172.6905\n",
      "Epoch 388/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0010 - val_loss: 172.6764\n",
      "Epoch 389/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9769 - val_loss: 172.6675\n",
      "Epoch 390/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9459 - val_loss: 172.6555\n",
      "Epoch 391/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8800 - val_loss: 172.6491\n",
      "Epoch 392/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8634 - val_loss: 172.6522\n",
      "Epoch 393/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8309 - val_loss: 172.6518\n",
      "Epoch 394/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.8024 - val_loss: 172.6511\n",
      "Epoch 395/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.7777 - val_loss: 172.6474\n",
      "Epoch 396/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7735 - val_loss: 172.5925\n",
      "Epoch 397/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6874 - val_loss: 172.6010\n",
      "Epoch 398/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6627 - val_loss: 172.5992\n",
      "Epoch 399/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6360 - val_loss: 172.5760\n",
      "Epoch 400/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.5914 - val_loss: 172.5937\n",
      "Epoch 401/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 223.5886 - val_loss: 172.5798\n",
      "Epoch 402/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.5395 - val_loss: 172.5484\n",
      "Epoch 403/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4961 - val_loss: 172.5438\n",
      "Epoch 404/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4809 - val_loss: 172.5644\n",
      "Epoch 405/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4565 - val_loss: 172.5724\n",
      "Epoch 406/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4058 - val_loss: 172.5817\n",
      "Epoch 407/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3962 - val_loss: 172.5604\n",
      "Epoch 408/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3596 - val_loss: 172.5338\n",
      "Epoch 409/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3265 - val_loss: 172.5496\n",
      "Epoch 410/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2827 - val_loss: 172.5410\n",
      "Epoch 411/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2673 - val_loss: 172.5493\n",
      "Epoch 412/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2285 - val_loss: 172.5495\n",
      "Epoch 413/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2054 - val_loss: 172.5567\n",
      "Epoch 414/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1834 - val_loss: 172.5386\n",
      "Epoch 415/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1631 - val_loss: 172.5229\n",
      "Epoch 416/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1253 - val_loss: 172.5104\n",
      "Epoch 417/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0847 - val_loss: 172.4992\n",
      "Epoch 418/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1321 - val_loss: 172.4453\n",
      "Epoch 419/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0510 - val_loss: 172.4550\n",
      "Epoch 420/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0347 - val_loss: 172.5055\n",
      "Epoch 421/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0039 - val_loss: 172.5031\n",
      "Epoch 422/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9564 - val_loss: 172.5094\n",
      "Epoch 423/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9459 - val_loss: 172.5335\n",
      "Epoch 424/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9355 - val_loss: 172.5546\n",
      "Epoch 425/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9141 - val_loss: 172.5311\n",
      "Epoch 426/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8640 - val_loss: 172.5236\n",
      "Epoch 427/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8428 - val_loss: 172.5385\n",
      "Epoch 428/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8177 - val_loss: 172.5590\n",
      "Epoch 429/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8080 - val_loss: 172.5896\n",
      "Epoch 430/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7654 - val_loss: 172.5947\n",
      "Epoch 431/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7607 - val_loss: 172.5736\n",
      "Epoch 432/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7254 - val_loss: 172.5694\n",
      "Epoch 433/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7105 - val_loss: 172.5986\n",
      "Epoch 434/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6781 - val_loss: 172.5908\n",
      "Epoch 435/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6532 - val_loss: 172.5983\n",
      "Epoch 436/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6407 - val_loss: 172.6052\n",
      "Epoch 437/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6208 - val_loss: 172.6016\n",
      "Epoch 438/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6062 - val_loss: 172.6095\n",
      "Epoch 439/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5723 - val_loss: 172.6042\n",
      "Epoch 440/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5730 - val_loss: 172.5976\n",
      "Epoch 441/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5698 - val_loss: 172.5873\n",
      "Epoch 442/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5259 - val_loss: 172.6155\n",
      "Epoch 443/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4952 - val_loss: 172.6479\n",
      "Epoch 444/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4922 - val_loss: 172.6931\n",
      "Epoch 445/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4702 - val_loss: 172.6764\n",
      "Epoch 446/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4352 - val_loss: 172.6954\n",
      "Epoch 447/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4379 - val_loss: 172.6748\n",
      "Epoch 448/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4065 - val_loss: 172.6974\n",
      "Epoch 449/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4299 - val_loss: 172.7480\n",
      "Epoch 450/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3749 - val_loss: 172.7518\n",
      "Epoch 451/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3711 - val_loss: 172.7294\n",
      "Epoch 452/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3511 - val_loss: 172.7417\n",
      "Epoch 453/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3242 - val_loss: 172.7562\n",
      "Epoch 454/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3302 - val_loss: 172.7318\n",
      "Epoch 455/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3289 - val_loss: 172.7281\n",
      "Epoch 456/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2823 - val_loss: 172.7620\n",
      "Epoch 457/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2597 - val_loss: 172.7774\n",
      "Epoch 458/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2578 - val_loss: 172.7929\n",
      "Epoch 459/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2494 - val_loss: 172.8045\n",
      "Epoch 460/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2179 - val_loss: 172.8212\n",
      "Epoch 461/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2065 - val_loss: 172.8614\n",
      "Epoch 462/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1885 - val_loss: 172.8779\n",
      "Epoch 463/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1846 - val_loss: 172.8985\n",
      "Epoch 464/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1708 - val_loss: 172.9339\n",
      "Epoch 465/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1597 - val_loss: 172.9332\n",
      "Epoch 466/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1532 - val_loss: 172.9780\n",
      "Epoch 467/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1251 - val_loss: 172.9910\n",
      "Epoch 468/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1128 - val_loss: 172.9781\n",
      "Epoch 469/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1152 - val_loss: 173.0108\n",
      "Epoch 470/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0967 - val_loss: 173.0139\n",
      "Epoch 471/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0736 - val_loss: 173.0244\n",
      "Epoch 472/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0995 - val_loss: 173.0687\n",
      "Epoch 473/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0842 - val_loss: 173.0266\n",
      "Epoch 474/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0369 - val_loss: 173.0395\n",
      "Epoch 475/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0268 - val_loss: 173.0595\n",
      "Epoch 476/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0268 - val_loss: 173.0854\n",
      "Epoch 477/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0388 - val_loss: 173.1138\n",
      "Epoch 478/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0434 - val_loss: 173.0811\n",
      "Epoch 479/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9987 - val_loss: 173.1215\n",
      "Epoch 480/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9969 - val_loss: 173.0967\n",
      "Epoch 481/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9556 - val_loss: 173.1115\n",
      "Epoch 482/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9439 - val_loss: 173.1336\n",
      "Epoch 483/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9433 - val_loss: 173.1473\n",
      "Epoch 484/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9370 - val_loss: 173.1468\n",
      "Epoch 485/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9355 - val_loss: 173.1621\n",
      "Epoch 486/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9197 - val_loss: 173.1987\n",
      "Epoch 487/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9248 - val_loss: 173.2213\n",
      "Epoch 488/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8976 - val_loss: 173.2350\n",
      "Epoch 489/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8861 - val_loss: 173.2420\n",
      "Epoch 490/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8773 - val_loss: 173.2480\n",
      "Epoch 491/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8631 - val_loss: 173.2495\n",
      "Epoch 492/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8733 - val_loss: 173.2626\n",
      "Epoch 493/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8385 - val_loss: 173.2771\n",
      "Epoch 494/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8426 - val_loss: 173.2582\n",
      "Epoch 495/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8301 - val_loss: 173.2631\n",
      "Epoch 496/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8865 - val_loss: 173.3337\n",
      "Epoch 497/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8062 - val_loss: 173.3294\n",
      "Epoch 498/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8048 - val_loss: 173.3232\n",
      "Epoch 499/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8123 - val_loss: 173.3485\n",
      "Epoch 500/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7736 - val_loss: 173.3285\n",
      "Epoch 501/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7690 - val_loss: 173.3097\n",
      "Epoch 502/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7850 - val_loss: 173.2992\n",
      "Epoch 503/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7539 - val_loss: 173.3138\n",
      "Epoch 504/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7680 - val_loss: 173.3149\n",
      "Epoch 505/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7480 - val_loss: 173.3357\n",
      "Epoch 506/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7422 - val_loss: 173.3846\n",
      "Epoch 507/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7270 - val_loss: 173.4094\n",
      "Epoch 508/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7264 - val_loss: 173.4422\n",
      "Epoch 509/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7105 - val_loss: 173.4546\n",
      "Epoch 510/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7228 - val_loss: 173.4697\n",
      "Epoch 511/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6792 - val_loss: 173.4478\n",
      "Epoch 512/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6757 - val_loss: 173.4435\n",
      "Epoch 513/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6731 - val_loss: 173.4363\n",
      "Epoch 514/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6886 - val_loss: 173.4276\n",
      "Epoch 515/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6688 - val_loss: 173.4496\n",
      "Epoch 516/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6544 - val_loss: 173.4688\n",
      "Epoch 517/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6563 - val_loss: 173.4874\n",
      "Epoch 518/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6577 - val_loss: 173.5086\n",
      "Epoch 519/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6389 - val_loss: 173.5364\n",
      "Epoch 520/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6478 - val_loss: 173.5759\n",
      "Epoch 521/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6334 - val_loss: 173.5656\n",
      "Epoch 522/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6213 - val_loss: 173.5546\n",
      "Epoch 523/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6157 - val_loss: 173.5985\n",
      "Epoch 524/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6305 - val_loss: 173.5868\n",
      "Epoch 525/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6056 - val_loss: 173.6076\n",
      "Epoch 526/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6131 - val_loss: 173.6624\n",
      "Epoch 527/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6176 - val_loss: 173.7103\n",
      "Epoch 528/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5918 - val_loss: 173.7184\n",
      "Epoch 529/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6278 - val_loss: 173.7559\n",
      "Epoch 530/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5674 - val_loss: 173.7567\n",
      "Epoch 531/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5916 - val_loss: 173.7729\n",
      "Epoch 532/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5631 - val_loss: 173.7829\n",
      "Epoch 533/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5607 - val_loss: 173.7676\n",
      "Epoch 534/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5553 - val_loss: 173.7702\n",
      "Epoch 535/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5590 - val_loss: 173.7757\n",
      "Epoch 536/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5603 - val_loss: 173.7831\n",
      "Epoch 537/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5399 - val_loss: 173.8077\n",
      "Epoch 538/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5364 - val_loss: 173.8198\n",
      "Epoch 539/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5610 - val_loss: 173.8108\n",
      "Epoch 540/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5468 - val_loss: 173.8356\n",
      "Epoch 541/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5345 - val_loss: 173.8183\n",
      "Epoch 542/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5288 - val_loss: 173.8337\n",
      "Epoch 543/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5238 - val_loss: 173.8580\n",
      "Epoch 544/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5186 - val_loss: 173.9053\n",
      "Epoch 545/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5148 - val_loss: 173.9426\n",
      "Epoch 546/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5234 - val_loss: 173.9778\n",
      "Epoch 547/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4980 - val_loss: 173.9806\n",
      "Epoch 548/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.5174 - val_loss: 173.9712\n",
      "Epoch 549/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4881 - val_loss: 174.0038\n",
      "Epoch 550/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4753 - val_loss: 174.0244\n",
      "Epoch 551/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4792 - val_loss: 174.0489\n",
      "Epoch 552/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4700 - val_loss: 174.0623\n",
      "Epoch 553/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4792 - val_loss: 174.0993\n",
      "Epoch 554/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4740 - val_loss: 174.1437\n",
      "Epoch 555/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4673 - val_loss: 174.1705\n",
      "Epoch 556/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4711 - val_loss: 174.1611\n",
      "Epoch 557/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4521 - val_loss: 174.1844\n",
      "Epoch 558/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4396 - val_loss: 174.1791\n",
      "Epoch 559/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4555 - val_loss: 174.1605\n",
      "Epoch 560/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4426 - val_loss: 174.1656\n",
      "Epoch 561/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4645 - val_loss: 174.1349\n",
      "Epoch 562/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4527 - val_loss: 174.1836\n",
      "Epoch 563/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4507 - val_loss: 174.2100\n",
      "Epoch 564/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4318 - val_loss: 174.2071\n",
      "Epoch 565/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4235 - val_loss: 174.2492\n",
      "Epoch 566/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4440 - val_loss: 174.2581\n",
      "Epoch 567/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4286 - val_loss: 174.2888\n",
      "Epoch 568/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4066 - val_loss: 174.3097\n",
      "Epoch 569/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4315 - val_loss: 174.3558\n",
      "Epoch 570/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4152 - val_loss: 174.3549\n",
      "Epoch 571/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3972 - val_loss: 174.3808\n",
      "Epoch 572/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3993 - val_loss: 174.4240\n",
      "Epoch 573/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3915 - val_loss: 174.4428\n",
      "Epoch 574/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4128 - val_loss: 174.4218\n",
      "Epoch 575/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4269 - val_loss: 174.4768\n",
      "Epoch 576/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3909 - val_loss: 174.4557\n",
      "Epoch 577/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3765 - val_loss: 174.4568\n",
      "Epoch 578/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3685 - val_loss: 174.4784\n",
      "Epoch 579/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3771 - val_loss: 174.5019\n",
      "Epoch 580/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3838 - val_loss: 174.5061\n",
      "Epoch 581/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.3687 - val_loss: 174.5096\n",
      "Epoch 582/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 221.3687 - val_loss: 174.5031\n",
      "Epoch 583/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.3750 - val_loss: 174.5536\n",
      "Epoch 584/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 221.3654 - val_loss: 174.5474\n",
      "Epoch 585/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.3608 - val_loss: 174.5514\n",
      "Epoch 586/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.3760 - val_loss: 174.5823\n",
      "Epoch 587/750\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 221.3633 - val_loss: 174.5525\n",
      "Epoch 588/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 221.3509 - val_loss: 174.5641\n",
      "Epoch 589/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3444 - val_loss: 174.5930\n",
      "Epoch 590/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3433 - val_loss: 174.6250\n",
      "Epoch 591/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.3732 - val_loss: 174.6186\n",
      "Epoch 592/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3427 - val_loss: 174.6432\n",
      "Epoch 593/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.3473 - val_loss: 174.6857\n",
      "Epoch 594/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3373 - val_loss: 174.7160\n",
      "Epoch 595/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3317 - val_loss: 174.6938\n",
      "Epoch 596/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3278 - val_loss: 174.6941\n",
      "Epoch 597/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3316 - val_loss: 174.7190\n",
      "Epoch 598/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3287 - val_loss: 174.7078\n",
      "Epoch 599/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3213 - val_loss: 174.7395\n",
      "Epoch 600/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3297 - val_loss: 174.7164\n",
      "Epoch 601/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3146 - val_loss: 174.7077\n",
      "Epoch 602/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3339 - val_loss: 174.7392\n",
      "Epoch 603/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3205 - val_loss: 174.7441\n",
      "Epoch 604/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3185 - val_loss: 174.7272\n",
      "Epoch 605/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3080 - val_loss: 174.7527\n",
      "Epoch 606/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3037 - val_loss: 174.7710\n",
      "Epoch 607/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3006 - val_loss: 174.7845\n",
      "Epoch 608/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3048 - val_loss: 174.7712\n",
      "Epoch 609/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3139 - val_loss: 174.8214\n",
      "Epoch 610/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3075 - val_loss: 174.8671\n",
      "Epoch 611/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2902 - val_loss: 174.8775\n",
      "Epoch 612/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2961 - val_loss: 174.8502\n",
      "Epoch 613/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2936 - val_loss: 174.8441\n",
      "Epoch 614/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3015 - val_loss: 174.8373\n",
      "Epoch 615/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2982 - val_loss: 174.8834\n",
      "Epoch 616/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2915 - val_loss: 174.9107\n",
      "Epoch 617/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2950 - val_loss: 174.9005\n",
      "Epoch 618/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2859 - val_loss: 174.9289\n",
      "Epoch 619/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2802 - val_loss: 174.9325\n",
      "Epoch 620/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2838 - val_loss: 174.9758\n",
      "Epoch 621/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3265 - val_loss: 174.9723\n",
      "Epoch 622/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2705 - val_loss: 175.0144\n",
      "Epoch 623/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2781 - val_loss: 175.0354\n",
      "Epoch 624/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2634 - val_loss: 175.0538\n",
      "Epoch 625/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.2825 - val_loss: 175.0585\n",
      "Epoch 626/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.2588 - val_loss: 175.0780\n",
      "Epoch 627/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2655 - val_loss: 175.0912\n",
      "Epoch 628/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2856 - val_loss: 175.1014\n",
      "Epoch 629/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2686 - val_loss: 175.1737\n",
      "Epoch 630/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3165 - val_loss: 175.1519\n",
      "Epoch 631/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3029 - val_loss: 175.2278\n",
      "Epoch 632/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2535 - val_loss: 175.2461\n",
      "Epoch 633/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2494 - val_loss: 175.2722\n",
      "Epoch 634/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2648 - val_loss: 175.3132\n",
      "Epoch 635/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2558 - val_loss: 175.3012\n",
      "Epoch 636/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2504 - val_loss: 175.3201\n",
      "Epoch 637/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2620 - val_loss: 175.2865\n",
      "Epoch 638/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2717 - val_loss: 175.2682\n",
      "Epoch 639/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2477 - val_loss: 175.3102\n",
      "Epoch 640/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2468 - val_loss: 175.3181\n",
      "Epoch 641/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2585 - val_loss: 175.3396\n",
      "Epoch 642/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2832 - val_loss: 175.2918\n",
      "Epoch 643/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2430 - val_loss: 175.3135\n",
      "Epoch 644/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2461 - val_loss: 175.3482\n",
      "Epoch 645/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2318 - val_loss: 175.3590\n",
      "Epoch 646/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2413 - val_loss: 175.3687\n",
      "Epoch 647/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2339 - val_loss: 175.3769\n",
      "Epoch 648/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2641 - val_loss: 175.3967\n",
      "Epoch 649/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2294 - val_loss: 175.3554\n",
      "Epoch 650/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2512 - val_loss: 175.3289\n",
      "Epoch 651/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2458 - val_loss: 175.3057\n",
      "Epoch 652/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2359 - val_loss: 175.3434\n",
      "Epoch 653/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2309 - val_loss: 175.3305\n",
      "Epoch 654/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2241 - val_loss: 175.3418\n",
      "Epoch 655/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2232 - val_loss: 175.3624\n",
      "Epoch 656/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2414 - val_loss: 175.3922\n",
      "Epoch 657/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2180 - val_loss: 175.4025\n",
      "Epoch 658/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2170 - val_loss: 175.3716\n",
      "Epoch 659/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2234 - val_loss: 175.3609\n",
      "Epoch 660/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2259 - val_loss: 175.3963\n",
      "Epoch 661/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2362 - val_loss: 175.4063\n",
      "Epoch 662/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2229 - val_loss: 175.3870\n",
      "Epoch 663/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2183 - val_loss: 175.4167\n",
      "Epoch 664/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2296 - val_loss: 175.4104\n",
      "Epoch 665/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2386 - val_loss: 175.3979\n",
      "Epoch 666/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2186 - val_loss: 175.4168\n",
      "Epoch 667/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2130 - val_loss: 175.4320\n",
      "Epoch 668/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2102 - val_loss: 175.4525\n",
      "Epoch 669/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2316 - val_loss: 175.5054\n",
      "Epoch 670/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2018 - val_loss: 175.5229\n",
      "Epoch 671/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2052 - val_loss: 175.5608\n",
      "Epoch 672/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2047 - val_loss: 175.5934\n",
      "Epoch 673/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2341 - val_loss: 175.5822\n",
      "Epoch 674/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1993 - val_loss: 175.6046\n",
      "Epoch 675/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1980 - val_loss: 175.6186\n",
      "Epoch 676/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2047 - val_loss: 175.6323\n",
      "Epoch 677/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1921 - val_loss: 175.6450\n",
      "Epoch 678/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1945 - val_loss: 175.6446\n",
      "Epoch 679/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1902 - val_loss: 175.6461\n",
      "Epoch 680/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1933 - val_loss: 175.6368\n",
      "Epoch 681/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2069 - val_loss: 175.6649\n",
      "Epoch 682/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1968 - val_loss: 175.6506\n",
      "Epoch 683/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2009 - val_loss: 175.6800\n",
      "Epoch 684/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1959 - val_loss: 175.6809\n",
      "Epoch 685/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1941 - val_loss: 175.7140\n",
      "Epoch 686/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1880 - val_loss: 175.6922\n",
      "Epoch 687/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2104 - val_loss: 175.7004\n",
      "Epoch 688/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1871 - val_loss: 175.7320\n",
      "Epoch 689/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1995 - val_loss: 175.7775\n",
      "Epoch 690/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2135 - val_loss: 175.8359\n",
      "Epoch 691/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1949 - val_loss: 175.8211\n",
      "Epoch 692/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1954 - val_loss: 175.8259\n",
      "Epoch 693/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1827 - val_loss: 175.8353\n",
      "Epoch 694/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1784 - val_loss: 175.8401\n",
      "Epoch 695/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1875 - val_loss: 175.8263\n",
      "Epoch 696/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1855 - val_loss: 175.8238\n",
      "Epoch 697/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1803 - val_loss: 175.8397\n",
      "Epoch 698/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1830 - val_loss: 175.8628\n",
      "Epoch 699/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1852 - val_loss: 175.8682\n",
      "Epoch 700/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1904 - val_loss: 175.9020\n",
      "Epoch 701/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2035 - val_loss: 175.8681\n",
      "Epoch 702/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1730 - val_loss: 175.9034\n",
      "Epoch 703/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1730 - val_loss: 175.9125\n",
      "Epoch 704/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1656 - val_loss: 175.9171\n",
      "Epoch 705/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1637 - val_loss: 175.9219\n",
      "Epoch 706/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1833 - val_loss: 175.9845\n",
      "Epoch 707/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1720 - val_loss: 175.9642\n",
      "Epoch 708/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1858 - val_loss: 175.9370\n",
      "Epoch 709/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1689 - val_loss: 175.9453\n",
      "Epoch 710/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1761 - val_loss: 175.9661\n",
      "Epoch 711/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1647 - val_loss: 175.9686\n",
      "Epoch 712/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1937 - val_loss: 176.0023\n",
      "Epoch 713/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1569 - val_loss: 175.9833\n",
      "Epoch 714/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1475 - val_loss: 175.9688\n",
      "Epoch 715/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1587 - val_loss: 175.9631\n",
      "Epoch 716/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1556 - val_loss: 175.9658\n",
      "Epoch 717/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1583 - val_loss: 175.9627\n",
      "Epoch 718/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1633 - val_loss: 175.9824\n",
      "Epoch 719/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1680 - val_loss: 175.9723\n",
      "Epoch 720/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1610 - val_loss: 175.9902\n",
      "Epoch 721/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1934 - val_loss: 176.0408\n",
      "Epoch 722/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1540 - val_loss: 176.0469\n",
      "Epoch 723/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1654 - val_loss: 176.0369\n",
      "Epoch 724/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1629 - val_loss: 176.0919\n",
      "Epoch 725/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1609 - val_loss: 176.1371\n",
      "Epoch 726/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1748 - val_loss: 176.1573\n",
      "Epoch 727/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1793 - val_loss: 176.1272\n",
      "Epoch 728/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1604 - val_loss: 176.1542\n",
      "Epoch 729/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1816 - val_loss: 176.1152\n",
      "Epoch 730/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1509 - val_loss: 176.1249\n",
      "Epoch 731/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1463 - val_loss: 176.1466\n",
      "Epoch 732/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1412 - val_loss: 176.1834\n",
      "Epoch 733/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1504 - val_loss: 176.2031\n",
      "Epoch 734/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1385 - val_loss: 176.2371\n",
      "Epoch 735/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1556 - val_loss: 176.2680\n",
      "Epoch 736/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1578 - val_loss: 176.2847\n",
      "Epoch 737/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1891 - val_loss: 176.2326\n",
      "Epoch 738/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1424 - val_loss: 176.2661\n",
      "Epoch 739/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1605 - val_loss: 176.2605\n",
      "Epoch 740/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1402 - val_loss: 176.2961\n",
      "Epoch 741/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1402 - val_loss: 176.3384\n",
      "Epoch 742/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1598 - val_loss: 176.3706\n",
      "Epoch 743/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1433 - val_loss: 176.3465\n",
      "Epoch 744/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1371 - val_loss: 176.3483\n",
      "Epoch 745/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1396 - val_loss: 176.3653\n",
      "Epoch 746/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1349 - val_loss: 176.3667\n",
      "Epoch 747/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1537 - val_loss: 176.3845\n",
      "Epoch 748/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1373 - val_loss: 176.3810\n",
      "Epoch 749/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1310 - val_loss: 176.3919\n",
      "Epoch 750/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1420 - val_loss: 176.3958\n",
      ">Saved ./models/comp4948_a2_model_4.h5\n",
      "Epoch 1/750\n",
      "6/6 [==============================] - 1s 31ms/step - loss: 4526.3174 - val_loss: 4522.8794\n",
      "Epoch 2/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4519.9507 - val_loss: 4516.5596\n",
      "Epoch 3/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4513.6826 - val_loss: 4510.3652\n",
      "Epoch 4/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4507.5356 - val_loss: 4504.1987\n",
      "Epoch 5/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4501.4082 - val_loss: 4498.1074\n",
      "Epoch 6/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4495.2964 - val_loss: 4492.0317\n",
      "Epoch 7/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4489.2021 - val_loss: 4485.8789\n",
      "Epoch 8/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4483.0078 - val_loss: 4479.6343\n",
      "Epoch 9/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4476.7490 - val_loss: 4473.1978\n",
      "Epoch 10/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4470.2520 - val_loss: 4466.6128\n",
      "Epoch 11/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4463.5581 - val_loss: 4459.9307\n",
      "Epoch 12/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4456.9751 - val_loss: 4453.2573\n",
      "Epoch 13/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4450.2168 - val_loss: 4446.4141\n",
      "Epoch 14/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4443.5425 - val_loss: 4439.8047\n",
      "Epoch 15/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4436.8032 - val_loss: 4433.0327\n",
      "Epoch 16/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4429.9722 - val_loss: 4426.0186\n",
      "Epoch 17/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4422.8940 - val_loss: 4418.7886\n",
      "Epoch 18/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4415.6245 - val_loss: 4411.4683\n",
      "Epoch 19/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4408.1763 - val_loss: 4404.0942\n",
      "Epoch 20/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4400.8696 - val_loss: 4396.5732\n",
      "Epoch 21/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4393.2690 - val_loss: 4388.8286\n",
      "Epoch 22/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4385.4990 - val_loss: 4380.8359\n",
      "Epoch 23/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4377.4429 - val_loss: 4372.9360\n",
      "Epoch 24/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4369.6016 - val_loss: 4364.8901\n",
      "Epoch 25/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4361.3896 - val_loss: 4356.6357\n",
      "Epoch 26/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4353.1816 - val_loss: 4348.1245\n",
      "Epoch 27/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4344.5581 - val_loss: 4339.3706\n",
      "Epoch 28/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4335.7192 - val_loss: 4330.4219\n",
      "Epoch 29/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4326.8125 - val_loss: 4321.3911\n",
      "Epoch 30/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4317.6362 - val_loss: 4312.1768\n",
      "Epoch 31/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4308.7290 - val_loss: 4303.2349\n",
      "Epoch 32/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4299.6577 - val_loss: 4294.1372\n",
      "Epoch 33/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4290.4619 - val_loss: 4284.7515\n",
      "Epoch 34/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4280.9424 - val_loss: 4275.0889\n",
      "Epoch 35/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4271.2026 - val_loss: 4265.1597\n",
      "Epoch 36/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4261.3374 - val_loss: 4255.0776\n",
      "Epoch 37/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4251.1807 - val_loss: 4245.0195\n",
      "Epoch 38/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4241.0654 - val_loss: 4234.6396\n",
      "Epoch 39/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4230.5605 - val_loss: 4224.0933\n",
      "Epoch 40/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4220.1919 - val_loss: 4213.7529\n",
      "Epoch 41/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4209.7192 - val_loss: 4203.1934\n",
      "Epoch 42/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4199.0278 - val_loss: 4192.5488\n",
      "Epoch 43/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4188.5151 - val_loss: 4181.8218\n",
      "Epoch 44/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4177.6519 - val_loss: 4170.8018\n",
      "Epoch 45/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4166.6401 - val_loss: 4159.5527\n",
      "Epoch 46/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4155.2734 - val_loss: 4148.0356\n",
      "Epoch 47/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4143.7534 - val_loss: 4136.4707\n",
      "Epoch 48/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4132.1323 - val_loss: 4124.6689\n",
      "Epoch 49/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4120.1899 - val_loss: 4112.6909\n",
      "Epoch 50/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4108.0020 - val_loss: 4100.6206\n",
      "Epoch 51/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4096.0771 - val_loss: 4088.5562\n",
      "Epoch 52/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4083.9546 - val_loss: 4076.1562\n",
      "Epoch 53/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4071.4351 - val_loss: 4063.6721\n",
      "Epoch 54/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4059.0923 - val_loss: 4051.0562\n",
      "Epoch 55/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4046.3674 - val_loss: 4038.1492\n",
      "Epoch 56/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4033.3831 - val_loss: 4025.0110\n",
      "Epoch 57/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4020.1406 - val_loss: 4011.7571\n",
      "Epoch 58/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4006.9478 - val_loss: 3998.4116\n",
      "Epoch 59/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3993.7390 - val_loss: 3985.1204\n",
      "Epoch 60/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3980.3782 - val_loss: 3971.4807\n",
      "Epoch 61/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3966.5984 - val_loss: 3957.7061\n",
      "Epoch 62/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3952.8826 - val_loss: 3944.1846\n",
      "Epoch 63/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3939.4670 - val_loss: 3930.4658\n",
      "Epoch 64/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3925.6631 - val_loss: 3916.4187\n",
      "Epoch 65/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3911.5444 - val_loss: 3902.1064\n",
      "Epoch 66/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3897.3140 - val_loss: 3887.6064\n",
      "Epoch 67/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3882.8652 - val_loss: 3873.0845\n",
      "Epoch 68/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3868.2063 - val_loss: 3858.3845\n",
      "Epoch 69/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3853.5117 - val_loss: 3843.3921\n",
      "Epoch 70/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3838.7002 - val_loss: 3828.3662\n",
      "Epoch 71/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3823.5388 - val_loss: 3813.2046\n",
      "Epoch 72/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3808.4270 - val_loss: 3798.0554\n",
      "Epoch 73/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3793.4851 - val_loss: 3783.2563\n",
      "Epoch 74/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3778.6992 - val_loss: 3768.0632\n",
      "Epoch 75/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3763.4080 - val_loss: 3752.5962\n",
      "Epoch 76/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3747.7791 - val_loss: 3736.9014\n",
      "Epoch 77/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3732.1929 - val_loss: 3721.1235\n",
      "Epoch 78/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3716.3518 - val_loss: 3705.0334\n",
      "Epoch 79/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3700.2144 - val_loss: 3688.7656\n",
      "Epoch 80/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3683.7305 - val_loss: 3672.2905\n",
      "Epoch 81/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3667.2104 - val_loss: 3655.4756\n",
      "Epoch 82/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3650.3765 - val_loss: 3638.2976\n",
      "Epoch 83/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3632.8728 - val_loss: 3621.1250\n",
      "Epoch 84/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3616.0701 - val_loss: 3604.5305\n",
      "Epoch 85/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3599.5869 - val_loss: 3587.5342\n",
      "Epoch 86/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3582.5771 - val_loss: 3570.4929\n",
      "Epoch 87/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3565.7346 - val_loss: 3553.5935\n",
      "Epoch 88/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3548.6272 - val_loss: 3536.4900\n",
      "Epoch 89/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3531.5908 - val_loss: 3518.9697\n",
      "Epoch 90/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3514.1948 - val_loss: 3501.1289\n",
      "Epoch 91/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3496.0088 - val_loss: 3483.4985\n",
      "Epoch 92/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3478.7024 - val_loss: 3465.9092\n",
      "Epoch 93/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3461.0437 - val_loss: 3447.9475\n",
      "Epoch 94/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3443.1638 - val_loss: 3429.9041\n",
      "Epoch 95/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3424.8481 - val_loss: 3411.8240\n",
      "Epoch 96/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3407.0112 - val_loss: 3393.7698\n",
      "Epoch 97/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3389.1318 - val_loss: 3375.2295\n",
      "Epoch 98/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3370.5444 - val_loss: 3356.5427\n",
      "Epoch 99/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3351.8804 - val_loss: 3337.7373\n",
      "Epoch 100/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3332.8950 - val_loss: 3318.7812\n",
      "Epoch 101/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3313.8618 - val_loss: 3299.5398\n",
      "Epoch 102/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3294.6384 - val_loss: 3279.9963\n",
      "Epoch 103/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3275.2068 - val_loss: 3260.4541\n",
      "Epoch 104/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3255.5723 - val_loss: 3240.9595\n",
      "Epoch 105/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3236.0110 - val_loss: 3221.3401\n",
      "Epoch 106/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3216.7397 - val_loss: 3201.7109\n",
      "Epoch 107/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3196.9360 - val_loss: 3182.0491\n",
      "Epoch 108/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3177.2842 - val_loss: 3161.9846\n",
      "Epoch 109/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3157.2590 - val_loss: 3141.7039\n",
      "Epoch 110/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3136.8818 - val_loss: 3121.4580\n",
      "Epoch 111/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3116.7039 - val_loss: 3100.9094\n",
      "Epoch 112/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3095.8386 - val_loss: 3080.2529\n",
      "Epoch 113/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3075.6531 - val_loss: 3059.6499\n",
      "Epoch 114/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3054.9585 - val_loss: 3038.9353\n",
      "Epoch 115/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3034.2288 - val_loss: 3018.2898\n",
      "Epoch 116/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3014.0090 - val_loss: 2997.4072\n",
      "Epoch 117/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2993.1858 - val_loss: 2976.7200\n",
      "Epoch 118/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2972.4563 - val_loss: 2955.7334\n",
      "Epoch 119/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2951.3457 - val_loss: 2934.5571\n",
      "Epoch 120/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2930.2065 - val_loss: 2913.1396\n",
      "Epoch 121/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2908.9268 - val_loss: 2891.5015\n",
      "Epoch 122/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2887.2610 - val_loss: 2869.6877\n",
      "Epoch 123/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2865.4941 - val_loss: 2847.7102\n",
      "Epoch 124/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2843.5735 - val_loss: 2825.5640\n",
      "Epoch 125/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2821.5491 - val_loss: 2803.9775\n",
      "Epoch 126/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2800.2048 - val_loss: 2782.5391\n",
      "Epoch 127/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2779.0884 - val_loss: 2760.6350\n",
      "Epoch 128/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2756.8025 - val_loss: 2738.7734\n",
      "Epoch 129/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2735.1069 - val_loss: 2716.4951\n",
      "Epoch 130/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2713.0374 - val_loss: 2694.8564\n",
      "Epoch 131/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2691.5017 - val_loss: 2673.2681\n",
      "Epoch 132/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2670.0391 - val_loss: 2651.3555\n",
      "Epoch 133/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2648.4021 - val_loss: 2629.4424\n",
      "Epoch 134/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2626.3596 - val_loss: 2607.5186\n",
      "Epoch 135/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2604.2429 - val_loss: 2585.5596\n",
      "Epoch 136/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2582.7947 - val_loss: 2563.4792\n",
      "Epoch 137/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2560.5918 - val_loss: 2541.3088\n",
      "Epoch 138/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2538.1389 - val_loss: 2519.2002\n",
      "Epoch 139/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2516.5210 - val_loss: 2497.2671\n",
      "Epoch 140/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2494.5564 - val_loss: 2475.2715\n",
      "Epoch 141/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2472.5242 - val_loss: 2453.2202\n",
      "Epoch 142/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2450.6853 - val_loss: 2431.1499\n",
      "Epoch 143/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2428.8477 - val_loss: 2409.2498\n",
      "Epoch 144/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2407.3381 - val_loss: 2387.0322\n",
      "Epoch 145/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2385.0955 - val_loss: 2364.8538\n",
      "Epoch 146/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2362.7815 - val_loss: 2342.6130\n",
      "Epoch 147/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2340.5388 - val_loss: 2320.1577\n",
      "Epoch 148/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2318.1631 - val_loss: 2297.6917\n",
      "Epoch 149/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2296.0876 - val_loss: 2274.7788\n",
      "Epoch 150/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2273.3435 - val_loss: 2251.7888\n",
      "Epoch 151/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2250.3254 - val_loss: 2228.8186\n",
      "Epoch 152/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2227.3521 - val_loss: 2205.7786\n",
      "Epoch 153/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2204.5950 - val_loss: 2182.6799\n",
      "Epoch 154/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2181.5784 - val_loss: 2159.6589\n",
      "Epoch 155/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2158.9126 - val_loss: 2136.5920\n",
      "Epoch 156/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2135.8901 - val_loss: 2113.6475\n",
      "Epoch 157/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2113.4561 - val_loss: 2090.3213\n",
      "Epoch 158/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2090.2458 - val_loss: 2067.1716\n",
      "Epoch 159/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2067.0876 - val_loss: 2044.1520\n",
      "Epoch 160/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2045.6016 - val_loss: 2022.0520\n",
      "Epoch 161/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2023.1849 - val_loss: 2000.3354\n",
      "Epoch 162/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2001.5801 - val_loss: 1978.5020\n",
      "Epoch 163/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1980.1030 - val_loss: 1956.5675\n",
      "Epoch 164/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1957.9995 - val_loss: 1934.7437\n",
      "Epoch 165/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1936.5287 - val_loss: 1912.5621\n",
      "Epoch 166/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1914.3970 - val_loss: 1890.3602\n",
      "Epoch 167/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1892.2871 - val_loss: 1868.3915\n",
      "Epoch 168/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1871.1395 - val_loss: 1846.8683\n",
      "Epoch 169/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1849.6919 - val_loss: 1825.3480\n",
      "Epoch 170/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1828.5859 - val_loss: 1803.7981\n",
      "Epoch 171/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1806.8961 - val_loss: 1782.4894\n",
      "Epoch 172/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1786.1666 - val_loss: 1761.2063\n",
      "Epoch 173/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1765.2753 - val_loss: 1739.8402\n",
      "Epoch 174/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1743.9327 - val_loss: 1718.6481\n",
      "Epoch 175/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1722.7905 - val_loss: 1697.4956\n",
      "Epoch 176/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1702.2686 - val_loss: 1676.4265\n",
      "Epoch 177/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1681.2598 - val_loss: 1655.4637\n",
      "Epoch 178/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1660.4666 - val_loss: 1634.3960\n",
      "Epoch 179/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1639.3840 - val_loss: 1613.3988\n",
      "Epoch 180/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1618.8794 - val_loss: 1592.1772\n",
      "Epoch 181/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1597.8252 - val_loss: 1571.0125\n",
      "Epoch 182/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1576.9004 - val_loss: 1549.8221\n",
      "Epoch 183/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1556.0138 - val_loss: 1528.6890\n",
      "Epoch 184/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1535.0607 - val_loss: 1507.6078\n",
      "Epoch 185/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1513.9240 - val_loss: 1486.6919\n",
      "Epoch 186/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1493.4512 - val_loss: 1465.6685\n",
      "Epoch 187/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1472.8207 - val_loss: 1444.5908\n",
      "Epoch 188/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1451.6919 - val_loss: 1423.7388\n",
      "Epoch 189/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1431.2595 - val_loss: 1402.7319\n",
      "Epoch 190/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1410.2502 - val_loss: 1381.9366\n",
      "Epoch 191/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1389.7930 - val_loss: 1361.0177\n",
      "Epoch 192/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1368.8685 - val_loss: 1340.3297\n",
      "Epoch 193/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1348.6675 - val_loss: 1319.8219\n",
      "Epoch 194/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1328.7202 - val_loss: 1299.2803\n",
      "Epoch 195/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1308.2942 - val_loss: 1279.0801\n",
      "Epoch 196/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1288.3185 - val_loss: 1258.8994\n",
      "Epoch 197/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1268.2793 - val_loss: 1238.7839\n",
      "Epoch 198/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1248.4130 - val_loss: 1218.6641\n",
      "Epoch 199/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1228.5465 - val_loss: 1198.5962\n",
      "Epoch 200/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1208.7566 - val_loss: 1178.6232\n",
      "Epoch 201/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1188.9926 - val_loss: 1158.7231\n",
      "Epoch 202/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1169.3458 - val_loss: 1139.1202\n",
      "Epoch 203/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1150.5880 - val_loss: 1119.9059\n",
      "Epoch 204/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1131.4918 - val_loss: 1100.9595\n",
      "Epoch 205/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1112.5891 - val_loss: 1082.2118\n",
      "Epoch 206/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1094.2471 - val_loss: 1063.4790\n",
      "Epoch 207/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1076.0892 - val_loss: 1044.8394\n",
      "Epoch 208/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1057.5737 - val_loss: 1026.5710\n",
      "Epoch 209/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1039.6522 - val_loss: 1008.3194\n",
      "Epoch 210/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1021.7809 - val_loss: 990.1280\n",
      "Epoch 211/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1004.0537 - val_loss: 972.0433\n",
      "Epoch 212/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 985.9821 - val_loss: 954.2752\n",
      "Epoch 213/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 968.7700 - val_loss: 936.4088\n",
      "Epoch 214/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 951.3638 - val_loss: 918.6772\n",
      "Epoch 215/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 934.0565 - val_loss: 901.2180\n",
      "Epoch 216/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 916.9284 - val_loss: 883.9711\n",
      "Epoch 217/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 900.3716 - val_loss: 866.7590\n",
      "Epoch 218/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 883.1343 - val_loss: 850.0840\n",
      "Epoch 219/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 867.1670 - val_loss: 833.6453\n",
      "Epoch 220/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 851.2139 - val_loss: 817.4267\n",
      "Epoch 221/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 835.2659 - val_loss: 801.4828\n",
      "Epoch 222/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 819.5951 - val_loss: 785.7701\n",
      "Epoch 223/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 804.6891 - val_loss: 769.9762\n",
      "Epoch 224/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 789.0813 - val_loss: 754.6671\n",
      "Epoch 225/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 774.0302 - val_loss: 739.5647\n",
      "Epoch 226/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 759.6541 - val_loss: 724.4588\n",
      "Epoch 227/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 745.0247 - val_loss: 709.6374\n",
      "Epoch 228/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 730.2496 - val_loss: 695.2395\n",
      "Epoch 229/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 716.3090 - val_loss: 680.8967\n",
      "Epoch 230/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 702.4677 - val_loss: 666.6696\n",
      "Epoch 231/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 688.8823 - val_loss: 652.5511\n",
      "Epoch 232/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 674.6314 - val_loss: 639.1332\n",
      "Epoch 233/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 662.2318 - val_loss: 625.4150\n",
      "Epoch 234/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 648.7834 - val_loss: 612.2081\n",
      "Epoch 235/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 635.9760 - val_loss: 599.2319\n",
      "Epoch 236/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 623.2500 - val_loss: 586.4800\n",
      "Epoch 237/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 611.1076 - val_loss: 573.8333\n",
      "Epoch 238/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 598.7159 - val_loss: 561.5518\n",
      "Epoch 239/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 586.8749 - val_loss: 549.3913\n",
      "Epoch 240/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 575.2455 - val_loss: 537.4164\n",
      "Epoch 241/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 563.7755 - val_loss: 525.7354\n",
      "Epoch 242/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 552.4250 - val_loss: 514.3963\n",
      "Epoch 243/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 541.4877 - val_loss: 503.2618\n",
      "Epoch 244/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 530.7820 - val_loss: 492.3396\n",
      "Epoch 245/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 520.4976 - val_loss: 481.5359\n",
      "Epoch 246/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 510.1079 - val_loss: 471.0307\n",
      "Epoch 247/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 500.2366 - val_loss: 460.6938\n",
      "Epoch 248/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 490.1468 - val_loss: 450.7918\n",
      "Epoch 249/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 480.6901 - val_loss: 441.1206\n",
      "Epoch 250/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 471.4586 - val_loss: 431.6570\n",
      "Epoch 251/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 462.5989 - val_loss: 422.2637\n",
      "Epoch 252/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 453.2850 - val_loss: 413.4276\n",
      "Epoch 253/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 444.8743 - val_loss: 404.7061\n",
      "Epoch 254/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 436.6147 - val_loss: 396.1337\n",
      "Epoch 255/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 428.4171 - val_loss: 387.8217\n",
      "Epoch 256/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 420.4916 - val_loss: 379.7124\n",
      "Epoch 257/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 412.8452 - val_loss: 371.7771\n",
      "Epoch 258/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 405.3034 - val_loss: 364.0612\n",
      "Epoch 259/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 398.1616 - val_loss: 356.4596\n",
      "Epoch 260/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 390.5016 - val_loss: 349.4051\n",
      "Epoch 261/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 383.7345 - val_loss: 342.3834\n",
      "Epoch 262/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 377.2089 - val_loss: 335.4041\n",
      "Epoch 263/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 370.6844 - val_loss: 328.6385\n",
      "Epoch 264/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 364.3641 - val_loss: 322.1120\n",
      "Epoch 265/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 358.2655 - val_loss: 315.8725\n",
      "Epoch 266/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 352.4301 - val_loss: 309.8283\n",
      "Epoch 267/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 346.8239 - val_loss: 303.9323\n",
      "Epoch 268/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 341.3452 - val_loss: 298.2273\n",
      "Epoch 269/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 336.1506 - val_loss: 292.6937\n",
      "Epoch 270/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 330.7473 - val_loss: 287.5844\n",
      "Epoch 271/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 326.1042 - val_loss: 282.5047\n",
      "Epoch 272/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 321.2217 - val_loss: 277.7020\n",
      "Epoch 273/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 316.8900 - val_loss: 272.9697\n",
      "Epoch 274/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 312.6219 - val_loss: 268.4096\n",
      "Epoch 275/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 308.3774 - val_loss: 264.1181\n",
      "Epoch 276/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 304.4533 - val_loss: 259.9545\n",
      "Epoch 277/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 300.5536 - val_loss: 256.0309\n",
      "Epoch 278/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 296.9586 - val_loss: 252.2496\n",
      "Epoch 279/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 293.5899 - val_loss: 248.4867\n",
      "Epoch 280/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 290.1607 - val_loss: 244.9127\n",
      "Epoch 281/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 286.9224 - val_loss: 241.4894\n",
      "Epoch 282/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 283.8547 - val_loss: 238.2403\n",
      "Epoch 283/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 280.7328 - val_loss: 235.2682\n",
      "Epoch 284/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 277.9532 - val_loss: 232.3431\n",
      "Epoch 285/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 275.3262 - val_loss: 229.4634\n",
      "Epoch 286/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 272.7356 - val_loss: 226.7316\n",
      "Epoch 287/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 270.2519 - val_loss: 224.1196\n",
      "Epoch 288/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 268.0861 - val_loss: 221.5111\n",
      "Epoch 289/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 265.7742 - val_loss: 219.0785\n",
      "Epoch 290/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 263.6493 - val_loss: 216.7446\n",
      "Epoch 291/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 261.2766 - val_loss: 214.7238\n",
      "Epoch 292/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 259.5550 - val_loss: 212.5748\n",
      "Epoch 293/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 257.8280 - val_loss: 210.4697\n",
      "Epoch 294/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 255.7650 - val_loss: 208.6618\n",
      "Epoch 295/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 254.1756 - val_loss: 206.8170\n",
      "Epoch 296/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 252.5305 - val_loss: 205.0722\n",
      "Epoch 297/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 251.2066 - val_loss: 203.2676\n",
      "Epoch 298/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 249.5850 - val_loss: 201.6248\n",
      "Epoch 299/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 248.1600 - val_loss: 200.0976\n",
      "Epoch 300/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 246.8104 - val_loss: 198.6835\n",
      "Epoch 301/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 245.6311 - val_loss: 197.2634\n",
      "Epoch 302/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 244.4515 - val_loss: 195.9298\n",
      "Epoch 303/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 243.2889 - val_loss: 194.7141\n",
      "Epoch 304/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 242.1643 - val_loss: 193.5981\n",
      "Epoch 305/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 241.3147 - val_loss: 192.4683\n",
      "Epoch 306/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 240.3267 - val_loss: 191.4717\n",
      "Epoch 307/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 239.5020 - val_loss: 190.4923\n",
      "Epoch 308/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 238.6124 - val_loss: 189.6068\n",
      "Epoch 309/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 237.9198 - val_loss: 188.7024\n",
      "Epoch 310/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 237.1528 - val_loss: 187.8731\n",
      "Epoch 311/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 236.4776 - val_loss: 187.0754\n",
      "Epoch 312/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 235.7731 - val_loss: 186.3547\n",
      "Epoch 313/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 235.2598 - val_loss: 185.5786\n",
      "Epoch 314/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 234.6768 - val_loss: 184.8445\n",
      "Epoch 315/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 234.0249 - val_loss: 184.2093\n",
      "Epoch 316/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 233.4874 - val_loss: 183.5819\n",
      "Epoch 317/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 233.0220 - val_loss: 182.9330\n",
      "Epoch 318/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232.5723 - val_loss: 182.3288\n",
      "Epoch 319/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.9819 - val_loss: 181.8574\n",
      "Epoch 320/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 231.7286 - val_loss: 181.2978\n",
      "Epoch 321/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 231.1936 - val_loss: 180.8685\n",
      "Epoch 322/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 230.9594 - val_loss: 180.3850\n",
      "Epoch 323/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 230.5236 - val_loss: 179.9790\n",
      "Epoch 324/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 230.2245 - val_loss: 179.6118\n",
      "Epoch 325/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.9106 - val_loss: 179.2623\n",
      "Epoch 326/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.6856 - val_loss: 178.8979\n",
      "Epoch 327/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.3698 - val_loss: 178.5824\n",
      "Epoch 328/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.1883 - val_loss: 178.2439\n",
      "Epoch 329/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.8999 - val_loss: 177.9802\n",
      "Epoch 330/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.7647 - val_loss: 177.6748\n",
      "Epoch 331/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.4439 - val_loss: 177.4799\n",
      "Epoch 332/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.2907 - val_loss: 177.2182\n",
      "Epoch 333/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.0872 - val_loss: 176.9906\n",
      "Epoch 334/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.9207 - val_loss: 176.7542\n",
      "Epoch 335/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.7794 - val_loss: 176.4887\n",
      "Epoch 336/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.5542 - val_loss: 176.3392\n",
      "Epoch 337/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.4811 - val_loss: 176.0979\n",
      "Epoch 338/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.2745 - val_loss: 175.9322\n",
      "Epoch 339/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.1365 - val_loss: 175.7625\n",
      "Epoch 340/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.0434 - val_loss: 175.5620\n",
      "Epoch 341/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.8900 - val_loss: 175.4413\n",
      "Epoch 342/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.8002 - val_loss: 175.2850\n",
      "Epoch 343/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.7014 - val_loss: 175.1381\n",
      "Epoch 344/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.5811 - val_loss: 175.0189\n",
      "Epoch 345/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.4662 - val_loss: 174.9295\n",
      "Epoch 346/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.3789 - val_loss: 174.8085\n",
      "Epoch 347/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.2957 - val_loss: 174.6720\n",
      "Epoch 348/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.2269 - val_loss: 174.5472\n",
      "Epoch 349/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.1418 - val_loss: 174.4447\n",
      "Epoch 350/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.0506 - val_loss: 174.3973\n",
      "Epoch 351/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.9926 - val_loss: 174.2753\n",
      "Epoch 352/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.9045 - val_loss: 174.2092\n",
      "Epoch 353/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.8827 - val_loss: 174.0816\n",
      "Epoch 354/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.7732 - val_loss: 174.0147\n",
      "Epoch 355/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.7052 - val_loss: 173.9751\n",
      "Epoch 356/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.6968 - val_loss: 173.8591\n",
      "Epoch 357/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.5743 - val_loss: 173.8152\n",
      "Epoch 358/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.5318 - val_loss: 173.7733\n",
      "Epoch 359/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.4663 - val_loss: 173.7093\n",
      "Epoch 360/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.4196 - val_loss: 173.6561\n",
      "Epoch 361/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.3746 - val_loss: 173.5807\n",
      "Epoch 362/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.3195 - val_loss: 173.5172\n",
      "Epoch 363/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.2478 - val_loss: 173.4740\n",
      "Epoch 364/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.2111 - val_loss: 173.4366\n",
      "Epoch 365/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.1661 - val_loss: 173.4081\n",
      "Epoch 366/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.1295 - val_loss: 173.3521\n",
      "Epoch 367/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.0650 - val_loss: 173.3257\n",
      "Epoch 368/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.0225 - val_loss: 173.2947\n",
      "Epoch 369/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 224.9759 - val_loss: 173.2519\n",
      "Epoch 370/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 224.9315 - val_loss: 173.1989\n",
      "Epoch 371/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 224.8840 - val_loss: 173.1565\n",
      "Epoch 372/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 224.8295 - val_loss: 173.1057\n",
      "Epoch 373/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 224.8065 - val_loss: 173.0788\n",
      "Epoch 374/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 224.7521 - val_loss: 173.0541\n",
      "Epoch 375/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 224.7300 - val_loss: 173.0050\n",
      "Epoch 376/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 224.6774 - val_loss: 172.9747\n",
      "Epoch 377/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 224.6449 - val_loss: 172.9539\n",
      "Epoch 378/750\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 224.6192 - val_loss: 172.9038\n",
      "Epoch 379/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 224.5565 - val_loss: 172.8857\n",
      "Epoch 380/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 224.5550 - val_loss: 172.8407\n",
      "Epoch 381/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 224.4969 - val_loss: 172.8526\n",
      "Epoch 382/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 224.4540 - val_loss: 172.8518\n",
      "Epoch 383/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 224.4203 - val_loss: 172.8472\n",
      "Epoch 384/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3996 - val_loss: 172.7927\n",
      "Epoch 385/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3630 - val_loss: 172.7682\n",
      "Epoch 386/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3168 - val_loss: 172.7775\n",
      "Epoch 387/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2829 - val_loss: 172.7464\n",
      "Epoch 388/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2420 - val_loss: 172.7487\n",
      "Epoch 389/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2085 - val_loss: 172.7216\n",
      "Epoch 390/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.1888 - val_loss: 172.6900\n",
      "Epoch 391/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1508 - val_loss: 172.6709\n",
      "Epoch 392/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1119 - val_loss: 172.6531\n",
      "Epoch 393/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0859 - val_loss: 172.6317\n",
      "Epoch 394/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0446 - val_loss: 172.6443\n",
      "Epoch 395/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0180 - val_loss: 172.6159\n",
      "Epoch 396/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9921 - val_loss: 172.5927\n",
      "Epoch 397/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9458 - val_loss: 172.5961\n",
      "Epoch 398/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9337 - val_loss: 172.6225\n",
      "Epoch 399/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8672 - val_loss: 172.5919\n",
      "Epoch 400/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.8405 - val_loss: 172.5677\n",
      "Epoch 401/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.8202 - val_loss: 172.5331\n",
      "Epoch 402/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7794 - val_loss: 172.5292\n",
      "Epoch 403/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7564 - val_loss: 172.5342\n",
      "Epoch 404/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7479 - val_loss: 172.5037\n",
      "Epoch 405/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6824 - val_loss: 172.5236\n",
      "Epoch 406/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.6583 - val_loss: 172.5449\n",
      "Epoch 407/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6222 - val_loss: 172.5358\n",
      "Epoch 408/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5966 - val_loss: 172.5035\n",
      "Epoch 409/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5721 - val_loss: 172.5143\n",
      "Epoch 410/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5378 - val_loss: 172.4987\n",
      "Epoch 411/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4994 - val_loss: 172.4757\n",
      "Epoch 412/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4975 - val_loss: 172.4647\n",
      "Epoch 413/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4695 - val_loss: 172.4586\n",
      "Epoch 414/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4310 - val_loss: 172.4738\n",
      "Epoch 415/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.4124 - val_loss: 172.4920\n",
      "Epoch 416/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3767 - val_loss: 172.4988\n",
      "Epoch 417/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.3519 - val_loss: 172.5117\n",
      "Epoch 418/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3542 - val_loss: 172.4970\n",
      "Epoch 419/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.3253 - val_loss: 172.5363\n",
      "Epoch 420/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3028 - val_loss: 172.5348\n",
      "Epoch 421/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2697 - val_loss: 172.5038\n",
      "Epoch 422/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.2487 - val_loss: 172.4772\n",
      "Epoch 423/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.1931 - val_loss: 172.4914\n",
      "Epoch 424/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.1790 - val_loss: 172.5063\n",
      "Epoch 425/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.1465 - val_loss: 172.5003\n",
      "Epoch 426/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.1318 - val_loss: 172.5312\n",
      "Epoch 427/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0994 - val_loss: 172.5377\n",
      "Epoch 428/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.0779 - val_loss: 172.5214\n",
      "Epoch 429/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0785 - val_loss: 172.5326\n",
      "Epoch 430/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.0550 - val_loss: 172.4617\n",
      "Epoch 431/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0051 - val_loss: 172.4375\n",
      "Epoch 432/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9809 - val_loss: 172.4590\n",
      "Epoch 433/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9556 - val_loss: 172.4536\n",
      "Epoch 434/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9771 - val_loss: 172.4757\n",
      "Epoch 435/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9482 - val_loss: 172.4570\n",
      "Epoch 436/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8992 - val_loss: 172.4765\n",
      "Epoch 437/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8678 - val_loss: 172.4980\n",
      "Epoch 438/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8556 - val_loss: 172.5073\n",
      "Epoch 439/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8531 - val_loss: 172.5283\n",
      "Epoch 440/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8214 - val_loss: 172.5172\n",
      "Epoch 441/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7812 - val_loss: 172.5062\n",
      "Epoch 442/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7640 - val_loss: 172.5044\n",
      "Epoch 443/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7476 - val_loss: 172.5253\n",
      "Epoch 444/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7296 - val_loss: 172.5445\n",
      "Epoch 445/750\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 222.7290 - val_loss: 172.5041\n",
      "Epoch 446/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.6861 - val_loss: 172.5096\n",
      "Epoch 447/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.7013 - val_loss: 172.5005\n",
      "Epoch 448/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6503 - val_loss: 172.5274\n",
      "Epoch 449/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6256 - val_loss: 172.5455\n",
      "Epoch 450/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6219 - val_loss: 172.5626\n",
      "Epoch 451/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6081 - val_loss: 172.5921\n",
      "Epoch 452/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5912 - val_loss: 172.5630\n",
      "Epoch 453/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5600 - val_loss: 172.5523\n",
      "Epoch 454/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5488 - val_loss: 172.5659\n",
      "Epoch 455/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5260 - val_loss: 172.5751\n",
      "Epoch 456/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5134 - val_loss: 172.5581\n",
      "Epoch 457/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4951 - val_loss: 172.5790\n",
      "Epoch 458/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4857 - val_loss: 172.5733\n",
      "Epoch 459/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4543 - val_loss: 172.5622\n",
      "Epoch 460/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4631 - val_loss: 172.5874\n",
      "Epoch 461/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4320 - val_loss: 172.5836\n",
      "Epoch 462/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4233 - val_loss: 172.5950\n",
      "Epoch 463/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3971 - val_loss: 172.5916\n",
      "Epoch 464/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4104 - val_loss: 172.6307\n",
      "Epoch 465/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3926 - val_loss: 172.6569\n",
      "Epoch 466/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3504 - val_loss: 172.6355\n",
      "Epoch 467/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3383 - val_loss: 172.6150\n",
      "Epoch 468/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3251 - val_loss: 172.6124\n",
      "Epoch 469/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3201 - val_loss: 172.6234\n",
      "Epoch 470/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3067 - val_loss: 172.6057\n",
      "Epoch 471/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3191 - val_loss: 172.6053\n",
      "Epoch 472/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3217 - val_loss: 172.6421\n",
      "Epoch 473/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2671 - val_loss: 172.6374\n",
      "Epoch 474/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2636 - val_loss: 172.6712\n",
      "Epoch 475/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2648 - val_loss: 172.6529\n",
      "Epoch 476/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.2316 - val_loss: 172.6525\n",
      "Epoch 477/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2491 - val_loss: 172.6883\n",
      "Epoch 478/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2242 - val_loss: 172.6927\n",
      "Epoch 479/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.2275 - val_loss: 172.7575\n",
      "Epoch 480/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1996 - val_loss: 172.7708\n",
      "Epoch 481/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1791 - val_loss: 172.7767\n",
      "Epoch 482/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1615 - val_loss: 172.7861\n",
      "Epoch 483/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1772 - val_loss: 172.8202\n",
      "Epoch 484/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1387 - val_loss: 172.8230\n",
      "Epoch 485/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1648 - val_loss: 172.8529\n",
      "Epoch 486/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1339 - val_loss: 172.8623\n",
      "Epoch 487/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2160 - val_loss: 172.9366\n",
      "Epoch 488/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1077 - val_loss: 172.9327\n",
      "Epoch 489/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0953 - val_loss: 172.9182\n",
      "Epoch 490/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1257 - val_loss: 172.9543\n",
      "Epoch 491/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0806 - val_loss: 172.9495\n",
      "Epoch 492/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0620 - val_loss: 172.9367\n",
      "Epoch 493/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0770 - val_loss: 172.9280\n",
      "Epoch 494/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0518 - val_loss: 172.9634\n",
      "Epoch 495/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0454 - val_loss: 172.9726\n",
      "Epoch 496/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0387 - val_loss: 172.9686\n",
      "Epoch 497/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0616 - val_loss: 172.9993\n",
      "Epoch 498/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0079 - val_loss: 172.9751\n",
      "Epoch 499/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0041 - val_loss: 172.9778\n",
      "Epoch 500/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0062 - val_loss: 172.9688\n",
      "Epoch 501/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9845 - val_loss: 172.9870\n",
      "Epoch 502/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9914 - val_loss: 173.0027\n",
      "Epoch 503/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9730 - val_loss: 173.0130\n",
      "Epoch 504/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9902 - val_loss: 173.0560\n",
      "Epoch 505/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.9536 - val_loss: 173.0494\n",
      "Epoch 506/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.9635 - val_loss: 173.0718\n",
      "Epoch 507/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9405 - val_loss: 173.0461\n",
      "Epoch 508/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9564 - val_loss: 173.0213\n",
      "Epoch 509/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9278 - val_loss: 173.0356\n",
      "Epoch 510/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9284 - val_loss: 173.0709\n",
      "Epoch 511/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9208 - val_loss: 173.0493\n",
      "Epoch 512/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9156 - val_loss: 173.0377\n",
      "Epoch 513/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9082 - val_loss: 173.0583\n",
      "Epoch 514/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8949 - val_loss: 173.0775\n",
      "Epoch 515/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8776 - val_loss: 173.1310\n",
      "Epoch 516/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8799 - val_loss: 173.1392\n",
      "Epoch 517/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8959 - val_loss: 173.2114\n",
      "Epoch 518/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8849 - val_loss: 173.2417\n",
      "Epoch 519/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8594 - val_loss: 173.2428\n",
      "Epoch 520/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8506 - val_loss: 173.2759\n",
      "Epoch 521/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8525 - val_loss: 173.2889\n",
      "Epoch 522/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8410 - val_loss: 173.2831\n",
      "Epoch 523/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8280 - val_loss: 173.2767\n",
      "Epoch 524/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8429 - val_loss: 173.2815\n",
      "Epoch 525/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8029 - val_loss: 173.3001\n",
      "Epoch 526/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8170 - val_loss: 173.3107\n",
      "Epoch 527/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7925 - val_loss: 173.3041\n",
      "Epoch 528/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8133 - val_loss: 173.3366\n",
      "Epoch 529/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7939 - val_loss: 173.3625\n",
      "Epoch 530/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7828 - val_loss: 173.3360\n",
      "Epoch 531/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8193 - val_loss: 173.3145\n",
      "Epoch 532/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7747 - val_loss: 173.3633\n",
      "Epoch 533/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7712 - val_loss: 173.3943\n",
      "Epoch 534/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7561 - val_loss: 173.4050\n",
      "Epoch 535/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7464 - val_loss: 173.4062\n",
      "Epoch 536/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7773 - val_loss: 173.3900\n",
      "Epoch 537/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7779 - val_loss: 173.4352\n",
      "Epoch 538/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7464 - val_loss: 173.4407\n",
      "Epoch 539/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7216 - val_loss: 173.4479\n",
      "Epoch 540/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7292 - val_loss: 173.4624\n",
      "Epoch 541/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7119 - val_loss: 173.4551\n",
      "Epoch 542/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.7320 - val_loss: 173.4132\n",
      "Epoch 543/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7057 - val_loss: 173.3889\n",
      "Epoch 544/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7071 - val_loss: 173.4059\n",
      "Epoch 545/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6935 - val_loss: 173.4420\n",
      "Epoch 546/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6888 - val_loss: 173.4540\n",
      "Epoch 547/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6926 - val_loss: 173.4655\n",
      "Epoch 548/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7043 - val_loss: 173.4732\n",
      "Epoch 549/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7123 - val_loss: 173.4590\n",
      "Epoch 550/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7026 - val_loss: 173.4868\n",
      "Epoch 551/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6711 - val_loss: 173.4928\n",
      "Epoch 552/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6742 - val_loss: 173.5283\n",
      "Epoch 553/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6716 - val_loss: 173.4891\n",
      "Epoch 554/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6656 - val_loss: 173.5039\n",
      "Epoch 555/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6565 - val_loss: 173.4897\n",
      "Epoch 556/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6834 - val_loss: 173.4777\n",
      "Epoch 557/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6729 - val_loss: 173.5241\n",
      "Epoch 558/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.6539 - val_loss: 173.5381\n",
      "Epoch 559/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6476 - val_loss: 173.5554\n",
      "Epoch 560/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6451 - val_loss: 173.5967\n",
      "Epoch 561/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6402 - val_loss: 173.5937\n",
      "Epoch 562/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6583 - val_loss: 173.5673\n",
      "Epoch 563/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6165 - val_loss: 173.5916\n",
      "Epoch 564/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6234 - val_loss: 173.6214\n",
      "Epoch 565/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6272 - val_loss: 173.6522\n",
      "Epoch 566/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6108 - val_loss: 173.6520\n",
      "Epoch 567/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6054 - val_loss: 173.6726\n",
      "Epoch 568/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6277 - val_loss: 173.7055\n",
      "Epoch 569/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6010 - val_loss: 173.7515\n",
      "Epoch 570/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6152 - val_loss: 173.7957\n",
      "Epoch 571/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5855 - val_loss: 173.7749\n",
      "Epoch 572/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6047 - val_loss: 173.7490\n",
      "Epoch 573/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5877 - val_loss: 173.7805\n",
      "Epoch 574/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6086 - val_loss: 173.7948\n",
      "Epoch 575/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5820 - val_loss: 173.7946\n",
      "Epoch 576/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5724 - val_loss: 173.7853\n",
      "Epoch 577/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5818 - val_loss: 173.8136\n",
      "Epoch 578/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5765 - val_loss: 173.8316\n",
      "Epoch 579/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6025 - val_loss: 173.7769\n",
      "Epoch 580/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5639 - val_loss: 173.7656\n",
      "Epoch 581/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5612 - val_loss: 173.8014\n",
      "Epoch 582/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5527 - val_loss: 173.8015\n",
      "Epoch 583/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5401 - val_loss: 173.8137\n",
      "Epoch 584/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5476 - val_loss: 173.8111\n",
      "Epoch 585/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5469 - val_loss: 173.8351\n",
      "Epoch 586/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5415 - val_loss: 173.8429\n",
      "Epoch 587/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5442 - val_loss: 173.8930\n",
      "Epoch 588/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5542 - val_loss: 173.9314\n",
      "Epoch 589/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5303 - val_loss: 173.9187\n",
      "Epoch 590/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5379 - val_loss: 173.8937\n",
      "Epoch 591/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5557 - val_loss: 173.9386\n",
      "Epoch 592/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5452 - val_loss: 173.9647\n",
      "Epoch 593/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5361 - val_loss: 173.9039\n",
      "Epoch 594/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5337 - val_loss: 173.8887\n",
      "Epoch 595/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5135 - val_loss: 173.8999\n",
      "Epoch 596/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5206 - val_loss: 173.9467\n",
      "Epoch 597/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5114 - val_loss: 173.9378\n",
      "Epoch 598/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5074 - val_loss: 173.9818\n",
      "Epoch 599/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5171 - val_loss: 174.0155\n",
      "Epoch 600/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5065 - val_loss: 173.9821\n",
      "Epoch 601/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5039 - val_loss: 173.9703\n",
      "Epoch 602/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4903 - val_loss: 173.9861\n",
      "Epoch 603/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5040 - val_loss: 173.9904\n",
      "Epoch 604/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5125 - val_loss: 173.9774\n",
      "Epoch 605/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4849 - val_loss: 173.9811\n",
      "Epoch 606/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4985 - val_loss: 173.9951\n",
      "Epoch 607/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4845 - val_loss: 174.0224\n",
      "Epoch 608/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4945 - val_loss: 174.0454\n",
      "Epoch 609/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4920 - val_loss: 174.0286\n",
      "Epoch 610/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4751 - val_loss: 174.0594\n",
      "Epoch 611/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4892 - val_loss: 174.0516\n",
      "Epoch 612/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4742 - val_loss: 174.0833\n",
      "Epoch 613/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4696 - val_loss: 174.1098\n",
      "Epoch 614/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4754 - val_loss: 174.0848\n",
      "Epoch 615/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4590 - val_loss: 174.1053\n",
      "Epoch 616/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4659 - val_loss: 174.1094\n",
      "Epoch 617/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4928 - val_loss: 174.0684\n",
      "Epoch 618/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4613 - val_loss: 174.0817\n",
      "Epoch 619/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4610 - val_loss: 174.0928\n",
      "Epoch 620/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4455 - val_loss: 174.1093\n",
      "Epoch 621/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4491 - val_loss: 174.1424\n",
      "Epoch 622/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4525 - val_loss: 174.1462\n",
      "Epoch 623/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4430 - val_loss: 174.1604\n",
      "Epoch 624/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4801 - val_loss: 174.2273\n",
      "Epoch 625/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4433 - val_loss: 174.2317\n",
      "Epoch 626/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4550 - val_loss: 174.2008\n",
      "Epoch 627/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4569 - val_loss: 174.2577\n",
      "Epoch 628/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4235 - val_loss: 174.2792\n",
      "Epoch 629/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4249 - val_loss: 174.2769\n",
      "Epoch 630/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4210 - val_loss: 174.2805\n",
      "Epoch 631/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4296 - val_loss: 174.3325\n",
      "Epoch 632/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4150 - val_loss: 174.3517\n",
      "Epoch 633/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4258 - val_loss: 174.3699\n",
      "Epoch 634/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4243 - val_loss: 174.3579\n",
      "Epoch 635/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4250 - val_loss: 174.3945\n",
      "Epoch 636/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4112 - val_loss: 174.4154\n",
      "Epoch 637/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4250 - val_loss: 174.4130\n",
      "Epoch 638/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4532 - val_loss: 174.3758\n",
      "Epoch 639/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4196 - val_loss: 174.4099\n",
      "Epoch 640/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4061 - val_loss: 174.4107\n",
      "Epoch 641/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4147 - val_loss: 174.4575\n",
      "Epoch 642/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4023 - val_loss: 174.4560\n",
      "Epoch 643/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4072 - val_loss: 174.4936\n",
      "Epoch 644/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4033 - val_loss: 174.5299\n",
      "Epoch 645/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3820 - val_loss: 174.5307\n",
      "Epoch 646/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3977 - val_loss: 174.5447\n",
      "Epoch 647/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3858 - val_loss: 174.5327\n",
      "Epoch 648/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3956 - val_loss: 174.5259\n",
      "Epoch 649/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3777 - val_loss: 174.5691\n",
      "Epoch 650/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4154 - val_loss: 174.5372\n",
      "Epoch 651/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3749 - val_loss: 174.5610\n",
      "Epoch 652/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3651 - val_loss: 174.5913\n",
      "Epoch 653/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4500 - val_loss: 174.6577\n",
      "Epoch 654/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4019 - val_loss: 174.6934\n",
      "Epoch 655/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3600 - val_loss: 174.6906\n",
      "Epoch 656/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3697 - val_loss: 174.6607\n",
      "Epoch 657/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3842 - val_loss: 174.6866\n",
      "Epoch 658/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3890 - val_loss: 174.6578\n",
      "Epoch 659/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4288 - val_loss: 174.6895\n",
      "Epoch 660/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3566 - val_loss: 174.6763\n",
      "Epoch 661/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3563 - val_loss: 174.6665\n",
      "Epoch 662/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3659 - val_loss: 174.6538\n",
      "Epoch 663/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3681 - val_loss: 174.6925\n",
      "Epoch 664/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3530 - val_loss: 174.6936\n",
      "Epoch 665/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3859 - val_loss: 174.7187\n",
      "Epoch 666/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3610 - val_loss: 174.7195\n",
      "Epoch 667/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3642 - val_loss: 174.7130\n",
      "Epoch 668/750\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 221.3492 - val_loss: 174.7295\n",
      "Epoch 669/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 221.3548 - val_loss: 174.7320\n",
      "Epoch 670/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3457 - val_loss: 174.7322\n",
      "Epoch 671/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3619 - val_loss: 174.7770\n",
      "Epoch 672/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3480 - val_loss: 174.7976\n",
      "Epoch 673/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3720 - val_loss: 174.7654\n",
      "Epoch 674/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3408 - val_loss: 174.7852\n",
      "Epoch 675/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3332 - val_loss: 174.7930\n",
      "Epoch 676/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.3265 - val_loss: 174.7885\n",
      "Epoch 677/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3405 - val_loss: 174.8163\n",
      "Epoch 678/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3264 - val_loss: 174.8110\n",
      "Epoch 679/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3353 - val_loss: 174.8081\n",
      "Epoch 680/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3344 - val_loss: 174.8048\n",
      "Epoch 681/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3569 - val_loss: 174.8197\n",
      "Epoch 682/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3334 - val_loss: 174.8009\n",
      "Epoch 683/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3475 - val_loss: 174.7869\n",
      "Epoch 684/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3938 - val_loss: 174.8533\n",
      "Epoch 685/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3316 - val_loss: 174.8462\n",
      "Epoch 686/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3286 - val_loss: 174.8525\n",
      "Epoch 687/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3283 - val_loss: 174.8361\n",
      "Epoch 688/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3252 - val_loss: 174.8762\n",
      "Epoch 689/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3147 - val_loss: 174.8808\n",
      "Epoch 690/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3158 - val_loss: 174.8825\n",
      "Epoch 691/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3297 - val_loss: 174.9304\n",
      "Epoch 692/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3278 - val_loss: 174.9066\n",
      "Epoch 693/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3268 - val_loss: 174.9383\n",
      "Epoch 694/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3429 - val_loss: 174.9054\n",
      "Epoch 695/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3025 - val_loss: 174.9149\n",
      "Epoch 696/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3230 - val_loss: 174.9510\n",
      "Epoch 697/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3108 - val_loss: 174.9925\n",
      "Epoch 698/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3117 - val_loss: 175.0009\n",
      "Epoch 699/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3067 - val_loss: 174.9887\n",
      "Epoch 700/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3279 - val_loss: 174.9985\n",
      "Epoch 701/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3151 - val_loss: 175.0262\n",
      "Epoch 702/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3163 - val_loss: 175.0202\n",
      "Epoch 703/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3015 - val_loss: 175.0099\n",
      "Epoch 704/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3022 - val_loss: 175.0045\n",
      "Epoch 705/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2889 - val_loss: 175.0246\n",
      "Epoch 706/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3244 - val_loss: 175.0910\n",
      "Epoch 707/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3043 - val_loss: 175.0880\n",
      "Epoch 708/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2894 - val_loss: 175.0932\n",
      "Epoch 709/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2990 - val_loss: 175.1154\n",
      "Epoch 710/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2921 - val_loss: 175.0909\n",
      "Epoch 711/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2830 - val_loss: 175.1104\n",
      "Epoch 712/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2929 - val_loss: 175.1091\n",
      "Epoch 713/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3095 - val_loss: 175.1394\n",
      "Epoch 714/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3086 - val_loss: 175.1276\n",
      "Epoch 715/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3028 - val_loss: 175.1753\n",
      "Epoch 716/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2826 - val_loss: 175.1671\n",
      "Epoch 717/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2922 - val_loss: 175.1787\n",
      "Epoch 718/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2778 - val_loss: 175.1793\n",
      "Epoch 719/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3093 - val_loss: 175.1591\n",
      "Epoch 720/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2878 - val_loss: 175.1891\n",
      "Epoch 721/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2969 - val_loss: 175.2078\n",
      "Epoch 722/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2724 - val_loss: 175.2006\n",
      "Epoch 723/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2708 - val_loss: 175.2221\n",
      "Epoch 724/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2826 - val_loss: 175.2453\n",
      "Epoch 725/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2777 - val_loss: 175.2399\n",
      "Epoch 726/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2700 - val_loss: 175.2406\n",
      "Epoch 727/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2648 - val_loss: 175.2344\n",
      "Epoch 728/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2686 - val_loss: 175.2305\n",
      "Epoch 729/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2665 - val_loss: 175.2431\n",
      "Epoch 730/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2781 - val_loss: 175.2840\n",
      "Epoch 731/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2845 - val_loss: 175.3050\n",
      "Epoch 732/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2755 - val_loss: 175.3234\n",
      "Epoch 733/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2753 - val_loss: 175.2919\n",
      "Epoch 734/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2681 - val_loss: 175.2633\n",
      "Epoch 735/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2729 - val_loss: 175.2478\n",
      "Epoch 736/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2590 - val_loss: 175.2671\n",
      "Epoch 737/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2564 - val_loss: 175.2877\n",
      "Epoch 738/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2624 - val_loss: 175.3236\n",
      "Epoch 739/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2612 - val_loss: 175.3480\n",
      "Epoch 740/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2801 - val_loss: 175.3840\n",
      "Epoch 741/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2567 - val_loss: 175.4025\n",
      "Epoch 742/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2577 - val_loss: 175.3918\n",
      "Epoch 743/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2823 - val_loss: 175.3743\n",
      "Epoch 744/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2529 - val_loss: 175.3802\n",
      "Epoch 745/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2550 - val_loss: 175.3910\n",
      "Epoch 746/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2720 - val_loss: 175.4399\n",
      "Epoch 747/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2533 - val_loss: 175.4432\n",
      "Epoch 748/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2608 - val_loss: 175.4798\n",
      "Epoch 749/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2899 - val_loss: 175.4176\n",
      "Epoch 750/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2483 - val_loss: 175.4659\n",
      ">Saved ./models/comp4948_a2_model_5.h5\n",
      "Epoch 1/750\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 4525.4658 - val_loss: 4522.1221\n",
      "Epoch 2/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4519.2793 - val_loss: 4515.9448\n",
      "Epoch 3/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4513.0513 - val_loss: 4509.8799\n",
      "Epoch 4/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4507.0405 - val_loss: 4503.8887\n",
      "Epoch 5/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4501.0615 - val_loss: 4497.8062\n",
      "Epoch 6/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4494.9468 - val_loss: 4491.6362\n",
      "Epoch 7/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4488.7378 - val_loss: 4485.4106\n",
      "Epoch 8/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4482.5732 - val_loss: 4479.1479\n",
      "Epoch 9/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4476.2495 - val_loss: 4472.8091\n",
      "Epoch 10/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4469.9014 - val_loss: 4466.2817\n",
      "Epoch 11/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4463.2920 - val_loss: 4459.6685\n",
      "Epoch 12/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4456.7344 - val_loss: 4452.9492\n",
      "Epoch 13/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4449.9038 - val_loss: 4446.1064\n",
      "Epoch 14/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4442.9927 - val_loss: 4439.0332\n",
      "Epoch 15/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4435.8452 - val_loss: 4431.8511\n",
      "Epoch 16/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4428.6016 - val_loss: 4424.4263\n",
      "Epoch 17/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4421.1631 - val_loss: 4417.0200\n",
      "Epoch 18/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4413.8345 - val_loss: 4409.5107\n",
      "Epoch 19/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4406.2217 - val_loss: 4401.8354\n",
      "Epoch 20/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4398.4731 - val_loss: 4393.9487\n",
      "Epoch 21/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4390.4688 - val_loss: 4385.8745\n",
      "Epoch 22/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4382.4082 - val_loss: 4377.7847\n",
      "Epoch 23/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4374.2729 - val_loss: 4369.5464\n",
      "Epoch 24/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4365.9087 - val_loss: 4361.1162\n",
      "Epoch 25/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4357.4316 - val_loss: 4352.4565\n",
      "Epoch 26/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4348.8174 - val_loss: 4343.8950\n",
      "Epoch 27/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4340.2119 - val_loss: 4335.1372\n",
      "Epoch 28/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4331.4038 - val_loss: 4326.1372\n",
      "Epoch 29/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4322.3140 - val_loss: 4316.9194\n",
      "Epoch 30/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4313.0039 - val_loss: 4307.6519\n",
      "Epoch 31/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4303.7344 - val_loss: 4298.1987\n",
      "Epoch 32/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4294.4868 - val_loss: 4288.9263\n",
      "Epoch 33/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4285.1221 - val_loss: 4279.4600\n",
      "Epoch 34/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4275.4883 - val_loss: 4269.8628\n",
      "Epoch 35/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4265.7759 - val_loss: 4260.0752\n",
      "Epoch 36/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4256.1562 - val_loss: 4250.4077\n",
      "Epoch 37/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4246.4624 - val_loss: 4240.4126\n",
      "Epoch 38/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4236.3076 - val_loss: 4230.2363\n",
      "Epoch 39/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4226.0591 - val_loss: 4219.6973\n",
      "Epoch 40/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4215.4487 - val_loss: 4209.1235\n",
      "Epoch 41/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4204.8828 - val_loss: 4198.4106\n",
      "Epoch 42/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4193.9912 - val_loss: 4187.4604\n",
      "Epoch 43/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4182.9551 - val_loss: 4176.2500\n",
      "Epoch 44/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4171.8735 - val_loss: 4164.8628\n",
      "Epoch 45/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4160.7349 - val_loss: 4153.7681\n",
      "Epoch 46/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4149.4170 - val_loss: 4142.4771\n",
      "Epoch 47/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4138.0361 - val_loss: 4130.9116\n",
      "Epoch 48/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4126.4341 - val_loss: 4119.0786\n",
      "Epoch 49/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4114.5063 - val_loss: 4106.9888\n",
      "Epoch 50/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4102.2998 - val_loss: 4094.6069\n",
      "Epoch 51/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4089.8809 - val_loss: 4082.1616\n",
      "Epoch 52/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4077.2744 - val_loss: 4069.7632\n",
      "Epoch 53/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4065.0857 - val_loss: 4057.3713\n",
      "Epoch 54/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4052.7671 - val_loss: 4044.6655\n",
      "Epoch 55/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4039.8481 - val_loss: 4031.7788\n",
      "Epoch 56/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4026.9016 - val_loss: 4018.5525\n",
      "Epoch 57/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4013.7275 - val_loss: 4005.3647\n",
      "Epoch 58/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4000.3604 - val_loss: 3992.0042\n",
      "Epoch 59/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3987.0452 - val_loss: 3978.5662\n",
      "Epoch 60/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3973.4165 - val_loss: 3964.8350\n",
      "Epoch 61/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3959.6675 - val_loss: 3951.0244\n",
      "Epoch 62/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3946.3320 - val_loss: 3937.7891\n",
      "Epoch 63/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3933.1453 - val_loss: 3924.4797\n",
      "Epoch 64/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3919.6250 - val_loss: 3910.9412\n",
      "Epoch 65/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3905.8384 - val_loss: 3897.0327\n",
      "Epoch 66/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3892.0493 - val_loss: 3882.6069\n",
      "Epoch 67/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3877.5857 - val_loss: 3868.1577\n",
      "Epoch 68/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3863.0315 - val_loss: 3853.4595\n",
      "Epoch 69/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3848.2268 - val_loss: 3838.4268\n",
      "Epoch 70/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3833.1667 - val_loss: 3823.2195\n",
      "Epoch 71/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3817.8931 - val_loss: 3808.1392\n",
      "Epoch 72/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3803.1028 - val_loss: 3793.1165\n",
      "Epoch 73/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3788.0974 - val_loss: 3777.7024\n",
      "Epoch 74/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3772.5125 - val_loss: 3762.0679\n",
      "Epoch 75/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3756.9360 - val_loss: 3746.0505\n",
      "Epoch 76/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3740.7261 - val_loss: 3729.9438\n",
      "Epoch 77/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3724.6465 - val_loss: 3713.6782\n",
      "Epoch 78/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3708.3972 - val_loss: 3697.2607\n",
      "Epoch 79/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3691.9109 - val_loss: 3680.6462\n",
      "Epoch 80/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3675.5327 - val_loss: 3664.4412\n",
      "Epoch 81/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3659.3809 - val_loss: 3648.0212\n",
      "Epoch 82/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3643.2188 - val_loss: 3631.2634\n",
      "Epoch 83/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3626.2456 - val_loss: 3614.3486\n",
      "Epoch 84/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3609.2693 - val_loss: 3597.7158\n",
      "Epoch 85/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3592.9175 - val_loss: 3580.9099\n",
      "Epoch 86/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3575.9731 - val_loss: 3563.7771\n",
      "Epoch 87/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3558.7314 - val_loss: 3546.3103\n",
      "Epoch 88/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3541.3162 - val_loss: 3528.5251\n",
      "Epoch 89/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3523.4790 - val_loss: 3510.4343\n",
      "Epoch 90/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3505.1943 - val_loss: 3492.3943\n",
      "Epoch 91/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3487.2986 - val_loss: 3474.2852\n",
      "Epoch 92/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3469.1890 - val_loss: 3456.1633\n",
      "Epoch 93/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3450.9727 - val_loss: 3437.8601\n",
      "Epoch 94/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3432.7031 - val_loss: 3419.5420\n",
      "Epoch 95/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3414.5642 - val_loss: 3401.2024\n",
      "Epoch 96/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3396.1003 - val_loss: 3382.5273\n",
      "Epoch 97/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3377.3540 - val_loss: 3363.6108\n",
      "Epoch 98/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3358.3796 - val_loss: 3344.3821\n",
      "Epoch 99/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3339.5686 - val_loss: 3325.3665\n",
      "Epoch 100/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3320.2856 - val_loss: 3306.2227\n",
      "Epoch 101/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3301.0229 - val_loss: 3286.9006\n",
      "Epoch 102/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3281.5996 - val_loss: 3267.4182\n",
      "Epoch 103/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3261.8804 - val_loss: 3247.6953\n",
      "Epoch 104/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3242.3250 - val_loss: 3227.9358\n",
      "Epoch 105/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3222.6780 - val_loss: 3208.0029\n",
      "Epoch 106/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3202.8662 - val_loss: 3187.7913\n",
      "Epoch 107/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3182.6133 - val_loss: 3167.3547\n",
      "Epoch 108/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3162.0066 - val_loss: 3146.7307\n",
      "Epoch 109/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3141.6133 - val_loss: 3125.8555\n",
      "Epoch 110/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3120.6108 - val_loss: 3105.0007\n",
      "Epoch 111/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3099.7881 - val_loss: 3083.9482\n",
      "Epoch 112/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3078.6638 - val_loss: 3062.7432\n",
      "Epoch 113/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3057.6174 - val_loss: 3041.6648\n",
      "Epoch 114/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3036.6467 - val_loss: 3020.5242\n",
      "Epoch 115/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3015.3503 - val_loss: 2999.1328\n",
      "Epoch 116/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2993.8875 - val_loss: 2977.6633\n",
      "Epoch 117/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2972.7681 - val_loss: 2956.0427\n",
      "Epoch 118/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2950.9131 - val_loss: 2934.3203\n",
      "Epoch 119/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2929.2976 - val_loss: 2912.3103\n",
      "Epoch 120/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2907.4351 - val_loss: 2890.1233\n",
      "Epoch 121/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2885.3984 - val_loss: 2868.0359\n",
      "Epoch 122/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2863.2236 - val_loss: 2846.0242\n",
      "Epoch 123/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2841.2080 - val_loss: 2823.8914\n",
      "Epoch 124/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2819.3511 - val_loss: 2801.5383\n",
      "Epoch 125/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2797.0884 - val_loss: 2778.9712\n",
      "Epoch 126/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2774.4373 - val_loss: 2756.4436\n",
      "Epoch 127/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2752.0549 - val_loss: 2733.5349\n",
      "Epoch 128/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2729.2278 - val_loss: 2710.8435\n",
      "Epoch 129/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2706.6052 - val_loss: 2688.2615\n",
      "Epoch 130/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2684.1040 - val_loss: 2665.6179\n",
      "Epoch 131/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2662.0330 - val_loss: 2643.0620\n",
      "Epoch 132/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2639.3396 - val_loss: 2620.8147\n",
      "Epoch 133/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2617.5437 - val_loss: 2598.4009\n",
      "Epoch 134/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 2595.0076 - val_loss: 2576.1345\n",
      "Epoch 135/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2573.3357 - val_loss: 2553.8845\n",
      "Epoch 136/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2551.1736 - val_loss: 2531.8584\n",
      "Epoch 137/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2529.0784 - val_loss: 2509.8669\n",
      "Epoch 138/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2507.1746 - val_loss: 2487.5596\n",
      "Epoch 139/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2484.5017 - val_loss: 2465.3044\n",
      "Epoch 140/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2462.7219 - val_loss: 2442.9250\n",
      "Epoch 141/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2440.6719 - val_loss: 2420.5869\n",
      "Epoch 142/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2418.4229 - val_loss: 2398.2646\n",
      "Epoch 143/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2395.9846 - val_loss: 2375.9429\n",
      "Epoch 144/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2373.7180 - val_loss: 2353.3647\n",
      "Epoch 145/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2350.9204 - val_loss: 2330.9636\n",
      "Epoch 146/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2329.1270 - val_loss: 2308.6331\n",
      "Epoch 147/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2306.9426 - val_loss: 2286.1069\n",
      "Epoch 148/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2284.3757 - val_loss: 2263.5647\n",
      "Epoch 149/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2261.9385 - val_loss: 2240.8765\n",
      "Epoch 150/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2239.3271 - val_loss: 2218.0366\n",
      "Epoch 151/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2216.7234 - val_loss: 2195.1802\n",
      "Epoch 152/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2193.7666 - val_loss: 2172.3838\n",
      "Epoch 153/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2171.4534 - val_loss: 2149.2043\n",
      "Epoch 154/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2148.1299 - val_loss: 2126.1931\n",
      "Epoch 155/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2125.5554 - val_loss: 2102.7776\n",
      "Epoch 156/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2102.5713 - val_loss: 2080.2803\n",
      "Epoch 157/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2080.0532 - val_loss: 2057.7073\n",
      "Epoch 158/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2057.5640 - val_loss: 2034.9581\n",
      "Epoch 159/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2034.9025 - val_loss: 2012.1827\n",
      "Epoch 160/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2012.3619 - val_loss: 1989.1759\n",
      "Epoch 161/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1989.2128 - val_loss: 1966.2507\n",
      "Epoch 162/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1966.4955 - val_loss: 1943.1495\n",
      "Epoch 163/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1943.7892 - val_loss: 1919.9546\n",
      "Epoch 164/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1920.7455 - val_loss: 1897.0718\n",
      "Epoch 165/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1897.8074 - val_loss: 1874.5077\n",
      "Epoch 166/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1876.2073 - val_loss: 1852.5988\n",
      "Epoch 167/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1854.3073 - val_loss: 1830.8427\n",
      "Epoch 168/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1833.0632 - val_loss: 1808.8575\n",
      "Epoch 169/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1810.8879 - val_loss: 1787.1581\n",
      "Epoch 170/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1789.4398 - val_loss: 1765.3505\n",
      "Epoch 171/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1767.6268 - val_loss: 1743.6421\n",
      "Epoch 172/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1746.4818 - val_loss: 1721.6973\n",
      "Epoch 173/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1724.3322 - val_loss: 1700.0050\n",
      "Epoch 174/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1702.8824 - val_loss: 1678.1188\n",
      "Epoch 175/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1681.4003 - val_loss: 1656.1483\n",
      "Epoch 176/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1659.6121 - val_loss: 1634.2137\n",
      "Epoch 177/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1637.7314 - val_loss: 1612.3536\n",
      "Epoch 178/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1616.1381 - val_loss: 1590.3774\n",
      "Epoch 179/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1594.5494 - val_loss: 1568.3579\n",
      "Epoch 180/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1572.8601 - val_loss: 1546.6332\n",
      "Epoch 181/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1551.1779 - val_loss: 1525.0536\n",
      "Epoch 182/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1529.6753 - val_loss: 1503.6827\n",
      "Epoch 183/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1509.0234 - val_loss: 1482.9210\n",
      "Epoch 184/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1488.4651 - val_loss: 1462.2521\n",
      "Epoch 185/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1467.9872 - val_loss: 1441.6173\n",
      "Epoch 186/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1447.6128 - val_loss: 1420.9451\n",
      "Epoch 187/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1427.3191 - val_loss: 1400.1632\n",
      "Epoch 188/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1406.3969 - val_loss: 1379.7241\n",
      "Epoch 189/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1386.4344 - val_loss: 1359.1224\n",
      "Epoch 190/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1366.4270 - val_loss: 1338.9501\n",
      "Epoch 191/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1346.4792 - val_loss: 1319.2889\n",
      "Epoch 192/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1326.9330 - val_loss: 1299.7562\n",
      "Epoch 193/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1307.7832 - val_loss: 1280.0739\n",
      "Epoch 194/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1287.9535 - val_loss: 1260.7355\n",
      "Epoch 195/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1268.8754 - val_loss: 1241.2372\n",
      "Epoch 196/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1250.1157 - val_loss: 1221.4785\n",
      "Epoch 197/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1230.5178 - val_loss: 1202.0652\n",
      "Epoch 198/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1211.1337 - val_loss: 1182.9644\n",
      "Epoch 199/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1192.6261 - val_loss: 1164.1973\n",
      "Epoch 200/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1174.2863 - val_loss: 1145.4302\n",
      "Epoch 201/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1155.8463 - val_loss: 1126.7592\n",
      "Epoch 202/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1137.3579 - val_loss: 1108.3019\n",
      "Epoch 203/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1119.2793 - val_loss: 1089.9136\n",
      "Epoch 204/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1101.5007 - val_loss: 1071.5568\n",
      "Epoch 205/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1083.2402 - val_loss: 1053.6271\n",
      "Epoch 206/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1065.4425 - val_loss: 1035.7765\n",
      "Epoch 207/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1048.2327 - val_loss: 1017.7830\n",
      "Epoch 208/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1030.3147 - val_loss: 1000.1500\n",
      "Epoch 209/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1012.9704 - val_loss: 982.5730\n",
      "Epoch 210/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 995.8049 - val_loss: 964.9916\n",
      "Epoch 211/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 978.4858 - val_loss: 947.6067\n",
      "Epoch 212/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 961.4464 - val_loss: 930.3217\n",
      "Epoch 213/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 944.3965 - val_loss: 913.2314\n",
      "Epoch 214/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 928.0621 - val_loss: 896.1910\n",
      "Epoch 215/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 911.3158 - val_loss: 879.4711\n",
      "Epoch 216/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 894.9540 - val_loss: 862.8897\n",
      "Epoch 217/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 878.6833 - val_loss: 846.5143\n",
      "Epoch 218/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 862.5633 - val_loss: 830.3367\n",
      "Epoch 219/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 846.9301 - val_loss: 814.1918\n",
      "Epoch 220/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 831.0744 - val_loss: 798.3350\n",
      "Epoch 221/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 815.5986 - val_loss: 782.6602\n",
      "Epoch 222/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 800.3690 - val_loss: 767.1841\n",
      "Epoch 223/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 785.2725 - val_loss: 751.8657\n",
      "Epoch 224/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 770.2623 - val_loss: 736.8674\n",
      "Epoch 225/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 755.7141 - val_loss: 722.0729\n",
      "Epoch 226/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 741.2512 - val_loss: 707.5225\n",
      "Epoch 227/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 727.1290 - val_loss: 693.1319\n",
      "Epoch 228/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 713.1674 - val_loss: 678.8950\n",
      "Epoch 229/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 699.5431 - val_loss: 664.7675\n",
      "Epoch 230/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 685.4961 - val_loss: 651.1042\n",
      "Epoch 231/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 672.5111 - val_loss: 637.4315\n",
      "Epoch 232/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 659.1205 - val_loss: 624.0928\n",
      "Epoch 233/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 646.1273 - val_loss: 610.9680\n",
      "Epoch 234/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 633.7055 - val_loss: 597.8795\n",
      "Epoch 235/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 621.2311 - val_loss: 585.0433\n",
      "Epoch 236/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 608.8777 - val_loss: 572.5627\n",
      "Epoch 237/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 596.6064 - val_loss: 560.5688\n",
      "Epoch 238/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 584.8871 - val_loss: 548.7603\n",
      "Epoch 239/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 573.4711 - val_loss: 537.0195\n",
      "Epoch 240/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 562.3168 - val_loss: 525.3674\n",
      "Epoch 241/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 550.9092 - val_loss: 514.1091\n",
      "Epoch 242/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 540.3634 - val_loss: 502.8843\n",
      "Epoch 243/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 529.4127 - val_loss: 492.1219\n",
      "Epoch 244/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 519.0281 - val_loss: 481.5897\n",
      "Epoch 245/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 508.7714 - val_loss: 471.2851\n",
      "Epoch 246/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 498.8455 - val_loss: 461.1326\n",
      "Epoch 247/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 489.2133 - val_loss: 451.1151\n",
      "Epoch 248/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 479.6597 - val_loss: 441.3225\n",
      "Epoch 249/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 470.1498 - val_loss: 431.8739\n",
      "Epoch 250/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 461.5105 - val_loss: 422.4052\n",
      "Epoch 251/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 452.1967 - val_loss: 413.4691\n",
      "Epoch 252/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 443.6782 - val_loss: 404.7313\n",
      "Epoch 253/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 435.2040 - val_loss: 396.2593\n",
      "Epoch 254/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 427.1649 - val_loss: 387.9434\n",
      "Epoch 255/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 419.7202 - val_loss: 379.5738\n",
      "Epoch 256/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 411.6014 - val_loss: 371.7365\n",
      "Epoch 257/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 404.1548 - val_loss: 364.1681\n",
      "Epoch 258/750\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 397.0920 - val_loss: 356.6944\n",
      "Epoch 259/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 389.9484 - val_loss: 349.4671\n",
      "Epoch 260/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 383.0955 - val_loss: 342.5256\n",
      "Epoch 261/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 376.6205 - val_loss: 335.7098\n",
      "Epoch 262/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 370.2039 - val_loss: 329.0814\n",
      "Epoch 263/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 364.1248 - val_loss: 322.6126\n",
      "Epoch 264/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 357.7023 - val_loss: 316.6222\n",
      "Epoch 265/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 352.3336 - val_loss: 310.5499\n",
      "Epoch 266/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 346.7387 - val_loss: 304.6922\n",
      "Epoch 267/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 341.1653 - val_loss: 299.1391\n",
      "Epoch 268/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 335.9421 - val_loss: 293.7808\n",
      "Epoch 269/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 331.1934 - val_loss: 288.4333\n",
      "Epoch 270/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 325.8315 - val_loss: 283.6150\n",
      "Epoch 271/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 321.3808 - val_loss: 278.8281\n",
      "Epoch 272/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 317.0841 - val_loss: 274.0611\n",
      "Epoch 273/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 312.5470 - val_loss: 269.5972\n",
      "Epoch 274/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 308.9299 - val_loss: 265.0036\n",
      "Epoch 275/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 304.4897 - val_loss: 260.8967\n",
      "Epoch 276/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 300.5905 - val_loss: 257.0071\n",
      "Epoch 277/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 296.9911 - val_loss: 253.1943\n",
      "Epoch 278/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 293.4818 - val_loss: 249.5328\n",
      "Epoch 279/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 290.1635 - val_loss: 245.9411\n",
      "Epoch 280/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 286.8223 - val_loss: 242.5160\n",
      "Epoch 281/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 283.6536 - val_loss: 239.2380\n",
      "Epoch 282/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 280.8540 - val_loss: 235.9975\n",
      "Epoch 283/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 277.8389 - val_loss: 232.9673\n",
      "Epoch 284/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 275.2480 - val_loss: 229.9682\n",
      "Epoch 285/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 272.4674 - val_loss: 227.2166\n",
      "Epoch 286/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 269.9770 - val_loss: 224.5786\n",
      "Epoch 287/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 267.6492 - val_loss: 222.0115\n",
      "Epoch 288/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 265.3018 - val_loss: 219.6486\n",
      "Epoch 289/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 263.3553 - val_loss: 217.2751\n",
      "Epoch 290/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 261.2168 - val_loss: 215.1000\n",
      "Epoch 291/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 259.3175 - val_loss: 213.0402\n",
      "Epoch 292/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 257.5630 - val_loss: 211.0133\n",
      "Epoch 293/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 255.7725 - val_loss: 209.1196\n",
      "Epoch 294/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 254.0240 - val_loss: 207.3963\n",
      "Epoch 295/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 252.3805 - val_loss: 205.7916\n",
      "Epoch 296/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 251.0148 - val_loss: 204.1009\n",
      "Epoch 297/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 249.6309 - val_loss: 202.4643\n",
      "Epoch 298/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 248.2063 - val_loss: 200.9251\n",
      "Epoch 299/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 246.7291 - val_loss: 199.5784\n",
      "Epoch 300/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 245.6784 - val_loss: 198.1469\n",
      "Epoch 301/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 244.5938 - val_loss: 196.7487\n",
      "Epoch 302/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 243.2682 - val_loss: 195.5773\n",
      "Epoch 303/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 242.3283 - val_loss: 194.3904\n",
      "Epoch 304/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 241.2344 - val_loss: 193.2975\n",
      "Epoch 305/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 240.3015 - val_loss: 192.2274\n",
      "Epoch 306/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 239.3810 - val_loss: 191.2277\n",
      "Epoch 307/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 238.7146 - val_loss: 190.1640\n",
      "Epoch 308/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 237.7413 - val_loss: 189.2610\n",
      "Epoch 309/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 237.0881 - val_loss: 188.3463\n",
      "Epoch 310/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 236.2980 - val_loss: 187.5399\n",
      "Epoch 311/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 235.7371 - val_loss: 186.7301\n",
      "Epoch 312/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 234.9773 - val_loss: 186.0647\n",
      "Epoch 313/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 234.4182 - val_loss: 185.3896\n",
      "Epoch 314/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 233.9221 - val_loss: 184.7036\n",
      "Epoch 315/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 233.5520 - val_loss: 184.0037\n",
      "Epoch 316/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232.8976 - val_loss: 183.4644\n",
      "Epoch 317/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 232.4349 - val_loss: 182.9589\n",
      "Epoch 318/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.9924 - val_loss: 182.4906\n",
      "Epoch 319/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.6272 - val_loss: 181.9944\n",
      "Epoch 320/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.2271 - val_loss: 181.5339\n",
      "Epoch 321/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.9220 - val_loss: 181.0719\n",
      "Epoch 322/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 230.5247 - val_loss: 180.6545\n",
      "Epoch 323/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.1634 - val_loss: 180.2767\n",
      "Epoch 324/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.9426 - val_loss: 179.8505\n",
      "Epoch 325/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.6521 - val_loss: 179.4660\n",
      "Epoch 326/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.3717 - val_loss: 179.1082\n",
      "Epoch 327/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.1394 - val_loss: 178.7837\n",
      "Epoch 328/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.8589 - val_loss: 178.5218\n",
      "Epoch 329/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.6528 - val_loss: 178.2725\n",
      "Epoch 330/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.4607 - val_loss: 178.0451\n",
      "Epoch 331/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.2672 - val_loss: 177.8306\n",
      "Epoch 332/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.1428 - val_loss: 177.5755\n",
      "Epoch 333/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.9568 - val_loss: 177.3552\n",
      "Epoch 334/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.7583 - val_loss: 177.1825\n",
      "Epoch 335/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.6022 - val_loss: 177.0146\n",
      "Epoch 336/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.4807 - val_loss: 176.8300\n",
      "Epoch 337/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.3393 - val_loss: 176.6546\n",
      "Epoch 338/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.2366 - val_loss: 176.4435\n",
      "Epoch 339/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.0705 - val_loss: 176.2936\n",
      "Epoch 340/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.9581 - val_loss: 176.1191\n",
      "Epoch 341/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.8588 - val_loss: 175.9258\n",
      "Epoch 342/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.6907 - val_loss: 175.8056\n",
      "Epoch 343/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.6129 - val_loss: 175.6616\n",
      "Epoch 344/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.4862 - val_loss: 175.5269\n",
      "Epoch 345/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.3909 - val_loss: 175.4093\n",
      "Epoch 346/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.3281 - val_loss: 175.2774\n",
      "Epoch 347/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.2050 - val_loss: 175.2026\n",
      "Epoch 348/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.1822 - val_loss: 175.0702\n",
      "Epoch 349/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.0588 - val_loss: 174.9809\n",
      "Epoch 350/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.9804 - val_loss: 174.9072\n",
      "Epoch 351/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.9051 - val_loss: 174.8018\n",
      "Epoch 352/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.8534 - val_loss: 174.6947\n",
      "Epoch 353/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.7572 - val_loss: 174.6403\n",
      "Epoch 354/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.7176 - val_loss: 174.5427\n",
      "Epoch 355/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.6452 - val_loss: 174.4521\n",
      "Epoch 356/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 225.5639 - val_loss: 174.4216\n",
      "Epoch 357/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 225.5003 - val_loss: 174.3697\n",
      "Epoch 358/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.4449 - val_loss: 174.2970\n",
      "Epoch 359/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.3927 - val_loss: 174.2187\n",
      "Epoch 360/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.3383 - val_loss: 174.1491\n",
      "Epoch 361/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.2742 - val_loss: 174.0883\n",
      "Epoch 362/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.2126 - val_loss: 174.0665\n",
      "Epoch 363/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.1641 - val_loss: 174.0288\n",
      "Epoch 364/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.1190 - val_loss: 173.9554\n",
      "Epoch 365/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.0637 - val_loss: 173.8909\n",
      "Epoch 366/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.0148 - val_loss: 173.8670\n",
      "Epoch 367/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.9663 - val_loss: 173.7926\n",
      "Epoch 368/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.9308 - val_loss: 173.7355\n",
      "Epoch 369/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.8894 - val_loss: 173.6777\n",
      "Epoch 370/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.8210 - val_loss: 173.6700\n",
      "Epoch 371/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.7891 - val_loss: 173.6558\n",
      "Epoch 372/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.7353 - val_loss: 173.6463\n",
      "Epoch 373/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.6932 - val_loss: 173.6376\n",
      "Epoch 374/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.6798 - val_loss: 173.5984\n",
      "Epoch 375/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.6103 - val_loss: 173.5658\n",
      "Epoch 376/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5758 - val_loss: 173.5327\n",
      "Epoch 377/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.5490 - val_loss: 173.4778\n",
      "Epoch 378/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5088 - val_loss: 173.4338\n",
      "Epoch 379/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.4491 - val_loss: 173.4321\n",
      "Epoch 380/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.4211 - val_loss: 173.4080\n",
      "Epoch 381/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.3682 - val_loss: 173.3860\n",
      "Epoch 382/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.3292 - val_loss: 173.3851\n",
      "Epoch 383/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2874 - val_loss: 173.3721\n",
      "Epoch 384/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2522 - val_loss: 173.3489\n",
      "Epoch 385/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2220 - val_loss: 173.3071\n",
      "Epoch 386/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1758 - val_loss: 173.2724\n",
      "Epoch 387/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1467 - val_loss: 173.2543\n",
      "Epoch 388/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1151 - val_loss: 173.2493\n",
      "Epoch 389/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.0596 - val_loss: 173.2289\n",
      "Epoch 390/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0596 - val_loss: 173.1814\n",
      "Epoch 391/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.0025 - val_loss: 173.1800\n",
      "Epoch 392/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.9863 - val_loss: 173.1242\n",
      "Epoch 393/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9263 - val_loss: 173.1386\n",
      "Epoch 394/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.8930 - val_loss: 173.1125\n",
      "Epoch 395/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.8542 - val_loss: 173.1008\n",
      "Epoch 396/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.8134 - val_loss: 173.0885\n",
      "Epoch 397/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7878 - val_loss: 173.0762\n",
      "Epoch 398/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7363 - val_loss: 173.0875\n",
      "Epoch 399/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7288 - val_loss: 173.0720\n",
      "Epoch 400/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6811 - val_loss: 173.0725\n",
      "Epoch 401/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6458 - val_loss: 173.0514\n",
      "Epoch 402/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.6214 - val_loss: 173.0500\n",
      "Epoch 403/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6001 - val_loss: 173.0600\n",
      "Epoch 404/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5853 - val_loss: 173.0243\n",
      "Epoch 405/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5294 - val_loss: 172.9894\n",
      "Epoch 406/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4814 - val_loss: 173.0185\n",
      "Epoch 407/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4551 - val_loss: 173.0250\n",
      "Epoch 408/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4471 - val_loss: 172.9909\n",
      "Epoch 409/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3997 - val_loss: 172.9835\n",
      "Epoch 410/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3902 - val_loss: 172.9580\n",
      "Epoch 411/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3517 - val_loss: 172.9579\n",
      "Epoch 412/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3136 - val_loss: 172.9759\n",
      "Epoch 413/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.2809 - val_loss: 172.9684\n",
      "Epoch 414/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.2705 - val_loss: 172.9335\n",
      "Epoch 415/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2328 - val_loss: 172.9346\n",
      "Epoch 416/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.2023 - val_loss: 172.9160\n",
      "Epoch 417/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1940 - val_loss: 172.9487\n",
      "Epoch 418/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1910 - val_loss: 172.9024\n",
      "Epoch 419/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1258 - val_loss: 172.9266\n",
      "Epoch 420/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1127 - val_loss: 172.9383\n",
      "Epoch 421/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0667 - val_loss: 172.9435\n",
      "Epoch 422/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0621 - val_loss: 172.9642\n",
      "Epoch 423/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0210 - val_loss: 172.9128\n",
      "Epoch 424/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9857 - val_loss: 172.9014\n",
      "Epoch 425/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9855 - val_loss: 172.8694\n",
      "Epoch 426/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9389 - val_loss: 172.8738\n",
      "Epoch 427/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9198 - val_loss: 172.8529\n",
      "Epoch 428/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8997 - val_loss: 172.8557\n",
      "Epoch 429/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8797 - val_loss: 172.8637\n",
      "Epoch 430/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8997 - val_loss: 172.9025\n",
      "Epoch 431/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8288 - val_loss: 172.8768\n",
      "Epoch 432/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8200 - val_loss: 172.8928\n",
      "Epoch 433/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8228 - val_loss: 172.8585\n",
      "Epoch 434/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7555 - val_loss: 172.8770\n",
      "Epoch 435/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7379 - val_loss: 172.8740\n",
      "Epoch 436/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7232 - val_loss: 172.8963\n",
      "Epoch 437/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6933 - val_loss: 172.8887\n",
      "Epoch 438/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6886 - val_loss: 172.8645\n",
      "Epoch 439/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6562 - val_loss: 172.8787\n",
      "Epoch 440/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6547 - val_loss: 172.8810\n",
      "Epoch 441/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6087 - val_loss: 172.9006\n",
      "Epoch 442/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6030 - val_loss: 172.9202\n",
      "Epoch 443/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5663 - val_loss: 172.9410\n",
      "Epoch 444/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5772 - val_loss: 172.9485\n",
      "Epoch 445/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5307 - val_loss: 172.9509\n",
      "Epoch 446/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5120 - val_loss: 173.0025\n",
      "Epoch 447/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5045 - val_loss: 173.0433\n",
      "Epoch 448/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4879 - val_loss: 173.0520\n",
      "Epoch 449/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4703 - val_loss: 173.0688\n",
      "Epoch 450/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4594 - val_loss: 173.0443\n",
      "Epoch 451/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4214 - val_loss: 173.0548\n",
      "Epoch 452/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4031 - val_loss: 173.0563\n",
      "Epoch 453/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3851 - val_loss: 173.0383\n",
      "Epoch 454/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3797 - val_loss: 173.0000\n",
      "Epoch 455/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4270 - val_loss: 172.9686\n",
      "Epoch 456/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3626 - val_loss: 173.0245\n",
      "Epoch 457/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 222.3299 - val_loss: 173.0342\n",
      "Epoch 458/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2941 - val_loss: 173.0567\n",
      "Epoch 459/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2908 - val_loss: 173.0804\n",
      "Epoch 460/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3043 - val_loss: 173.0814\n",
      "Epoch 461/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2647 - val_loss: 173.1151\n",
      "Epoch 462/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2429 - val_loss: 173.1236\n",
      "Epoch 463/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2526 - val_loss: 173.1160\n",
      "Epoch 464/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2226 - val_loss: 173.1703\n",
      "Epoch 465/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2073 - val_loss: 173.1839\n",
      "Epoch 466/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1950 - val_loss: 173.1901\n",
      "Epoch 467/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1840 - val_loss: 173.1981\n",
      "Epoch 468/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1740 - val_loss: 173.2386\n",
      "Epoch 469/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1609 - val_loss: 173.2369\n",
      "Epoch 470/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1805 - val_loss: 173.2212\n",
      "Epoch 471/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1507 - val_loss: 173.2524\n",
      "Epoch 472/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1326 - val_loss: 173.2276\n",
      "Epoch 473/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1029 - val_loss: 173.2200\n",
      "Epoch 474/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1065 - val_loss: 173.2411\n",
      "Epoch 475/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0975 - val_loss: 173.2576\n",
      "Epoch 476/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0843 - val_loss: 173.2785\n",
      "Epoch 477/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0690 - val_loss: 173.2707\n",
      "Epoch 478/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0866 - val_loss: 173.2651\n",
      "Epoch 479/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0784 - val_loss: 173.3017\n",
      "Epoch 480/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0652 - val_loss: 173.2628\n",
      "Epoch 481/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0228 - val_loss: 173.2811\n",
      "Epoch 482/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0227 - val_loss: 173.2965\n",
      "Epoch 483/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0154 - val_loss: 173.3033\n",
      "Epoch 484/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0002 - val_loss: 173.2982\n",
      "Epoch 485/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0294 - val_loss: 173.3259\n",
      "Epoch 486/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0249 - val_loss: 173.2712\n",
      "Epoch 487/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9687 - val_loss: 173.2746\n",
      "Epoch 488/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9954 - val_loss: 173.3151\n",
      "Epoch 489/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9860 - val_loss: 173.2840\n",
      "Epoch 490/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.9742 - val_loss: 173.2639\n",
      "Epoch 491/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9395 - val_loss: 173.2913\n",
      "Epoch 492/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9226 - val_loss: 173.2864\n",
      "Epoch 493/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9966 - val_loss: 173.2538\n",
      "Epoch 494/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9141 - val_loss: 173.2887\n",
      "Epoch 495/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9164 - val_loss: 173.3118\n",
      "Epoch 496/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8937 - val_loss: 173.3015\n",
      "Epoch 497/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8878 - val_loss: 173.3171\n",
      "Epoch 498/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8777 - val_loss: 173.3178\n",
      "Epoch 499/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9250 - val_loss: 173.3599\n",
      "Epoch 500/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8884 - val_loss: 173.3324\n",
      "Epoch 501/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8480 - val_loss: 173.3341\n",
      "Epoch 502/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8538 - val_loss: 173.3240\n",
      "Epoch 503/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8301 - val_loss: 173.3337\n",
      "Epoch 504/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8419 - val_loss: 173.3632\n",
      "Epoch 505/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8295 - val_loss: 173.3750\n",
      "Epoch 506/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8170 - val_loss: 173.3666\n",
      "Epoch 507/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8063 - val_loss: 173.3618\n",
      "Epoch 508/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8041 - val_loss: 173.3690\n",
      "Epoch 509/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7962 - val_loss: 173.3792\n",
      "Epoch 510/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7925 - val_loss: 173.3976\n",
      "Epoch 511/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7905 - val_loss: 173.4003\n",
      "Epoch 512/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7666 - val_loss: 173.4285\n",
      "Epoch 513/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7685 - val_loss: 173.4708\n",
      "Epoch 514/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7671 - val_loss: 173.4850\n",
      "Epoch 515/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7527 - val_loss: 173.5063\n",
      "Epoch 516/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7373 - val_loss: 173.5167\n",
      "Epoch 517/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7348 - val_loss: 173.5337\n",
      "Epoch 518/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7507 - val_loss: 173.5214\n",
      "Epoch 519/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7347 - val_loss: 173.5222\n",
      "Epoch 520/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7760 - val_loss: 173.5870\n",
      "Epoch 521/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7091 - val_loss: 173.5916\n",
      "Epoch 522/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7130 - val_loss: 173.6190\n",
      "Epoch 523/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7126 - val_loss: 173.6170\n",
      "Epoch 524/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7040 - val_loss: 173.6258\n",
      "Epoch 525/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7090 - val_loss: 173.6207\n",
      "Epoch 526/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7136 - val_loss: 173.6044\n",
      "Epoch 527/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6935 - val_loss: 173.6314\n",
      "Epoch 528/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6806 - val_loss: 173.6563\n",
      "Epoch 529/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6813 - val_loss: 173.6782\n",
      "Epoch 530/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6627 - val_loss: 173.6741\n",
      "Epoch 531/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6911 - val_loss: 173.6742\n",
      "Epoch 532/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6518 - val_loss: 173.6717\n",
      "Epoch 533/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6525 - val_loss: 173.6920\n",
      "Epoch 534/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6635 - val_loss: 173.6715\n",
      "Epoch 535/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6717 - val_loss: 173.6801\n",
      "Epoch 536/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6301 - val_loss: 173.7192\n",
      "Epoch 537/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6361 - val_loss: 173.7655\n",
      "Epoch 538/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6315 - val_loss: 173.7951\n",
      "Epoch 539/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6313 - val_loss: 173.7760\n",
      "Epoch 540/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6275 - val_loss: 173.7861\n",
      "Epoch 541/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6138 - val_loss: 173.7986\n",
      "Epoch 542/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6054 - val_loss: 173.8019\n",
      "Epoch 543/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6161 - val_loss: 173.8373\n",
      "Epoch 544/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6338 - val_loss: 173.8746\n",
      "Epoch 545/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5914 - val_loss: 173.8579\n",
      "Epoch 546/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5971 - val_loss: 173.8546\n",
      "Epoch 547/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5919 - val_loss: 173.8424\n",
      "Epoch 548/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6006 - val_loss: 173.8475\n",
      "Epoch 549/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5901 - val_loss: 173.8745\n",
      "Epoch 550/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5695 - val_loss: 173.8785\n",
      "Epoch 551/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5932 - val_loss: 173.8586\n",
      "Epoch 552/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5719 - val_loss: 173.8787\n",
      "Epoch 553/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5669 - val_loss: 173.8938\n",
      "Epoch 554/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5526 - val_loss: 173.9397\n",
      "Epoch 555/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5546 - val_loss: 173.9803\n",
      "Epoch 556/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5497 - val_loss: 174.0201\n",
      "Epoch 557/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5497 - val_loss: 174.0570\n",
      "Epoch 558/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5620 - val_loss: 174.0889\n",
      "Epoch 559/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5489 - val_loss: 174.0730\n",
      "Epoch 560/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5982 - val_loss: 174.1058\n",
      "Epoch 561/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5346 - val_loss: 174.0876\n",
      "Epoch 562/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5464 - val_loss: 174.1048\n",
      "Epoch 563/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5250 - val_loss: 174.0680\n",
      "Epoch 564/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5486 - val_loss: 174.0283\n",
      "Epoch 565/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.5391 - val_loss: 174.0013\n",
      "Epoch 566/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5139 - val_loss: 174.0386\n",
      "Epoch 567/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5129 - val_loss: 174.0764\n",
      "Epoch 568/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5086 - val_loss: 174.1014\n",
      "Epoch 569/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5220 - val_loss: 174.1334\n",
      "Epoch 570/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5203 - val_loss: 174.1775\n",
      "Epoch 571/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5079 - val_loss: 174.2038\n",
      "Epoch 572/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5032 - val_loss: 174.1917\n",
      "Epoch 573/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4966 - val_loss: 174.1750\n",
      "Epoch 574/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4839 - val_loss: 174.1859\n",
      "Epoch 575/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4844 - val_loss: 174.1934\n",
      "Epoch 576/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4772 - val_loss: 174.2169\n",
      "Epoch 577/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4932 - val_loss: 174.2315\n",
      "Epoch 578/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4862 - val_loss: 174.2859\n",
      "Epoch 579/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4703 - val_loss: 174.2755\n",
      "Epoch 580/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4731 - val_loss: 174.2981\n",
      "Epoch 581/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4732 - val_loss: 174.2825\n",
      "Epoch 582/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4700 - val_loss: 174.2848\n",
      "Epoch 583/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4655 - val_loss: 174.2714\n",
      "Epoch 584/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4583 - val_loss: 174.2970\n",
      "Epoch 585/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4535 - val_loss: 174.3118\n",
      "Epoch 586/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4518 - val_loss: 174.3309\n",
      "Epoch 587/750\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 221.4503 - val_loss: 174.3275\n",
      "Epoch 588/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4438 - val_loss: 174.3322\n",
      "Epoch 589/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4360 - val_loss: 174.3317\n",
      "Epoch 590/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4313 - val_loss: 174.3390\n",
      "Epoch 591/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4338 - val_loss: 174.3500\n",
      "Epoch 592/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4297 - val_loss: 174.3695\n",
      "Epoch 593/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4280 - val_loss: 174.3641\n",
      "Epoch 594/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4412 - val_loss: 174.3917\n",
      "Epoch 595/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4235 - val_loss: 174.3680\n",
      "Epoch 596/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4304 - val_loss: 174.3516\n",
      "Epoch 597/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4476 - val_loss: 174.3306\n",
      "Epoch 598/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4053 - val_loss: 174.3581\n",
      "Epoch 599/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4218 - val_loss: 174.4216\n",
      "Epoch 600/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4174 - val_loss: 174.4595\n",
      "Epoch 601/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4120 - val_loss: 174.4527\n",
      "Epoch 602/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4362 - val_loss: 174.4103\n",
      "Epoch 603/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4006 - val_loss: 174.4346\n",
      "Epoch 604/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4181 - val_loss: 174.4046\n",
      "Epoch 605/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4097 - val_loss: 174.3954\n",
      "Epoch 606/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4256 - val_loss: 174.4306\n",
      "Epoch 607/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3930 - val_loss: 174.4526\n",
      "Epoch 608/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4070 - val_loss: 174.4469\n",
      "Epoch 609/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4552 - val_loss: 174.5048\n",
      "Epoch 610/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3866 - val_loss: 174.5158\n",
      "Epoch 611/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4012 - val_loss: 174.4910\n",
      "Epoch 612/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3922 - val_loss: 174.5392\n",
      "Epoch 613/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3971 - val_loss: 174.5701\n",
      "Epoch 614/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3734 - val_loss: 174.5673\n",
      "Epoch 615/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3974 - val_loss: 174.5453\n",
      "Epoch 616/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3819 - val_loss: 174.5891\n",
      "Epoch 617/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3760 - val_loss: 174.6201\n",
      "Epoch 618/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3849 - val_loss: 174.6352\n",
      "Epoch 619/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3731 - val_loss: 174.6250\n",
      "Epoch 620/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3637 - val_loss: 174.6616\n",
      "Epoch 621/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3777 - val_loss: 174.6275\n",
      "Epoch 622/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3549 - val_loss: 174.6622\n",
      "Epoch 623/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3798 - val_loss: 174.7168\n",
      "Epoch 624/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3761 - val_loss: 174.7321\n",
      "Epoch 625/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3723 - val_loss: 174.7277\n",
      "Epoch 626/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3580 - val_loss: 174.7401\n",
      "Epoch 627/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3667 - val_loss: 174.7805\n",
      "Epoch 628/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3566 - val_loss: 174.7495\n",
      "Epoch 629/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3475 - val_loss: 174.7548\n",
      "Epoch 630/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3596 - val_loss: 174.7654\n",
      "Epoch 631/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3664 - val_loss: 174.7924\n",
      "Epoch 632/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3430 - val_loss: 174.8090\n",
      "Epoch 633/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3450 - val_loss: 174.8085\n",
      "Epoch 634/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3474 - val_loss: 174.8418\n",
      "Epoch 635/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3474 - val_loss: 174.8899\n",
      "Epoch 636/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3408 - val_loss: 174.8704\n",
      "Epoch 637/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3459 - val_loss: 174.8908\n",
      "Epoch 638/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3348 - val_loss: 174.8721\n",
      "Epoch 639/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3568 - val_loss: 174.8635\n",
      "Epoch 640/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3431 - val_loss: 174.8703\n",
      "Epoch 641/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3400 - val_loss: 174.8979\n",
      "Epoch 642/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3280 - val_loss: 174.8646\n",
      "Epoch 643/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3304 - val_loss: 174.8694\n",
      "Epoch 644/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3312 - val_loss: 174.8531\n",
      "Epoch 645/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3323 - val_loss: 174.8696\n",
      "Epoch 646/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3257 - val_loss: 174.8818\n",
      "Epoch 647/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3251 - val_loss: 174.8608\n",
      "Epoch 648/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3223 - val_loss: 174.8792\n",
      "Epoch 649/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3194 - val_loss: 174.9017\n",
      "Epoch 650/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3332 - val_loss: 174.8858\n",
      "Epoch 651/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3149 - val_loss: 174.9113\n",
      "Epoch 652/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3218 - val_loss: 174.9669\n",
      "Epoch 653/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3083 - val_loss: 174.9797\n",
      "Epoch 654/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3141 - val_loss: 174.9850\n",
      "Epoch 655/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3048 - val_loss: 175.0110\n",
      "Epoch 656/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3114 - val_loss: 175.0128\n",
      "Epoch 657/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3025 - val_loss: 175.0288\n",
      "Epoch 658/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3045 - val_loss: 175.0358\n",
      "Epoch 659/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2991 - val_loss: 175.0193\n",
      "Epoch 660/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3118 - val_loss: 175.0332\n",
      "Epoch 661/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3159 - val_loss: 175.0077\n",
      "Epoch 662/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3143 - val_loss: 175.0418\n",
      "Epoch 663/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3083 - val_loss: 175.0260\n",
      "Epoch 664/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3089 - val_loss: 175.0722\n",
      "Epoch 665/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2964 - val_loss: 175.0842\n",
      "Epoch 666/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3527 - val_loss: 175.1298\n",
      "Epoch 667/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2929 - val_loss: 175.1161\n",
      "Epoch 668/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2931 - val_loss: 175.1094\n",
      "Epoch 669/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2922 - val_loss: 175.1203\n",
      "Epoch 670/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3081 - val_loss: 175.1580\n",
      "Epoch 671/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2853 - val_loss: 175.1658\n",
      "Epoch 672/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2901 - val_loss: 175.1772\n",
      "Epoch 673/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2847 - val_loss: 175.1819\n",
      "Epoch 674/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.2931 - val_loss: 175.2113\n",
      "Epoch 675/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2828 - val_loss: 175.2049\n",
      "Epoch 676/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2957 - val_loss: 175.1835\n",
      "Epoch 677/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2890 - val_loss: 175.2089\n",
      "Epoch 678/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3064 - val_loss: 175.1847\n",
      "Epoch 679/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2988 - val_loss: 175.1715\n",
      "Epoch 680/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2882 - val_loss: 175.2373\n",
      "Epoch 681/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2753 - val_loss: 175.2699\n",
      "Epoch 682/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2950 - val_loss: 175.2666\n",
      "Epoch 683/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2661 - val_loss: 175.3120\n",
      "Epoch 684/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2780 - val_loss: 175.3441\n",
      "Epoch 685/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3018 - val_loss: 175.3808\n",
      "Epoch 686/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2631 - val_loss: 175.3845\n",
      "Epoch 687/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2762 - val_loss: 175.4208\n",
      "Epoch 688/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2643 - val_loss: 175.4217\n",
      "Epoch 689/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2753 - val_loss: 175.4040\n",
      "Epoch 690/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2542 - val_loss: 175.4227\n",
      "Epoch 691/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2925 - val_loss: 175.3971\n",
      "Epoch 692/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2539 - val_loss: 175.4323\n",
      "Epoch 693/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2615 - val_loss: 175.4493\n",
      "Epoch 694/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2844 - val_loss: 175.4996\n",
      "Epoch 695/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2554 - val_loss: 175.5048\n",
      "Epoch 696/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2564 - val_loss: 175.4913\n",
      "Epoch 697/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2511 - val_loss: 175.5018\n",
      "Epoch 698/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2759 - val_loss: 175.5331\n",
      "Epoch 699/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2498 - val_loss: 175.5179\n",
      "Epoch 700/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2566 - val_loss: 175.4850\n",
      "Epoch 701/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2424 - val_loss: 175.5052\n",
      "Epoch 702/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2585 - val_loss: 175.5143\n",
      "Epoch 703/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2623 - val_loss: 175.5556\n",
      "Epoch 704/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2529 - val_loss: 175.5424\n",
      "Epoch 705/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2526 - val_loss: 175.5510\n",
      "Epoch 706/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2518 - val_loss: 175.5582\n",
      "Epoch 707/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2450 - val_loss: 175.5741\n",
      "Epoch 708/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2354 - val_loss: 175.5863\n",
      "Epoch 709/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2597 - val_loss: 175.6022\n",
      "Epoch 710/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2644 - val_loss: 175.5832\n",
      "Epoch 711/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2503 - val_loss: 175.6109\n",
      "Epoch 712/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2501 - val_loss: 175.6311\n",
      "Epoch 713/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2333 - val_loss: 175.6129\n",
      "Epoch 714/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2371 - val_loss: 175.5998\n",
      "Epoch 715/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2319 - val_loss: 175.5857\n",
      "Epoch 716/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2348 - val_loss: 175.5757\n",
      "Epoch 717/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2673 - val_loss: 175.6131\n",
      "Epoch 718/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2283 - val_loss: 175.6145\n",
      "Epoch 719/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2381 - val_loss: 175.6349\n",
      "Epoch 720/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2300 - val_loss: 175.6438\n",
      "Epoch 721/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2406 - val_loss: 175.6809\n",
      "Epoch 722/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2360 - val_loss: 175.6524\n",
      "Epoch 723/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2279 - val_loss: 175.6607\n",
      "Epoch 724/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2361 - val_loss: 175.6427\n",
      "Epoch 725/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2861 - val_loss: 175.6141\n",
      "Epoch 726/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2348 - val_loss: 175.6290\n",
      "Epoch 727/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2469 - val_loss: 175.6302\n",
      "Epoch 728/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2273 - val_loss: 175.6720\n",
      "Epoch 729/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2197 - val_loss: 175.6660\n",
      "Epoch 730/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2461 - val_loss: 175.6543\n",
      "Epoch 731/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2231 - val_loss: 175.6905\n",
      "Epoch 732/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2232 - val_loss: 175.7234\n",
      "Epoch 733/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2074 - val_loss: 175.7331\n",
      "Epoch 734/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2347 - val_loss: 175.7912\n",
      "Epoch 735/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2264 - val_loss: 175.7828\n",
      "Epoch 736/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3188 - val_loss: 175.8405\n",
      "Epoch 737/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2103 - val_loss: 175.8159\n",
      "Epoch 738/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2284 - val_loss: 175.8055\n",
      "Epoch 739/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2260 - val_loss: 175.7585\n",
      "Epoch 740/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2212 - val_loss: 175.7250\n",
      "Epoch 741/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2133 - val_loss: 175.7384\n",
      "Epoch 742/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2124 - val_loss: 175.7533\n",
      "Epoch 743/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2123 - val_loss: 175.7735\n",
      "Epoch 744/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2152 - val_loss: 175.7850\n",
      "Epoch 745/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2017 - val_loss: 175.8038\n",
      "Epoch 746/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2092 - val_loss: 175.8065\n",
      "Epoch 747/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2071 - val_loss: 175.8341\n",
      "Epoch 748/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2316 - val_loss: 175.8083\n",
      "Epoch 749/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2228 - val_loss: 175.8248\n",
      "Epoch 750/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2007 - val_loss: 175.8249\n",
      ">Saved ./models/comp4948_a2_model_6.h5\n",
      "Epoch 1/750\n",
      "6/6 [==============================] - 0s 31ms/step - loss: 4539.4683 - val_loss: 4536.1084\n",
      "Epoch 2/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4533.1646 - val_loss: 4529.8394\n",
      "Epoch 3/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4526.9648 - val_loss: 4523.7168\n",
      "Epoch 4/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4520.8389 - val_loss: 4517.7686\n",
      "Epoch 5/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4515.1030 - val_loss: 4512.1025\n",
      "Epoch 6/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4509.4043 - val_loss: 4506.4116\n",
      "Epoch 7/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4503.6978 - val_loss: 4500.6362\n",
      "Epoch 8/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4497.9224 - val_loss: 4494.7480\n",
      "Epoch 9/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4492.0137 - val_loss: 4488.8198\n",
      "Epoch 10/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4486.0347 - val_loss: 4482.7583\n",
      "Epoch 11/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4479.9722 - val_loss: 4476.7065\n",
      "Epoch 12/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4473.9199 - val_loss: 4470.5859\n",
      "Epoch 13/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4467.7354 - val_loss: 4464.2886\n",
      "Epoch 14/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4461.3755 - val_loss: 4457.7944\n",
      "Epoch 15/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4454.8545 - val_loss: 4451.1543\n",
      "Epoch 16/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4448.1382 - val_loss: 4444.3374\n",
      "Epoch 17/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4441.3271 - val_loss: 4437.5537\n",
      "Epoch 18/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4434.5278 - val_loss: 4430.5796\n",
      "Epoch 19/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4427.4331 - val_loss: 4423.4282\n",
      "Epoch 20/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4420.2451 - val_loss: 4416.0947\n",
      "Epoch 21/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4412.8335 - val_loss: 4408.7852\n",
      "Epoch 22/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4405.6294 - val_loss: 4401.3901\n",
      "Epoch 23/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4398.1475 - val_loss: 4393.7891\n",
      "Epoch 24/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4390.6094 - val_loss: 4386.0029\n",
      "Epoch 25/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4382.6636 - val_loss: 4378.0854\n",
      "Epoch 26/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4374.7271 - val_loss: 4370.1226\n",
      "Epoch 27/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4366.7363 - val_loss: 4361.9766\n",
      "Epoch 28/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4358.6582 - val_loss: 4353.8281\n",
      "Epoch 29/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4350.3521 - val_loss: 4345.4824\n",
      "Epoch 30/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4341.9644 - val_loss: 4336.9341\n",
      "Epoch 31/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4333.3574 - val_loss: 4328.1216\n",
      "Epoch 32/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4324.4238 - val_loss: 4319.1836\n",
      "Epoch 33/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4315.4663 - val_loss: 4310.2344\n",
      "Epoch 34/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4306.6167 - val_loss: 4301.0864\n",
      "Epoch 35/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4297.4243 - val_loss: 4291.8208\n",
      "Epoch 36/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4288.0903 - val_loss: 4282.2803\n",
      "Epoch 37/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4278.3926 - val_loss: 4272.6206\n",
      "Epoch 38/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4268.9541 - val_loss: 4262.9746\n",
      "Epoch 39/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4259.1909 - val_loss: 4253.0830\n",
      "Epoch 40/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4249.1558 - val_loss: 4243.0942\n",
      "Epoch 41/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4239.2622 - val_loss: 4233.0112\n",
      "Epoch 42/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4229.1475 - val_loss: 4222.6968\n",
      "Epoch 43/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4218.6738 - val_loss: 4212.1743\n",
      "Epoch 44/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4208.0332 - val_loss: 4201.3691\n",
      "Epoch 45/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4197.2109 - val_loss: 4190.6294\n",
      "Epoch 46/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4186.5503 - val_loss: 4179.7969\n",
      "Epoch 47/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4175.6577 - val_loss: 4168.7832\n",
      "Epoch 48/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4164.6270 - val_loss: 4157.4658\n",
      "Epoch 49/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4153.1528 - val_loss: 4146.0278\n",
      "Epoch 50/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4141.9575 - val_loss: 4134.7139\n",
      "Epoch 51/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4130.5244 - val_loss: 4123.1621\n",
      "Epoch 52/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4118.9966 - val_loss: 4111.2949\n",
      "Epoch 53/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4107.0449 - val_loss: 4099.2168\n",
      "Epoch 54/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4094.9229 - val_loss: 4086.8562\n",
      "Epoch 55/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4082.3960 - val_loss: 4074.3826\n",
      "Epoch 56/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4069.8906 - val_loss: 4061.7458\n",
      "Epoch 57/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4057.2336 - val_loss: 4049.0068\n",
      "Epoch 58/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 4044.5857 - val_loss: 4036.0991\n",
      "Epoch 59/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 4031.9121 - val_loss: 4023.4141\n",
      "Epoch 60/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 4019.1287 - val_loss: 4010.4189\n",
      "Epoch 61/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 4006.0203 - val_loss: 3997.2993\n",
      "Epoch 62/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3993.0452 - val_loss: 3984.1018\n",
      "Epoch 63/750\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 3979.6824 - val_loss: 3970.6228\n",
      "Epoch 64/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3966.1228 - val_loss: 3956.8296\n",
      "Epoch 65/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3952.3169 - val_loss: 3942.7842\n",
      "Epoch 66/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3938.2864 - val_loss: 3928.4775\n",
      "Epoch 67/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3923.9678 - val_loss: 3914.1694\n",
      "Epoch 68/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 3909.7253 - val_loss: 3899.7759\n",
      "Epoch 69/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3895.4500 - val_loss: 3885.4709\n",
      "Epoch 70/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3881.0864 - val_loss: 3870.7756\n",
      "Epoch 71/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3866.2769 - val_loss: 3855.8240\n",
      "Epoch 72/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3851.4998 - val_loss: 3840.9177\n",
      "Epoch 73/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3836.5110 - val_loss: 3825.7664\n",
      "Epoch 74/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3821.3809 - val_loss: 3810.6389\n",
      "Epoch 75/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3806.0730 - val_loss: 3795.3423\n",
      "Epoch 76/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3790.8096 - val_loss: 3779.6931\n",
      "Epoch 77/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3775.0166 - val_loss: 3764.2275\n",
      "Epoch 78/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3759.9612 - val_loss: 3749.0381\n",
      "Epoch 79/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3744.5610 - val_loss: 3733.5466\n",
      "Epoch 80/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3729.1221 - val_loss: 3717.8494\n",
      "Epoch 81/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3713.4277 - val_loss: 3701.9788\n",
      "Epoch 82/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3697.4524 - val_loss: 3686.0510\n",
      "Epoch 83/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3681.3801 - val_loss: 3669.7400\n",
      "Epoch 84/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3665.1077 - val_loss: 3652.9829\n",
      "Epoch 85/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3648.5854 - val_loss: 3636.6143\n",
      "Epoch 86/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3631.9807 - val_loss: 3620.1555\n",
      "Epoch 87/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3615.5698 - val_loss: 3603.7080\n",
      "Epoch 88/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3599.0469 - val_loss: 3587.0852\n",
      "Epoch 89/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3582.4314 - val_loss: 3570.0781\n",
      "Epoch 90/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3565.1360 - val_loss: 3552.8857\n",
      "Epoch 91/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3547.8860 - val_loss: 3535.2913\n",
      "Epoch 92/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3530.2944 - val_loss: 3517.2908\n",
      "Epoch 93/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3512.0898 - val_loss: 3499.0649\n",
      "Epoch 94/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3493.9150 - val_loss: 3480.7068\n",
      "Epoch 95/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3475.5989 - val_loss: 3462.5369\n",
      "Epoch 96/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3457.4507 - val_loss: 3444.3438\n",
      "Epoch 97/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3439.5159 - val_loss: 3426.5330\n",
      "Epoch 98/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3421.6147 - val_loss: 3408.9070\n",
      "Epoch 99/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3404.1587 - val_loss: 3390.8811\n",
      "Epoch 100/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3386.0625 - val_loss: 3372.6721\n",
      "Epoch 101/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3367.6836 - val_loss: 3354.2595\n",
      "Epoch 102/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3349.1091 - val_loss: 3335.5923\n",
      "Epoch 103/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3330.4016 - val_loss: 3316.5310\n",
      "Epoch 104/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3311.3259 - val_loss: 3297.2952\n",
      "Epoch 105/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3291.9702 - val_loss: 3277.9883\n",
      "Epoch 106/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3272.6162 - val_loss: 3258.2556\n",
      "Epoch 107/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3252.6538 - val_loss: 3238.4072\n",
      "Epoch 108/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3232.8572 - val_loss: 3218.5945\n",
      "Epoch 109/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3213.2986 - val_loss: 3199.2578\n",
      "Epoch 110/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3193.9514 - val_loss: 3179.6389\n",
      "Epoch 111/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3174.1375 - val_loss: 3159.7485\n",
      "Epoch 112/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3154.4792 - val_loss: 3139.4338\n",
      "Epoch 113/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3134.0764 - val_loss: 3119.0298\n",
      "Epoch 114/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3113.6218 - val_loss: 3098.3455\n",
      "Epoch 115/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3092.7148 - val_loss: 3077.6992\n",
      "Epoch 116/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3072.3462 - val_loss: 3057.2126\n",
      "Epoch 117/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3051.8413 - val_loss: 3036.5940\n",
      "Epoch 118/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3031.3127 - val_loss: 3015.6470\n",
      "Epoch 119/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3010.1367 - val_loss: 2994.6409\n",
      "Epoch 120/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2989.4875 - val_loss: 2973.5264\n",
      "Epoch 121/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2968.3916 - val_loss: 2952.2744\n",
      "Epoch 122/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2947.0559 - val_loss: 2930.8477\n",
      "Epoch 123/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2925.8782 - val_loss: 2909.2390\n",
      "Epoch 124/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2904.2102 - val_loss: 2887.4314\n",
      "Epoch 125/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2882.5466 - val_loss: 2865.7000\n",
      "Epoch 126/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2860.9226 - val_loss: 2843.9634\n",
      "Epoch 127/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2839.3398 - val_loss: 2822.5557\n",
      "Epoch 128/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2817.9697 - val_loss: 2801.1299\n",
      "Epoch 129/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2796.4292 - val_loss: 2779.6938\n",
      "Epoch 130/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2775.3860 - val_loss: 2757.8240\n",
      "Epoch 131/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2753.4192 - val_loss: 2735.9500\n",
      "Epoch 132/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2731.5024 - val_loss: 2713.7749\n",
      "Epoch 133/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2709.2712 - val_loss: 2691.6213\n",
      "Epoch 134/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2687.3584 - val_loss: 2669.8474\n",
      "Epoch 135/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2665.7268 - val_loss: 2647.6399\n",
      "Epoch 136/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2643.3418 - val_loss: 2625.3821\n",
      "Epoch 137/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2621.2673 - val_loss: 2602.7703\n",
      "Epoch 138/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2598.8376 - val_loss: 2579.9568\n",
      "Epoch 139/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2575.9165 - val_loss: 2557.1426\n",
      "Epoch 140/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2553.0535 - val_loss: 2534.4912\n",
      "Epoch 141/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2531.1560 - val_loss: 2512.2371\n",
      "Epoch 142/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2508.8579 - val_loss: 2489.9661\n",
      "Epoch 143/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2486.5339 - val_loss: 2467.7107\n",
      "Epoch 144/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2464.2219 - val_loss: 2445.4788\n",
      "Epoch 145/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2442.0144 - val_loss: 2423.1570\n",
      "Epoch 146/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2420.0154 - val_loss: 2400.5203\n",
      "Epoch 147/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2397.3623 - val_loss: 2378.2478\n",
      "Epoch 148/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2375.2632 - val_loss: 2356.2305\n",
      "Epoch 149/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2353.4683 - val_loss: 2333.8464\n",
      "Epoch 150/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2331.0132 - val_loss: 2311.5361\n",
      "Epoch 151/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2308.6719 - val_loss: 2289.2158\n",
      "Epoch 152/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 2286.7207 - val_loss: 2266.6633\n",
      "Epoch 153/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2264.5061 - val_loss: 2244.4248\n",
      "Epoch 154/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2242.3328 - val_loss: 2222.2742\n",
      "Epoch 155/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2220.2002 - val_loss: 2199.9355\n",
      "Epoch 156/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2198.0303 - val_loss: 2177.3718\n",
      "Epoch 157/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2175.5215 - val_loss: 2155.0847\n",
      "Epoch 158/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2153.6836 - val_loss: 2132.8748\n",
      "Epoch 159/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2131.4604 - val_loss: 2110.5811\n",
      "Epoch 160/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2109.2666 - val_loss: 2088.1545\n",
      "Epoch 161/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2087.0742 - val_loss: 2065.5784\n",
      "Epoch 162/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2064.5742 - val_loss: 2043.0067\n",
      "Epoch 163/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2042.1637 - val_loss: 2020.3302\n",
      "Epoch 164/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2019.8130 - val_loss: 1997.5414\n",
      "Epoch 165/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1996.9540 - val_loss: 1974.9340\n",
      "Epoch 166/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1974.8713 - val_loss: 1952.7395\n",
      "Epoch 167/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1952.7480 - val_loss: 1930.4689\n",
      "Epoch 168/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1930.7179 - val_loss: 1908.1212\n",
      "Epoch 169/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1908.4316 - val_loss: 1885.8082\n",
      "Epoch 170/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1886.0632 - val_loss: 1863.4686\n",
      "Epoch 171/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1863.9941 - val_loss: 1841.0245\n",
      "Epoch 172/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1841.6332 - val_loss: 1818.6589\n",
      "Epoch 173/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1819.5233 - val_loss: 1796.2755\n",
      "Epoch 174/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1797.0101 - val_loss: 1774.0331\n",
      "Epoch 175/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1774.8928 - val_loss: 1751.8871\n",
      "Epoch 176/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1753.0874 - val_loss: 1729.5508\n",
      "Epoch 177/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1730.9989 - val_loss: 1707.2010\n",
      "Epoch 178/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1709.1012 - val_loss: 1685.1395\n",
      "Epoch 179/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1687.3500 - val_loss: 1663.3224\n",
      "Epoch 180/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1665.8170 - val_loss: 1641.4778\n",
      "Epoch 181/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1644.3962 - val_loss: 1619.5277\n",
      "Epoch 182/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1622.0129 - val_loss: 1598.0253\n",
      "Epoch 183/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 1600.6722 - val_loss: 1576.2241\n",
      "Epoch 184/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 1579.5082 - val_loss: 1554.0742\n",
      "Epoch 185/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1557.3751 - val_loss: 1532.2504\n",
      "Epoch 186/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1535.3043 - val_loss: 1510.7379\n",
      "Epoch 187/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1514.6478 - val_loss: 1489.0972\n",
      "Epoch 188/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1493.1764 - val_loss: 1467.6793\n",
      "Epoch 189/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1471.8341 - val_loss: 1446.3778\n",
      "Epoch 190/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1451.0730 - val_loss: 1424.9335\n",
      "Epoch 191/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1429.5277 - val_loss: 1403.8372\n",
      "Epoch 192/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1409.0181 - val_loss: 1382.7095\n",
      "Epoch 193/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1388.0857 - val_loss: 1361.9429\n",
      "Epoch 194/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1367.4707 - val_loss: 1341.2401\n",
      "Epoch 195/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1346.9910 - val_loss: 1320.6854\n",
      "Epoch 196/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1326.6179 - val_loss: 1300.2687\n",
      "Epoch 197/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1306.7422 - val_loss: 1279.9003\n",
      "Epoch 198/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1286.5588 - val_loss: 1259.7316\n",
      "Epoch 199/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1266.9871 - val_loss: 1239.5936\n",
      "Epoch 200/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1246.7355 - val_loss: 1219.9093\n",
      "Epoch 201/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1227.4003 - val_loss: 1200.2892\n",
      "Epoch 202/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1208.3354 - val_loss: 1180.5792\n",
      "Epoch 203/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1188.6923 - val_loss: 1161.1934\n",
      "Epoch 204/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1169.5004 - val_loss: 1141.8546\n",
      "Epoch 205/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1150.5861 - val_loss: 1122.5872\n",
      "Epoch 206/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1131.4648 - val_loss: 1103.4497\n",
      "Epoch 207/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1112.6699 - val_loss: 1084.3451\n",
      "Epoch 208/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1093.9358 - val_loss: 1065.3544\n",
      "Epoch 209/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1075.1777 - val_loss: 1046.5117\n",
      "Epoch 210/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1056.5708 - val_loss: 1027.7964\n",
      "Epoch 211/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1038.0713 - val_loss: 1009.2294\n",
      "Epoch 212/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1019.8743 - val_loss: 990.7227\n",
      "Epoch 213/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1001.5591 - val_loss: 972.4641\n",
      "Epoch 214/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 983.7334 - val_loss: 954.5839\n",
      "Epoch 215/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 966.9353 - val_loss: 936.8979\n",
      "Epoch 216/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 949.1693 - val_loss: 919.8174\n",
      "Epoch 217/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 932.5832 - val_loss: 902.7201\n",
      "Epoch 218/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 915.7961 - val_loss: 885.8160\n",
      "Epoch 219/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 899.1910 - val_loss: 869.1011\n",
      "Epoch 220/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 882.9547 - val_loss: 852.4469\n",
      "Epoch 221/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 866.8490 - val_loss: 836.0393\n",
      "Epoch 222/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 850.9029 - val_loss: 819.8582\n",
      "Epoch 223/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 835.2185 - val_loss: 803.8807\n",
      "Epoch 224/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 819.6165 - val_loss: 788.1170\n",
      "Epoch 225/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 803.8499 - val_loss: 772.7681\n",
      "Epoch 226/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 788.9523 - val_loss: 757.3643\n",
      "Epoch 227/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 774.3819 - val_loss: 741.9492\n",
      "Epoch 228/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 759.1877 - val_loss: 727.0608\n",
      "Epoch 229/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 744.6288 - val_loss: 712.3804\n",
      "Epoch 230/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 730.6901 - val_loss: 697.6817\n",
      "Epoch 231/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 716.2346 - val_loss: 683.4094\n",
      "Epoch 232/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 702.4908 - val_loss: 669.3093\n",
      "Epoch 233/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 688.6827 - val_loss: 655.4654\n",
      "Epoch 234/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 675.1598 - val_loss: 641.7652\n",
      "Epoch 235/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 661.9225 - val_loss: 628.2209\n",
      "Epoch 236/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 649.0643 - val_loss: 614.7365\n",
      "Epoch 237/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 636.0424 - val_loss: 601.5623\n",
      "Epoch 238/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 623.1445 - val_loss: 588.7789\n",
      "Epoch 239/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 610.8344 - val_loss: 576.1708\n",
      "Epoch 240/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 598.7677 - val_loss: 563.7799\n",
      "Epoch 241/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 587.1160 - val_loss: 551.5429\n",
      "Epoch 242/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 575.0814 - val_loss: 539.7595\n",
      "Epoch 243/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 563.8690 - val_loss: 528.1025\n",
      "Epoch 244/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 552.7726 - val_loss: 516.6570\n",
      "Epoch 245/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 541.7667 - val_loss: 505.4738\n",
      "Epoch 246/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 530.9266 - val_loss: 494.6319\n",
      "Epoch 247/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 520.3131 - val_loss: 484.0580\n",
      "Epoch 248/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 509.9944 - val_loss: 473.7139\n",
      "Epoch 249/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 500.3300 - val_loss: 463.3136\n",
      "Epoch 250/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 490.2812 - val_loss: 453.3072\n",
      "Epoch 251/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 480.7607 - val_loss: 443.5009\n",
      "Epoch 252/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 471.5106 - val_loss: 433.9372\n",
      "Epoch 253/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 462.5356 - val_loss: 424.5773\n",
      "Epoch 254/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 453.4328 - val_loss: 415.6286\n",
      "Epoch 255/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 445.0740 - val_loss: 406.7923\n",
      "Epoch 256/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 436.3814 - val_loss: 398.3586\n",
      "Epoch 257/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 428.7065 - val_loss: 389.8647\n",
      "Epoch 258/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 420.3706 - val_loss: 381.8019\n",
      "Epoch 259/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 412.9393 - val_loss: 373.7396\n",
      "Epoch 260/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 405.1698 - val_loss: 366.0812\n",
      "Epoch 261/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 397.9490 - val_loss: 358.5655\n",
      "Epoch 262/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 390.8542 - val_loss: 351.2614\n",
      "Epoch 263/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 383.9606 - val_loss: 344.1948\n",
      "Epoch 264/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 377.5046 - val_loss: 337.2028\n",
      "Epoch 265/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 371.0450 - val_loss: 330.4556\n",
      "Epoch 266/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 364.7915 - val_loss: 323.9409\n",
      "Epoch 267/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 358.4127 - val_loss: 317.8451\n",
      "Epoch 268/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 352.5918 - val_loss: 311.9590\n",
      "Epoch 269/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 347.1910 - val_loss: 306.1276\n",
      "Epoch 270/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 341.8652 - val_loss: 300.4558\n",
      "Epoch 271/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 336.4590 - val_loss: 295.1119\n",
      "Epoch 272/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 331.9204 - val_loss: 289.6501\n",
      "Epoch 273/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 326.6940 - val_loss: 284.6516\n",
      "Epoch 274/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 322.0059 - val_loss: 279.8683\n",
      "Epoch 275/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 317.7052 - val_loss: 275.1831\n",
      "Epoch 276/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 313.4414 - val_loss: 270.6484\n",
      "Epoch 277/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 309.0424 - val_loss: 266.4359\n",
      "Epoch 278/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 305.0478 - val_loss: 262.3789\n",
      "Epoch 279/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 301.3729 - val_loss: 258.3687\n",
      "Epoch 280/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 297.9067 - val_loss: 254.3410\n",
      "Epoch 281/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 294.3098 - val_loss: 250.5535\n",
      "Epoch 282/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 290.6842 - val_loss: 247.1209\n",
      "Epoch 283/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 287.5873 - val_loss: 243.7465\n",
      "Epoch 284/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 284.5159 - val_loss: 240.5248\n",
      "Epoch 285/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 281.6273 - val_loss: 237.4248\n",
      "Epoch 286/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 278.7213 - val_loss: 234.5332\n",
      "Epoch 287/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 276.1406 - val_loss: 231.6439\n",
      "Epoch 288/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 273.5576 - val_loss: 228.8924\n",
      "Epoch 289/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 271.1980 - val_loss: 226.2017\n",
      "Epoch 290/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 268.7660 - val_loss: 223.6902\n",
      "Epoch 291/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 266.5354 - val_loss: 221.3100\n",
      "Epoch 292/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 264.4737 - val_loss: 219.0594\n",
      "Epoch 293/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 262.3302 - val_loss: 216.9814\n",
      "Epoch 294/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 260.4872 - val_loss: 214.9077\n",
      "Epoch 295/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 258.7366 - val_loss: 212.8764\n",
      "Epoch 296/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 256.9832 - val_loss: 210.9304\n",
      "Epoch 297/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 255.3361 - val_loss: 209.0930\n",
      "Epoch 298/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 253.6880 - val_loss: 207.4155\n",
      "Epoch 299/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 252.4303 - val_loss: 205.6694\n",
      "Epoch 300/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 250.7099 - val_loss: 204.1751\n",
      "Epoch 301/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 249.5127 - val_loss: 202.6811\n",
      "Epoch 302/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 248.2202 - val_loss: 201.2854\n",
      "Epoch 303/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 246.9524 - val_loss: 199.9815\n",
      "Epoch 304/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 245.9599 - val_loss: 198.6389\n",
      "Epoch 305/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 244.7030 - val_loss: 197.4874\n",
      "Epoch 306/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 243.6651 - val_loss: 196.3586\n",
      "Epoch 307/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 242.7119 - val_loss: 195.2427\n",
      "Epoch 308/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 241.8421 - val_loss: 194.1439\n",
      "Epoch 309/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 240.9268 - val_loss: 193.1483\n",
      "Epoch 310/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 240.1133 - val_loss: 192.1930\n",
      "Epoch 311/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 239.4067 - val_loss: 191.2080\n",
      "Epoch 312/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 238.4869 - val_loss: 190.3922\n",
      "Epoch 313/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 237.8004 - val_loss: 189.6051\n",
      "Epoch 314/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 237.2108 - val_loss: 188.7827\n",
      "Epoch 315/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 236.4995 - val_loss: 188.0614\n",
      "Epoch 316/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 235.9746 - val_loss: 187.3389\n",
      "Epoch 317/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 235.3082 - val_loss: 186.7532\n",
      "Epoch 318/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 234.9182 - val_loss: 186.0865\n",
      "Epoch 319/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 234.3036 - val_loss: 185.5546\n",
      "Epoch 320/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 233.9058 - val_loss: 184.9839\n",
      "Epoch 321/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 233.4563 - val_loss: 184.4342\n",
      "Epoch 322/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 233.0153 - val_loss: 183.9371\n",
      "Epoch 323/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232.6146 - val_loss: 183.4534\n",
      "Epoch 324/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232.3136 - val_loss: 182.9388\n",
      "Epoch 325/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.9065 - val_loss: 182.5230\n",
      "Epoch 326/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 231.6434 - val_loss: 182.0889\n",
      "Epoch 327/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.2320 - val_loss: 181.7513\n",
      "Epoch 328/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.9940 - val_loss: 181.3948\n",
      "Epoch 329/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 230.7254 - val_loss: 181.0376\n",
      "Epoch 330/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.5101 - val_loss: 180.6780\n",
      "Epoch 331/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.2404 - val_loss: 180.3734\n",
      "Epoch 332/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.9600 - val_loss: 180.1638\n",
      "Epoch 333/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.7943 - val_loss: 179.8966\n",
      "Epoch 334/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.5686 - val_loss: 179.6565\n",
      "Epoch 335/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.3922 - val_loss: 179.3787\n",
      "Epoch 336/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.2144 - val_loss: 179.1155\n",
      "Epoch 337/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.9836 - val_loss: 178.8888\n",
      "Epoch 338/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.8822 - val_loss: 178.6194\n",
      "Epoch 339/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.6698 - val_loss: 178.4153\n",
      "Epoch 340/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.5111 - val_loss: 178.2341\n",
      "Epoch 341/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.4050 - val_loss: 178.0399\n",
      "Epoch 342/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.2439 - val_loss: 177.8771\n",
      "Epoch 343/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.1696 - val_loss: 177.6868\n",
      "Epoch 344/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.9879 - val_loss: 177.6087\n",
      "Epoch 345/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.9189 - val_loss: 177.4440\n",
      "Epoch 346/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.8024 - val_loss: 177.3021\n",
      "Epoch 347/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.6716 - val_loss: 177.2088\n",
      "Epoch 348/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.5921 - val_loss: 177.0736\n",
      "Epoch 349/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.5175 - val_loss: 176.9370\n",
      "Epoch 350/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.3829 - val_loss: 176.8405\n",
      "Epoch 351/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.3068 - val_loss: 176.7345\n",
      "Epoch 352/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.2266 - val_loss: 176.6610\n",
      "Epoch 353/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.1396 - val_loss: 176.5509\n",
      "Epoch 354/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.0459 - val_loss: 176.4406\n",
      "Epoch 355/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.9646 - val_loss: 176.3447\n",
      "Epoch 356/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.8792 - val_loss: 176.2573\n",
      "Epoch 357/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.8157 - val_loss: 176.1693\n",
      "Epoch 358/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.7542 - val_loss: 176.0678\n",
      "Epoch 359/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.6455 - val_loss: 176.0101\n",
      "Epoch 360/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.5740 - val_loss: 175.9270\n",
      "Epoch 361/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.5197 - val_loss: 175.8322\n",
      "Epoch 362/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.4330 - val_loss: 175.7622\n",
      "Epoch 363/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.3583 - val_loss: 175.6705\n",
      "Epoch 364/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.2870 - val_loss: 175.6006\n",
      "Epoch 365/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.2174 - val_loss: 175.5268\n",
      "Epoch 366/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.2155 - val_loss: 175.3969\n",
      "Epoch 367/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.0929 - val_loss: 175.3394\n",
      "Epoch 368/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.0367 - val_loss: 175.2929\n",
      "Epoch 369/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.9670 - val_loss: 175.2493\n",
      "Epoch 370/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.9284 - val_loss: 175.1807\n",
      "Epoch 371/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.8613 - val_loss: 175.1411\n",
      "Epoch 372/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.8268 - val_loss: 175.0698\n",
      "Epoch 373/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.7437 - val_loss: 175.0183\n",
      "Epoch 374/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.7280 - val_loss: 175.0002\n",
      "Epoch 375/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.6643 - val_loss: 174.9293\n",
      "Epoch 376/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.5943 - val_loss: 174.8648\n",
      "Epoch 377/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.5318 - val_loss: 174.8369\n",
      "Epoch 378/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.4886 - val_loss: 174.7987\n",
      "Epoch 379/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.4436 - val_loss: 174.7414\n",
      "Epoch 380/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.3885 - val_loss: 174.7040\n",
      "Epoch 381/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.3348 - val_loss: 174.6615\n",
      "Epoch 382/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.2841 - val_loss: 174.6477\n",
      "Epoch 383/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.2420 - val_loss: 174.6237\n",
      "Epoch 384/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.1810 - val_loss: 174.5952\n",
      "Epoch 385/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.1584 - val_loss: 174.5698\n",
      "Epoch 386/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.0946 - val_loss: 174.5290\n",
      "Epoch 387/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.0469 - val_loss: 174.5133\n",
      "Epoch 388/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.9969 - val_loss: 174.4779\n",
      "Epoch 389/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.9474 - val_loss: 174.4423\n",
      "Epoch 390/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.9428 - val_loss: 174.4439\n",
      "Epoch 391/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.8759 - val_loss: 174.4133\n",
      "Epoch 392/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.8286 - val_loss: 174.3721\n",
      "Epoch 393/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.7885 - val_loss: 174.3786\n",
      "Epoch 394/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.7359 - val_loss: 174.3207\n",
      "Epoch 395/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.6822 - val_loss: 174.2936\n",
      "Epoch 396/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.6272 - val_loss: 174.2672\n",
      "Epoch 397/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5863 - val_loss: 174.2407\n",
      "Epoch 398/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5592 - val_loss: 174.2424\n",
      "Epoch 399/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5062 - val_loss: 174.2228\n",
      "Epoch 400/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.4675 - val_loss: 174.1522\n",
      "Epoch 401/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.4282 - val_loss: 174.0939\n",
      "Epoch 402/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 224.3772 - val_loss: 174.0771\n",
      "Epoch 403/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3453 - val_loss: 174.0355\n",
      "Epoch 404/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2916 - val_loss: 174.0283\n",
      "Epoch 405/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2363 - val_loss: 174.0171\n",
      "Epoch 406/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2386 - val_loss: 174.0007\n",
      "Epoch 407/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1901 - val_loss: 173.9469\n",
      "Epoch 408/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1120 - val_loss: 173.9338\n",
      "Epoch 409/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 224.0854 - val_loss: 173.9022\n",
      "Epoch 410/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 224.0300 - val_loss: 173.8741\n",
      "Epoch 411/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0084 - val_loss: 173.8372\n",
      "Epoch 412/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9666 - val_loss: 173.8105\n",
      "Epoch 413/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9510 - val_loss: 173.7827\n",
      "Epoch 414/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8832 - val_loss: 173.7406\n",
      "Epoch 415/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8500 - val_loss: 173.7456\n",
      "Epoch 416/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8096 - val_loss: 173.7544\n",
      "Epoch 417/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7849 - val_loss: 173.7245\n",
      "Epoch 418/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7616 - val_loss: 173.7172\n",
      "Epoch 419/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6961 - val_loss: 173.6926\n",
      "Epoch 420/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6779 - val_loss: 173.6680\n",
      "Epoch 421/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6547 - val_loss: 173.6444\n",
      "Epoch 422/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.6412 - val_loss: 173.6067\n",
      "Epoch 423/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6003 - val_loss: 173.5802\n",
      "Epoch 424/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.5803 - val_loss: 173.5728\n",
      "Epoch 425/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 223.5224 - val_loss: 173.5881\n",
      "Epoch 426/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5164 - val_loss: 173.5596\n",
      "Epoch 427/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4639 - val_loss: 173.5733\n",
      "Epoch 428/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4395 - val_loss: 173.5696\n",
      "Epoch 429/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3955 - val_loss: 173.5657\n",
      "Epoch 430/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3996 - val_loss: 173.5482\n",
      "Epoch 431/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3415 - val_loss: 173.5339\n",
      "Epoch 432/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3052 - val_loss: 173.5367\n",
      "Epoch 433/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2799 - val_loss: 173.5248\n",
      "Epoch 434/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2849 - val_loss: 173.5064\n",
      "Epoch 435/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2330 - val_loss: 173.5259\n",
      "Epoch 436/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2106 - val_loss: 173.5083\n",
      "Epoch 437/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1868 - val_loss: 173.5204\n",
      "Epoch 438/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1847 - val_loss: 173.5378\n",
      "Epoch 439/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1116 - val_loss: 173.4896\n",
      "Epoch 440/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0767 - val_loss: 173.4635\n",
      "Epoch 441/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0630 - val_loss: 173.4729\n",
      "Epoch 442/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0384 - val_loss: 173.4736\n",
      "Epoch 443/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0143 - val_loss: 173.4630\n",
      "Epoch 444/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9975 - val_loss: 173.4570\n",
      "Epoch 445/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9844 - val_loss: 173.4422\n",
      "Epoch 446/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9539 - val_loss: 173.4262\n",
      "Epoch 447/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9327 - val_loss: 173.4224\n",
      "Epoch 448/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9051 - val_loss: 173.4084\n",
      "Epoch 449/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8789 - val_loss: 173.3853\n",
      "Epoch 450/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8777 - val_loss: 173.3582\n",
      "Epoch 451/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8599 - val_loss: 173.3220\n",
      "Epoch 452/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8404 - val_loss: 173.3550\n",
      "Epoch 453/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7984 - val_loss: 173.3640\n",
      "Epoch 454/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8419 - val_loss: 173.4034\n",
      "Epoch 455/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7633 - val_loss: 173.3824\n",
      "Epoch 456/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7449 - val_loss: 173.3783\n",
      "Epoch 457/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7273 - val_loss: 173.3924\n",
      "Epoch 458/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7311 - val_loss: 173.3829\n",
      "Epoch 459/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.6814 - val_loss: 173.3811\n",
      "Epoch 460/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.6662 - val_loss: 173.3874\n",
      "Epoch 461/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6508 - val_loss: 173.4055\n",
      "Epoch 462/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6217 - val_loss: 173.4468\n",
      "Epoch 463/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6210 - val_loss: 173.4866\n",
      "Epoch 464/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6012 - val_loss: 173.4613\n",
      "Epoch 465/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6223 - val_loss: 173.5084\n",
      "Epoch 466/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5640 - val_loss: 173.4846\n",
      "Epoch 467/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5414 - val_loss: 173.4733\n",
      "Epoch 468/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5284 - val_loss: 173.4583\n",
      "Epoch 469/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5234 - val_loss: 173.4959\n",
      "Epoch 470/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4868 - val_loss: 173.4798\n",
      "Epoch 471/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4814 - val_loss: 173.4916\n",
      "Epoch 472/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4577 - val_loss: 173.4659\n",
      "Epoch 473/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4369 - val_loss: 173.4615\n",
      "Epoch 474/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4333 - val_loss: 173.4444\n",
      "Epoch 475/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4251 - val_loss: 173.4710\n",
      "Epoch 476/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4509 - val_loss: 173.4185\n",
      "Epoch 477/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3582 - val_loss: 173.4411\n",
      "Epoch 478/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3869 - val_loss: 173.4087\n",
      "Epoch 479/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3459 - val_loss: 173.4586\n",
      "Epoch 480/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3214 - val_loss: 173.4529\n",
      "Epoch 481/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3169 - val_loss: 173.4579\n",
      "Epoch 482/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2944 - val_loss: 173.4863\n",
      "Epoch 483/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2812 - val_loss: 173.4649\n",
      "Epoch 484/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2716 - val_loss: 173.4797\n",
      "Epoch 485/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2453 - val_loss: 173.4848\n",
      "Epoch 486/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2321 - val_loss: 173.5191\n",
      "Epoch 487/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2128 - val_loss: 173.5184\n",
      "Epoch 488/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2457 - val_loss: 173.5381\n",
      "Epoch 489/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1944 - val_loss: 173.5215\n",
      "Epoch 490/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2111 - val_loss: 173.5671\n",
      "Epoch 491/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1597 - val_loss: 173.5441\n",
      "Epoch 492/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1609 - val_loss: 173.5152\n",
      "Epoch 493/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1831 - val_loss: 173.5591\n",
      "Epoch 494/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1208 - val_loss: 173.5408\n",
      "Epoch 495/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1136 - val_loss: 173.5288\n",
      "Epoch 496/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1015 - val_loss: 173.5541\n",
      "Epoch 497/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1131 - val_loss: 173.5185\n",
      "Epoch 498/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1100 - val_loss: 173.4953\n",
      "Epoch 499/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0598 - val_loss: 173.5149\n",
      "Epoch 500/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0756 - val_loss: 173.5630\n",
      "Epoch 501/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0389 - val_loss: 173.5455\n",
      "Epoch 502/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0441 - val_loss: 173.5240\n",
      "Epoch 503/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0377 - val_loss: 173.5171\n",
      "Epoch 504/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0019 - val_loss: 173.5589\n",
      "Epoch 505/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0049 - val_loss: 173.5941\n",
      "Epoch 506/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9980 - val_loss: 173.5976\n",
      "Epoch 507/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9822 - val_loss: 173.6189\n",
      "Epoch 508/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9730 - val_loss: 173.6467\n",
      "Epoch 509/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9691 - val_loss: 173.6263\n",
      "Epoch 510/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9529 - val_loss: 173.6345\n",
      "Epoch 511/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9516 - val_loss: 173.6380\n",
      "Epoch 512/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9470 - val_loss: 173.6588\n",
      "Epoch 513/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9184 - val_loss: 173.6747\n",
      "Epoch 514/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9118 - val_loss: 173.6887\n",
      "Epoch 515/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9421 - val_loss: 173.7335\n",
      "Epoch 516/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8918 - val_loss: 173.7261\n",
      "Epoch 517/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8932 - val_loss: 173.7203\n",
      "Epoch 518/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8831 - val_loss: 173.7114\n",
      "Epoch 519/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8729 - val_loss: 173.7160\n",
      "Epoch 520/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8763 - val_loss: 173.7228\n",
      "Epoch 521/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8537 - val_loss: 173.7198\n",
      "Epoch 522/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8516 - val_loss: 173.7312\n",
      "Epoch 523/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8419 - val_loss: 173.7377\n",
      "Epoch 524/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8265 - val_loss: 173.7435\n",
      "Epoch 525/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8153 - val_loss: 173.7773\n",
      "Epoch 526/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8157 - val_loss: 173.8107\n",
      "Epoch 527/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8082 - val_loss: 173.8288\n",
      "Epoch 528/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7914 - val_loss: 173.8331\n",
      "Epoch 529/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7943 - val_loss: 173.8319\n",
      "Epoch 530/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7959 - val_loss: 173.8333\n",
      "Epoch 531/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7764 - val_loss: 173.8646\n",
      "Epoch 532/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7908 - val_loss: 173.9005\n",
      "Epoch 533/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7697 - val_loss: 173.9194\n",
      "Epoch 534/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7560 - val_loss: 173.8837\n",
      "Epoch 535/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7553 - val_loss: 173.8478\n",
      "Epoch 536/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7725 - val_loss: 173.8655\n",
      "Epoch 537/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7468 - val_loss: 173.8485\n",
      "Epoch 538/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7274 - val_loss: 173.8616\n",
      "Epoch 539/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7497 - val_loss: 173.9083\n",
      "Epoch 540/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7185 - val_loss: 173.9249\n",
      "Epoch 541/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7269 - val_loss: 173.9212\n",
      "Epoch 542/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7046 - val_loss: 173.9423\n",
      "Epoch 543/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7111 - val_loss: 173.9646\n",
      "Epoch 544/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7017 - val_loss: 173.9800\n",
      "Epoch 545/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7077 - val_loss: 173.9564\n",
      "Epoch 546/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6792 - val_loss: 173.9789\n",
      "Epoch 547/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6827 - val_loss: 173.9761\n",
      "Epoch 548/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6789 - val_loss: 174.0070\n",
      "Epoch 549/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7180 - val_loss: 173.9573\n",
      "Epoch 550/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6512 - val_loss: 173.9893\n",
      "Epoch 551/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6655 - val_loss: 174.0405\n",
      "Epoch 552/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6566 - val_loss: 174.0584\n",
      "Epoch 553/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6422 - val_loss: 174.0823\n",
      "Epoch 554/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.6333 - val_loss: 174.0714\n",
      "Epoch 555/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6299 - val_loss: 174.0569\n",
      "Epoch 556/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6450 - val_loss: 174.1166\n",
      "Epoch 557/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6602 - val_loss: 174.0725\n",
      "Epoch 558/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6385 - val_loss: 174.0988\n",
      "Epoch 559/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6179 - val_loss: 174.0823\n",
      "Epoch 560/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6049 - val_loss: 174.1103\n",
      "Epoch 561/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6178 - val_loss: 174.1533\n",
      "Epoch 562/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5947 - val_loss: 174.1526\n",
      "Epoch 563/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6284 - val_loss: 174.1172\n",
      "Epoch 564/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5861 - val_loss: 174.1426\n",
      "Epoch 565/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5843 - val_loss: 174.1676\n",
      "Epoch 566/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5775 - val_loss: 174.1826\n",
      "Epoch 567/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5694 - val_loss: 174.1967\n",
      "Epoch 568/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5750 - val_loss: 174.1961\n",
      "Epoch 569/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5613 - val_loss: 174.2301\n",
      "Epoch 570/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5605 - val_loss: 174.2274\n",
      "Epoch 571/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5753 - val_loss: 174.2659\n",
      "Epoch 572/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5500 - val_loss: 174.2573\n",
      "Epoch 573/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5358 - val_loss: 174.2393\n",
      "Epoch 574/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5429 - val_loss: 174.2411\n",
      "Epoch 575/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5414 - val_loss: 174.2126\n",
      "Epoch 576/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5277 - val_loss: 174.2099\n",
      "Epoch 577/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5600 - val_loss: 174.2237\n",
      "Epoch 578/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5645 - val_loss: 174.1874\n",
      "Epoch 579/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5182 - val_loss: 174.2033\n",
      "Epoch 580/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5463 - val_loss: 174.1748\n",
      "Epoch 581/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5112 - val_loss: 174.2055\n",
      "Epoch 582/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5061 - val_loss: 174.2442\n",
      "Epoch 583/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5176 - val_loss: 174.2693\n",
      "Epoch 584/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5015 - val_loss: 174.2446\n",
      "Epoch 585/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5060 - val_loss: 174.2725\n",
      "Epoch 586/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4956 - val_loss: 174.2896\n",
      "Epoch 587/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4867 - val_loss: 174.2773\n",
      "Epoch 588/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4836 - val_loss: 174.2906\n",
      "Epoch 589/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4760 - val_loss: 174.2630\n",
      "Epoch 590/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4882 - val_loss: 174.2516\n",
      "Epoch 591/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4809 - val_loss: 174.2653\n",
      "Epoch 592/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4727 - val_loss: 174.2907\n",
      "Epoch 593/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4639 - val_loss: 174.2999\n",
      "Epoch 594/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4755 - val_loss: 174.3223\n",
      "Epoch 595/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4610 - val_loss: 174.3580\n",
      "Epoch 596/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4656 - val_loss: 174.3900\n",
      "Epoch 597/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4513 - val_loss: 174.3906\n",
      "Epoch 598/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4428 - val_loss: 174.4204\n",
      "Epoch 599/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4378 - val_loss: 174.4621\n",
      "Epoch 600/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4369 - val_loss: 174.4696\n",
      "Epoch 601/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4481 - val_loss: 174.4877\n",
      "Epoch 602/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4447 - val_loss: 174.4713\n",
      "Epoch 603/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4461 - val_loss: 174.5178\n",
      "Epoch 604/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4424 - val_loss: 174.5143\n",
      "Epoch 605/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4263 - val_loss: 174.5206\n",
      "Epoch 606/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4202 - val_loss: 174.5497\n",
      "Epoch 607/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4174 - val_loss: 174.5627\n",
      "Epoch 608/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4322 - val_loss: 174.5485\n",
      "Epoch 609/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4075 - val_loss: 174.5857\n",
      "Epoch 610/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4329 - val_loss: 174.6421\n",
      "Epoch 611/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4084 - val_loss: 174.6657\n",
      "Epoch 612/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4068 - val_loss: 174.6961\n",
      "Epoch 613/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4166 - val_loss: 174.6794\n",
      "Epoch 614/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4011 - val_loss: 174.6808\n",
      "Epoch 615/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4111 - val_loss: 174.6496\n",
      "Epoch 616/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4122 - val_loss: 174.7064\n",
      "Epoch 617/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4155 - val_loss: 174.6854\n",
      "Epoch 618/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3950 - val_loss: 174.7341\n",
      "Epoch 619/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3867 - val_loss: 174.7304\n",
      "Epoch 620/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.4215 - val_loss: 174.7752\n",
      "Epoch 621/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4395 - val_loss: 174.7333\n",
      "Epoch 622/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3798 - val_loss: 174.7452\n",
      "Epoch 623/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3726 - val_loss: 174.7337\n",
      "Epoch 624/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3765 - val_loss: 174.7204\n",
      "Epoch 625/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3645 - val_loss: 174.7405\n",
      "Epoch 626/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3618 - val_loss: 174.7561\n",
      "Epoch 627/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3660 - val_loss: 174.7867\n",
      "Epoch 628/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4519 - val_loss: 174.8421\n",
      "Epoch 629/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3507 - val_loss: 174.8227\n",
      "Epoch 630/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3645 - val_loss: 174.8009\n",
      "Epoch 631/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3477 - val_loss: 174.7802\n",
      "Epoch 632/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3551 - val_loss: 174.7995\n",
      "Epoch 633/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3917 - val_loss: 174.7689\n",
      "Epoch 634/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3607 - val_loss: 174.8092\n",
      "Epoch 635/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3380 - val_loss: 174.8315\n",
      "Epoch 636/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3638 - val_loss: 174.8513\n",
      "Epoch 637/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3321 - val_loss: 174.8352\n",
      "Epoch 638/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3448 - val_loss: 174.8587\n",
      "Epoch 639/750\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 221.3409 - val_loss: 174.8422\n",
      "Epoch 640/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 221.3405 - val_loss: 174.8577\n",
      "Epoch 641/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3355 - val_loss: 174.8306\n",
      "Epoch 642/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3247 - val_loss: 174.8331\n",
      "Epoch 643/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3218 - val_loss: 174.8712\n",
      "Epoch 644/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3296 - val_loss: 174.8990\n",
      "Epoch 645/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3361 - val_loss: 174.9055\n",
      "Epoch 646/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3232 - val_loss: 174.9415\n",
      "Epoch 647/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3600 - val_loss: 174.9822\n",
      "Epoch 648/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3443 - val_loss: 175.0119\n",
      "Epoch 649/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3208 - val_loss: 174.9819\n",
      "Epoch 650/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3123 - val_loss: 174.9805\n",
      "Epoch 651/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3466 - val_loss: 175.0208\n",
      "Epoch 652/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3203 - val_loss: 174.9921\n",
      "Epoch 653/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3100 - val_loss: 175.0004\n",
      "Epoch 654/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3054 - val_loss: 174.9964\n",
      "Epoch 655/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3230 - val_loss: 175.0043\n",
      "Epoch 656/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3242 - val_loss: 174.9885\n",
      "Epoch 657/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3050 - val_loss: 174.9993\n",
      "Epoch 658/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3109 - val_loss: 175.0271\n",
      "Epoch 659/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3091 - val_loss: 175.0500\n",
      "Epoch 660/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3000 - val_loss: 175.0649\n",
      "Epoch 661/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3110 - val_loss: 175.0503\n",
      "Epoch 662/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3797 - val_loss: 175.0139\n",
      "Epoch 663/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2911 - val_loss: 175.0643\n",
      "Epoch 664/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3028 - val_loss: 175.0982\n",
      "Epoch 665/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3004 - val_loss: 175.1128\n",
      "Epoch 666/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2981 - val_loss: 175.0951\n",
      "Epoch 667/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2964 - val_loss: 175.1348\n",
      "Epoch 668/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2871 - val_loss: 175.1402\n",
      "Epoch 669/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2899 - val_loss: 175.1542\n",
      "Epoch 670/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2865 - val_loss: 175.1556\n",
      "Epoch 671/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2991 - val_loss: 175.1396\n",
      "Epoch 672/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2804 - val_loss: 175.1526\n",
      "Epoch 673/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2891 - val_loss: 175.1562\n",
      "Epoch 674/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2849 - val_loss: 175.1636\n",
      "Epoch 675/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2799 - val_loss: 175.1616\n",
      "Epoch 676/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2811 - val_loss: 175.1620\n",
      "Epoch 677/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2844 - val_loss: 175.1508\n",
      "Epoch 678/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2769 - val_loss: 175.2022\n",
      "Epoch 679/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2846 - val_loss: 175.1973\n",
      "Epoch 680/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2692 - val_loss: 175.2027\n",
      "Epoch 681/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2679 - val_loss: 175.2363\n",
      "Epoch 682/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3158 - val_loss: 175.2222\n",
      "Epoch 683/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2784 - val_loss: 175.2486\n",
      "Epoch 684/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2648 - val_loss: 175.2322\n",
      "Epoch 685/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2645 - val_loss: 175.2508\n",
      "Epoch 686/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2901 - val_loss: 175.2875\n",
      "Epoch 687/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2685 - val_loss: 175.2773\n",
      "Epoch 688/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2672 - val_loss: 175.3139\n",
      "Epoch 689/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2686 - val_loss: 175.3274\n",
      "Epoch 690/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2641 - val_loss: 175.3060\n",
      "Epoch 691/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2555 - val_loss: 175.3202\n",
      "Epoch 692/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2612 - val_loss: 175.2908\n",
      "Epoch 693/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2645 - val_loss: 175.3247\n",
      "Epoch 694/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2599 - val_loss: 175.3150\n",
      "Epoch 695/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2476 - val_loss: 175.2982\n",
      "Epoch 696/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2621 - val_loss: 175.3030\n",
      "Epoch 697/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2670 - val_loss: 175.3102\n",
      "Epoch 698/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3144 - val_loss: 175.2726\n",
      "Epoch 699/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2692 - val_loss: 175.3172\n",
      "Epoch 700/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2544 - val_loss: 175.3530\n",
      "Epoch 701/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3049 - val_loss: 175.3300\n",
      "Epoch 702/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2485 - val_loss: 175.3467\n",
      "Epoch 703/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2807 - val_loss: 175.3849\n",
      "Epoch 704/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2435 - val_loss: 175.3776\n",
      "Epoch 705/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2431 - val_loss: 175.3849\n",
      "Epoch 706/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2566 - val_loss: 175.3951\n",
      "Epoch 707/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3006 - val_loss: 175.3468\n",
      "Epoch 708/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2411 - val_loss: 175.4039\n",
      "Epoch 709/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2461 - val_loss: 175.4393\n",
      "Epoch 710/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2735 - val_loss: 175.4913\n",
      "Epoch 711/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2327 - val_loss: 175.4835\n",
      "Epoch 712/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2360 - val_loss: 175.4637\n",
      "Epoch 713/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2388 - val_loss: 175.4823\n",
      "Epoch 714/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2471 - val_loss: 175.4883\n",
      "Epoch 715/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 221.2416 - val_loss: 175.5290\n",
      "Epoch 716/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 221.2332 - val_loss: 175.5415\n",
      "Epoch 717/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.2410 - val_loss: 175.5456\n",
      "Epoch 718/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2323 - val_loss: 175.5430\n",
      "Epoch 719/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.2369 - val_loss: 175.5421\n",
      "Epoch 720/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.2617 - val_loss: 175.5821\n",
      "Epoch 721/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2199 - val_loss: 175.5999\n",
      "Epoch 722/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.2486 - val_loss: 175.5807\n",
      "Epoch 723/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2205 - val_loss: 175.6301\n",
      "Epoch 724/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.2221 - val_loss: 175.6249\n",
      "Epoch 725/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 221.2154 - val_loss: 175.6416\n",
      "Epoch 726/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2455 - val_loss: 175.7064\n",
      "Epoch 727/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 221.2374 - val_loss: 175.7296\n",
      "Epoch 728/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2257 - val_loss: 175.7163\n",
      "Epoch 729/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2285 - val_loss: 175.7332\n",
      "Epoch 730/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2176 - val_loss: 175.7460\n",
      "Epoch 731/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2173 - val_loss: 175.7679\n",
      "Epoch 732/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2090 - val_loss: 175.7668\n",
      "Epoch 733/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2321 - val_loss: 175.7251\n",
      "Epoch 734/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2117 - val_loss: 175.7479\n",
      "Epoch 735/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2185 - val_loss: 175.7461\n",
      "Epoch 736/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2119 - val_loss: 175.7764\n",
      "Epoch 737/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2166 - val_loss: 175.7860\n",
      "Epoch 738/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.2082 - val_loss: 175.8033\n",
      "Epoch 739/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2383 - val_loss: 175.8363\n",
      "Epoch 740/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2233 - val_loss: 175.8072\n",
      "Epoch 741/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2085 - val_loss: 175.8301\n",
      "Epoch 742/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2704 - val_loss: 175.7952\n",
      "Epoch 743/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2304 - val_loss: 175.8622\n",
      "Epoch 744/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2191 - val_loss: 175.8826\n",
      "Epoch 745/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2071 - val_loss: 175.9110\n",
      "Epoch 746/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2088 - val_loss: 175.9006\n",
      "Epoch 747/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2032 - val_loss: 175.9109\n",
      "Epoch 748/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2077 - val_loss: 175.9451\n",
      "Epoch 749/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2228 - val_loss: 175.9100\n",
      "Epoch 750/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2381 - val_loss: 175.9514\n",
      ">Saved ./models/comp4948_a2_model_7.h5\n",
      "Epoch 1/750\n",
      "6/6 [==============================] - 0s 29ms/step - loss: 4535.1562 - val_loss: 4531.8203\n",
      "Epoch 2/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4528.8472 - val_loss: 4525.6401\n",
      "Epoch 3/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4522.8994 - val_loss: 4519.7832\n",
      "Epoch 4/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4517.0430 - val_loss: 4513.9097\n",
      "Epoch 5/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4511.1260 - val_loss: 4507.9824\n",
      "Epoch 6/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4505.1812 - val_loss: 4501.9502\n",
      "Epoch 7/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4499.0933 - val_loss: 4495.8730\n",
      "Epoch 8/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4493.0933 - val_loss: 4489.7612\n",
      "Epoch 9/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4486.9912 - val_loss: 4483.5410\n",
      "Epoch 10/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4480.6919 - val_loss: 4477.2114\n",
      "Epoch 11/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4474.3452 - val_loss: 4470.7305\n",
      "Epoch 12/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4467.7861 - val_loss: 4464.1450\n",
      "Epoch 13/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4461.2534 - val_loss: 4457.7407\n",
      "Epoch 14/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4454.8398 - val_loss: 4451.1953\n",
      "Epoch 15/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4448.2275 - val_loss: 4444.4390\n",
      "Epoch 16/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4441.3721 - val_loss: 4437.4932\n",
      "Epoch 17/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4434.3613 - val_loss: 4430.3276\n",
      "Epoch 18/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4427.2979 - val_loss: 4423.0972\n",
      "Epoch 19/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4419.9370 - val_loss: 4415.8975\n",
      "Epoch 20/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4412.7505 - val_loss: 4408.4790\n",
      "Epoch 21/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4405.2334 - val_loss: 4400.8271\n",
      "Epoch 22/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4397.5737 - val_loss: 4392.9492\n",
      "Epoch 23/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4389.6367 - val_loss: 4384.9019\n",
      "Epoch 24/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4381.4517 - val_loss: 4376.8354\n",
      "Epoch 25/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4373.3501 - val_loss: 4368.5923\n",
      "Epoch 26/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4365.0645 - val_loss: 4360.0864\n",
      "Epoch 27/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4356.5703 - val_loss: 4351.5171\n",
      "Epoch 28/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4347.8472 - val_loss: 4342.8877\n",
      "Epoch 29/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4339.3799 - val_loss: 4334.3496\n",
      "Epoch 30/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4330.7520 - val_loss: 4325.6172\n",
      "Epoch 31/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4322.1470 - val_loss: 4316.7798\n",
      "Epoch 32/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4313.3813 - val_loss: 4308.0171\n",
      "Epoch 33/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4304.4551 - val_loss: 4299.0396\n",
      "Epoch 34/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4295.4160 - val_loss: 4289.7817\n",
      "Epoch 35/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4286.0962 - val_loss: 4280.2520\n",
      "Epoch 36/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4276.6128 - val_loss: 4270.4966\n",
      "Epoch 37/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4266.6787 - val_loss: 4260.5205\n",
      "Epoch 38/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4256.6265 - val_loss: 4250.4839\n",
      "Epoch 39/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4246.7563 - val_loss: 4240.3960\n",
      "Epoch 40/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4236.6084 - val_loss: 4230.0425\n",
      "Epoch 41/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4226.1953 - val_loss: 4219.5200\n",
      "Epoch 42/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4215.5386 - val_loss: 4208.8936\n",
      "Epoch 43/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4205.0415 - val_loss: 4198.0239\n",
      "Epoch 44/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4194.1538 - val_loss: 4187.1313\n",
      "Epoch 45/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4183.1733 - val_loss: 4176.0942\n",
      "Epoch 46/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4172.1558 - val_loss: 4164.8867\n",
      "Epoch 47/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4160.7705 - val_loss: 4153.4395\n",
      "Epoch 48/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4149.3218 - val_loss: 4141.7231\n",
      "Epoch 49/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4137.5044 - val_loss: 4129.8770\n",
      "Epoch 50/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4125.6694 - val_loss: 4117.7910\n",
      "Epoch 51/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4113.3994 - val_loss: 4105.5796\n",
      "Epoch 52/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4101.3315 - val_loss: 4093.3252\n",
      "Epoch 53/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4089.0713 - val_loss: 4081.0000\n",
      "Epoch 54/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4076.7551 - val_loss: 4068.5334\n",
      "Epoch 55/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4064.2307 - val_loss: 4055.9460\n",
      "Epoch 56/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4051.5708 - val_loss: 4043.2275\n",
      "Epoch 57/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4038.8752 - val_loss: 4030.4451\n",
      "Epoch 58/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4026.0742 - val_loss: 4017.3423\n",
      "Epoch 59/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4012.8096 - val_loss: 4004.1267\n",
      "Epoch 60/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3999.8401 - val_loss: 3990.9507\n",
      "Epoch 61/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3986.5125 - val_loss: 3977.6008\n",
      "Epoch 62/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3973.2837 - val_loss: 3964.1470\n",
      "Epoch 63/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3959.6843 - val_loss: 3950.4014\n",
      "Epoch 64/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3945.8711 - val_loss: 3936.6157\n",
      "Epoch 65/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3932.1140 - val_loss: 3922.6201\n",
      "Epoch 66/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3918.0542 - val_loss: 3908.2571\n",
      "Epoch 67/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3903.5930 - val_loss: 3893.8726\n",
      "Epoch 68/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3889.2185 - val_loss: 3879.3518\n",
      "Epoch 69/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3874.5913 - val_loss: 3864.7429\n",
      "Epoch 70/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3860.1807 - val_loss: 3850.1917\n",
      "Epoch 71/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3845.7754 - val_loss: 3835.3467\n",
      "Epoch 72/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3830.7937 - val_loss: 3820.3948\n",
      "Epoch 73/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3815.7773 - val_loss: 3805.1123\n",
      "Epoch 74/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3800.2322 - val_loss: 3789.7600\n",
      "Epoch 75/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3785.4619 - val_loss: 3775.2734\n",
      "Epoch 76/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3770.8088 - val_loss: 3760.6125\n",
      "Epoch 77/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3756.2195 - val_loss: 3745.5618\n",
      "Epoch 78/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3741.3057 - val_loss: 3730.1609\n",
      "Epoch 79/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3725.7751 - val_loss: 3714.5583\n",
      "Epoch 80/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3710.1846 - val_loss: 3698.6455\n",
      "Epoch 81/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3694.2417 - val_loss: 3682.3850\n",
      "Epoch 82/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3677.8540 - val_loss: 3665.8406\n",
      "Epoch 83/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3661.0879 - val_loss: 3649.0916\n",
      "Epoch 84/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3644.3115 - val_loss: 3632.6016\n",
      "Epoch 85/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 3628.3994 - val_loss: 3616.2476\n",
      "Epoch 86/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3611.8298 - val_loss: 3599.7007\n",
      "Epoch 87/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3595.4080 - val_loss: 3582.7236\n",
      "Epoch 88/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3578.5579 - val_loss: 3565.4170\n",
      "Epoch 89/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3561.0500 - val_loss: 3548.2314\n",
      "Epoch 90/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3543.9705 - val_loss: 3530.6860\n",
      "Epoch 91/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3526.3093 - val_loss: 3512.9312\n",
      "Epoch 92/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3508.3176 - val_loss: 3494.8311\n",
      "Epoch 93/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3490.1042 - val_loss: 3476.4338\n",
      "Epoch 94/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3471.8818 - val_loss: 3458.2593\n",
      "Epoch 95/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3453.6309 - val_loss: 3440.0752\n",
      "Epoch 96/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3435.5310 - val_loss: 3421.7578\n",
      "Epoch 97/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3417.3896 - val_loss: 3403.6389\n",
      "Epoch 98/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3399.0520 - val_loss: 3385.3586\n",
      "Epoch 99/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3380.8752 - val_loss: 3366.8005\n",
      "Epoch 100/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 3362.1318 - val_loss: 3348.0134\n",
      "Epoch 101/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 3343.2986 - val_loss: 3328.9202\n",
      "Epoch 102/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3324.2595 - val_loss: 3309.8210\n",
      "Epoch 103/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3304.9910 - val_loss: 3290.7947\n",
      "Epoch 104/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3285.9868 - val_loss: 3271.5120\n",
      "Epoch 105/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3266.5686 - val_loss: 3251.9326\n",
      "Epoch 106/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3247.0647 - val_loss: 3232.2947\n",
      "Epoch 107/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3227.5046 - val_loss: 3212.4258\n",
      "Epoch 108/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3207.9807 - val_loss: 3192.7021\n",
      "Epoch 109/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3187.8564 - val_loss: 3173.0330\n",
      "Epoch 110/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3168.1450 - val_loss: 3152.9978\n",
      "Epoch 111/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3147.9590 - val_loss: 3132.8665\n",
      "Epoch 112/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3127.8235 - val_loss: 3112.3630\n",
      "Epoch 113/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3107.4092 - val_loss: 3091.5745\n",
      "Epoch 114/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3086.5542 - val_loss: 3070.8345\n",
      "Epoch 115/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3065.9712 - val_loss: 3050.1033\n",
      "Epoch 116/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3045.2156 - val_loss: 3029.1924\n",
      "Epoch 117/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3024.4370 - val_loss: 3008.0569\n",
      "Epoch 118/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3003.0625 - val_loss: 2987.0242\n",
      "Epoch 119/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2982.3733 - val_loss: 2966.3799\n",
      "Epoch 120/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2962.2346 - val_loss: 2946.1914\n",
      "Epoch 121/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2941.9080 - val_loss: 2925.9226\n",
      "Epoch 122/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2921.7104 - val_loss: 2905.2842\n",
      "Epoch 123/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2900.9478 - val_loss: 2884.4290\n",
      "Epoch 124/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2879.9497 - val_loss: 2863.5935\n",
      "Epoch 125/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2859.7043 - val_loss: 2843.2571\n",
      "Epoch 126/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2839.0728 - val_loss: 2822.8811\n",
      "Epoch 127/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2819.2178 - val_loss: 2801.9160\n",
      "Epoch 128/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2798.0557 - val_loss: 2781.0322\n",
      "Epoch 129/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2777.3782 - val_loss: 2759.7834\n",
      "Epoch 130/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2755.9055 - val_loss: 2738.5417\n",
      "Epoch 131/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2734.6289 - val_loss: 2717.1304\n",
      "Epoch 132/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2713.3828 - val_loss: 2695.3447\n",
      "Epoch 133/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2691.4087 - val_loss: 2673.4248\n",
      "Epoch 134/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2669.8491 - val_loss: 2651.8579\n",
      "Epoch 135/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2648.5427 - val_loss: 2630.4382\n",
      "Epoch 136/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2627.1169 - val_loss: 2608.7944\n",
      "Epoch 137/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2605.2454 - val_loss: 2586.9807\n",
      "Epoch 138/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2583.5955 - val_loss: 2564.8845\n",
      "Epoch 139/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2561.3542 - val_loss: 2542.6885\n",
      "Epoch 140/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2539.6128 - val_loss: 2520.7432\n",
      "Epoch 141/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2517.5366 - val_loss: 2499.0142\n",
      "Epoch 142/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2495.8418 - val_loss: 2476.9526\n",
      "Epoch 143/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2473.7671 - val_loss: 2454.7000\n",
      "Epoch 144/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2451.5427 - val_loss: 2432.2100\n",
      "Epoch 145/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2429.2014 - val_loss: 2409.5044\n",
      "Epoch 146/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2406.4573 - val_loss: 2386.9417\n",
      "Epoch 147/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2384.1436 - val_loss: 2364.6956\n",
      "Epoch 148/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2362.1465 - val_loss: 2342.1279\n",
      "Epoch 149/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2339.2888 - val_loss: 2319.6292\n",
      "Epoch 150/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2316.8276 - val_loss: 2296.8252\n",
      "Epoch 151/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2293.8782 - val_loss: 2274.1147\n",
      "Epoch 152/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2271.5786 - val_loss: 2251.6326\n",
      "Epoch 153/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2249.2498 - val_loss: 2229.0498\n",
      "Epoch 154/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2226.6067 - val_loss: 2206.4119\n",
      "Epoch 155/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2204.2241 - val_loss: 2183.5486\n",
      "Epoch 156/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2181.2622 - val_loss: 2160.6541\n",
      "Epoch 157/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2158.5317 - val_loss: 2137.6150\n",
      "Epoch 158/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2135.6851 - val_loss: 2114.5300\n",
      "Epoch 159/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2112.7332 - val_loss: 2091.2844\n",
      "Epoch 160/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2089.4978 - val_loss: 2067.9697\n",
      "Epoch 161/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2066.2517 - val_loss: 2044.5741\n",
      "Epoch 162/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2043.1636 - val_loss: 2021.5597\n",
      "Epoch 163/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2020.3267 - val_loss: 1998.3888\n",
      "Epoch 164/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1997.7267 - val_loss: 1976.0195\n",
      "Epoch 165/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1975.5018 - val_loss: 1953.4777\n",
      "Epoch 166/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1952.9568 - val_loss: 1931.0055\n",
      "Epoch 167/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1930.5874 - val_loss: 1908.4874\n",
      "Epoch 168/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1908.3650 - val_loss: 1885.8789\n",
      "Epoch 169/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 1885.9764 - val_loss: 1863.3335\n",
      "Epoch 170/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1863.7856 - val_loss: 1840.6051\n",
      "Epoch 171/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1840.8997 - val_loss: 1818.0410\n",
      "Epoch 172/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1818.4293 - val_loss: 1795.3143\n",
      "Epoch 173/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1795.8947 - val_loss: 1772.4269\n",
      "Epoch 174/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1773.3654 - val_loss: 1749.3988\n",
      "Epoch 175/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1750.4055 - val_loss: 1726.4688\n",
      "Epoch 176/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1728.3262 - val_loss: 1703.9432\n",
      "Epoch 177/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1705.8888 - val_loss: 1681.5939\n",
      "Epoch 178/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1683.3403 - val_loss: 1659.4786\n",
      "Epoch 179/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1661.8073 - val_loss: 1637.2523\n",
      "Epoch 180/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1639.6798 - val_loss: 1615.1743\n",
      "Epoch 181/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1617.6677 - val_loss: 1593.0929\n",
      "Epoch 182/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1595.6465 - val_loss: 1571.0312\n",
      "Epoch 183/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1574.1995 - val_loss: 1548.7316\n",
      "Epoch 184/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1552.1034 - val_loss: 1527.1062\n",
      "Epoch 185/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1530.9982 - val_loss: 1506.3293\n",
      "Epoch 186/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1510.5493 - val_loss: 1485.5724\n",
      "Epoch 187/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1490.3123 - val_loss: 1464.7123\n",
      "Epoch 188/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1469.3003 - val_loss: 1444.3262\n",
      "Epoch 189/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1448.9501 - val_loss: 1424.0012\n",
      "Epoch 190/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1429.1884 - val_loss: 1403.4231\n",
      "Epoch 191/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1408.9453 - val_loss: 1382.8368\n",
      "Epoch 192/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1388.6121 - val_loss: 1362.3175\n",
      "Epoch 193/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1368.3855 - val_loss: 1341.8450\n",
      "Epoch 194/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1348.0427 - val_loss: 1321.7040\n",
      "Epoch 195/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1328.3406 - val_loss: 1301.5929\n",
      "Epoch 196/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1308.5730 - val_loss: 1281.4706\n",
      "Epoch 197/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1288.3905 - val_loss: 1261.6460\n",
      "Epoch 198/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1269.1660 - val_loss: 1241.9274\n",
      "Epoch 199/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1249.7371 - val_loss: 1222.3992\n",
      "Epoch 200/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1230.4338 - val_loss: 1202.9399\n",
      "Epoch 201/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1211.2500 - val_loss: 1183.5493\n",
      "Epoch 202/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1192.0577 - val_loss: 1164.2900\n",
      "Epoch 203/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1172.9984 - val_loss: 1145.1614\n",
      "Epoch 204/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1154.4270 - val_loss: 1125.9601\n",
      "Epoch 205/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1135.4869 - val_loss: 1107.3605\n",
      "Epoch 206/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1117.3833 - val_loss: 1088.8263\n",
      "Epoch 207/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1099.1511 - val_loss: 1070.4347\n",
      "Epoch 208/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1080.8787 - val_loss: 1052.2570\n",
      "Epoch 209/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1062.6233 - val_loss: 1034.2599\n",
      "Epoch 210/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1045.3727 - val_loss: 1015.9479\n",
      "Epoch 211/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1027.2512 - val_loss: 997.9819\n",
      "Epoch 212/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1009.5469 - val_loss: 980.2438\n",
      "Epoch 213/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 991.9959 - val_loss: 962.6536\n",
      "Epoch 214/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 974.8715 - val_loss: 945.0052\n",
      "Epoch 215/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 957.7312 - val_loss: 927.4281\n",
      "Epoch 216/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 940.3942 - val_loss: 910.1566\n",
      "Epoch 217/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 923.3967 - val_loss: 893.1002\n",
      "Epoch 218/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 906.4901 - val_loss: 876.2119\n",
      "Epoch 219/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 889.9708 - val_loss: 859.4009\n",
      "Epoch 220/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 873.7596 - val_loss: 842.6743\n",
      "Epoch 221/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 857.5865 - val_loss: 826.1995\n",
      "Epoch 222/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 841.3389 - val_loss: 810.0361\n",
      "Epoch 223/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 825.5452 - val_loss: 794.0270\n",
      "Epoch 224/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 809.9991 - val_loss: 778.1519\n",
      "Epoch 225/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 794.3713 - val_loss: 762.5859\n",
      "Epoch 226/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 779.2027 - val_loss: 747.2621\n",
      "Epoch 227/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 764.1238 - val_loss: 732.3248\n",
      "Epoch 228/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 749.7584 - val_loss: 717.3342\n",
      "Epoch 229/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 735.2798 - val_loss: 702.5341\n",
      "Epoch 230/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 721.0313 - val_loss: 687.9130\n",
      "Epoch 231/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 706.5184 - val_loss: 673.8033\n",
      "Epoch 232/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 692.6696 - val_loss: 659.8755\n",
      "Epoch 233/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 679.3832 - val_loss: 645.9986\n",
      "Epoch 234/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 666.5336 - val_loss: 632.0750\n",
      "Epoch 235/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 652.5844 - val_loss: 618.9179\n",
      "Epoch 236/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 639.8408 - val_loss: 605.9013\n",
      "Epoch 237/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 626.9051 - val_loss: 593.1670\n",
      "Epoch 238/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 614.4976 - val_loss: 580.5268\n",
      "Epoch 239/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 602.7369 - val_loss: 567.9093\n",
      "Epoch 240/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 590.5956 - val_loss: 555.6691\n",
      "Epoch 241/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 578.7310 - val_loss: 543.7749\n",
      "Epoch 242/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 567.1437 - val_loss: 532.1994\n",
      "Epoch 243/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 556.0483 - val_loss: 520.7864\n",
      "Epoch 244/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 545.1624 - val_loss: 509.5721\n",
      "Epoch 245/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 534.3157 - val_loss: 498.6780\n",
      "Epoch 246/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 523.7782 - val_loss: 488.0277\n",
      "Epoch 247/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 513.6108 - val_loss: 477.5297\n",
      "Epoch 248/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 503.6650 - val_loss: 467.2280\n",
      "Epoch 249/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 493.7185 - val_loss: 457.2393\n",
      "Epoch 250/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 484.0200 - val_loss: 447.4814\n",
      "Epoch 251/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 474.7375 - val_loss: 437.8159\n",
      "Epoch 252/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 465.5999 - val_loss: 428.3465\n",
      "Epoch 253/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 456.5255 - val_loss: 419.1932\n",
      "Epoch 254/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 447.9449 - val_loss: 410.1997\n",
      "Epoch 255/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 439.4648 - val_loss: 401.5293\n",
      "Epoch 256/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 431.2805 - val_loss: 393.0692\n",
      "Epoch 257/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 423.1661 - val_loss: 384.9362\n",
      "Epoch 258/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 415.5461 - val_loss: 376.9423\n",
      "Epoch 259/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 408.0166 - val_loss: 369.1533\n",
      "Epoch 260/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 400.3700 - val_loss: 361.7822\n",
      "Epoch 261/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 393.3772 - val_loss: 354.4825\n",
      "Epoch 262/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 386.8267 - val_loss: 347.1604\n",
      "Epoch 263/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 379.9030 - val_loss: 340.1831\n",
      "Epoch 264/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 373.0789 - val_loss: 333.6057\n",
      "Epoch 265/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 366.9417 - val_loss: 327.1084\n",
      "Epoch 266/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 361.2093 - val_loss: 320.6249\n",
      "Epoch 267/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 354.8759 - val_loss: 314.6664\n",
      "Epoch 268/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 349.7522 - val_loss: 308.6006\n",
      "Epoch 269/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 343.6032 - val_loss: 303.0992\n",
      "Epoch 270/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 338.5954 - val_loss: 297.5832\n",
      "Epoch 271/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 333.4639 - val_loss: 292.2588\n",
      "Epoch 272/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 328.6332 - val_loss: 287.0555\n",
      "Epoch 273/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 324.0276 - val_loss: 281.9831\n",
      "Epoch 274/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 318.9778 - val_loss: 277.4170\n",
      "Epoch 275/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 314.7793 - val_loss: 272.9069\n",
      "Epoch 276/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 310.8252 - val_loss: 268.3844\n",
      "Epoch 277/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 306.6003 - val_loss: 264.0722\n",
      "Epoch 278/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 302.9102 - val_loss: 259.7897\n",
      "Epoch 279/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 298.7370 - val_loss: 255.9498\n",
      "Epoch 280/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 295.2658 - val_loss: 252.1583\n",
      "Epoch 281/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 291.8577 - val_loss: 248.4546\n",
      "Epoch 282/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 288.5451 - val_loss: 244.8848\n",
      "Epoch 283/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 285.1582 - val_loss: 241.5935\n",
      "Epoch 284/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 282.2447 - val_loss: 238.3362\n",
      "Epoch 285/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 279.2852 - val_loss: 235.1950\n",
      "Epoch 286/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 276.6090 - val_loss: 232.1435\n",
      "Epoch 287/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 273.7843 - val_loss: 229.3323\n",
      "Epoch 288/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 271.3776 - val_loss: 226.5509\n",
      "Epoch 289/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 268.9393 - val_loss: 223.9125\n",
      "Epoch 290/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 266.5711 - val_loss: 221.4218\n",
      "Epoch 291/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 264.2924 - val_loss: 219.1157\n",
      "Epoch 292/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 262.1793 - val_loss: 216.8813\n",
      "Epoch 293/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 260.3300 - val_loss: 214.6377\n",
      "Epoch 294/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 258.3218 - val_loss: 212.5737\n",
      "Epoch 295/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 256.5535 - val_loss: 210.5738\n",
      "Epoch 296/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 254.8349 - val_loss: 208.6876\n",
      "Epoch 297/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 253.0606 - val_loss: 206.9590\n",
      "Epoch 298/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 251.5365 - val_loss: 205.2425\n",
      "Epoch 299/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 250.2354 - val_loss: 203.5009\n",
      "Epoch 300/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 248.7879 - val_loss: 201.8844\n",
      "Epoch 301/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 247.2966 - val_loss: 200.4251\n",
      "Epoch 302/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 245.9772 - val_loss: 199.0632\n",
      "Epoch 303/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 244.8865 - val_loss: 197.6794\n",
      "Epoch 304/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 243.6892 - val_loss: 196.3920\n",
      "Epoch 305/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 242.6892 - val_loss: 195.1411\n",
      "Epoch 306/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 241.5379 - val_loss: 194.0360\n",
      "Epoch 307/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 240.7415 - val_loss: 192.8774\n",
      "Epoch 308/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 239.7906 - val_loss: 191.7928\n",
      "Epoch 309/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 238.9846 - val_loss: 190.7586\n",
      "Epoch 310/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 238.0273 - val_loss: 189.8782\n",
      "Epoch 311/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 237.2392 - val_loss: 189.0627\n",
      "Epoch 312/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 236.5472 - val_loss: 188.2249\n",
      "Epoch 313/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 235.9659 - val_loss: 187.3808\n",
      "Epoch 314/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 235.2551 - val_loss: 186.6267\n",
      "Epoch 315/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 234.6491 - val_loss: 185.9118\n",
      "Epoch 316/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 234.0607 - val_loss: 185.2465\n",
      "Epoch 317/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 233.5882 - val_loss: 184.5649\n",
      "Epoch 318/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 233.1074 - val_loss: 183.9205\n",
      "Epoch 319/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232.5405 - val_loss: 183.3998\n",
      "Epoch 320/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232.0751 - val_loss: 182.9108\n",
      "Epoch 321/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.8072 - val_loss: 182.2969\n",
      "Epoch 322/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 231.2715 - val_loss: 181.8616\n",
      "Epoch 323/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.9351 - val_loss: 181.3829\n",
      "Epoch 324/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 230.5755 - val_loss: 180.9470\n",
      "Epoch 325/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.2620 - val_loss: 180.4988\n",
      "Epoch 326/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 229.9206 - val_loss: 180.1037\n",
      "Epoch 327/750\n",
      "6/6 [==============================] - 0s 14ms/step - loss: 229.6162 - val_loss: 179.7143\n",
      "Epoch 328/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.3855 - val_loss: 179.3234\n",
      "Epoch 329/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.0813 - val_loss: 178.9794\n",
      "Epoch 330/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.8296 - val_loss: 178.6684\n",
      "Epoch 331/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.5642 - val_loss: 178.3803\n",
      "Epoch 332/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.3798 - val_loss: 178.0799\n",
      "Epoch 333/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.1633 - val_loss: 177.7897\n",
      "Epoch 334/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.9618 - val_loss: 177.5209\n",
      "Epoch 335/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.7729 - val_loss: 177.3025\n",
      "Epoch 336/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.6236 - val_loss: 177.0309\n",
      "Epoch 337/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.4147 - val_loss: 176.8117\n",
      "Epoch 338/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.2824 - val_loss: 176.6022\n",
      "Epoch 339/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.1110 - val_loss: 176.4128\n",
      "Epoch 340/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.9736 - val_loss: 176.2129\n",
      "Epoch 341/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.8611 - val_loss: 175.9956\n",
      "Epoch 342/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.7322 - val_loss: 175.7959\n",
      "Epoch 343/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.5638 - val_loss: 175.6486\n",
      "Epoch 344/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.4749 - val_loss: 175.4913\n",
      "Epoch 345/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.3971 - val_loss: 175.3037\n",
      "Epoch 346/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.2658 - val_loss: 175.1773\n",
      "Epoch 347/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.1588 - val_loss: 175.0435\n",
      "Epoch 348/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.0676 - val_loss: 174.9052\n",
      "Epoch 349/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.9843 - val_loss: 174.7866\n",
      "Epoch 350/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.8787 - val_loss: 174.6870\n",
      "Epoch 351/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.8438 - val_loss: 174.5575\n",
      "Epoch 352/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.7321 - val_loss: 174.4470\n",
      "Epoch 353/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.6512 - val_loss: 174.3460\n",
      "Epoch 354/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.6164 - val_loss: 174.2209\n",
      "Epoch 355/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.5387 - val_loss: 174.1216\n",
      "Epoch 356/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.4665 - val_loss: 174.0788\n",
      "Epoch 357/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.4013 - val_loss: 174.0142\n",
      "Epoch 358/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.3492 - val_loss: 173.9413\n",
      "Epoch 359/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.3164 - val_loss: 173.8345\n",
      "Epoch 360/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 225.2307 - val_loss: 173.7631\n",
      "Epoch 361/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.1928 - val_loss: 173.6996\n",
      "Epoch 362/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.1441 - val_loss: 173.6469\n",
      "Epoch 363/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.0868 - val_loss: 173.5982\n",
      "Epoch 364/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.0379 - val_loss: 173.5649\n",
      "Epoch 365/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.9983 - val_loss: 173.5462\n",
      "Epoch 366/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.9444 - val_loss: 173.4707\n",
      "Epoch 367/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.9000 - val_loss: 173.4173\n",
      "Epoch 368/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.8509 - val_loss: 173.3592\n",
      "Epoch 369/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.8083 - val_loss: 173.2944\n",
      "Epoch 370/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.7916 - val_loss: 173.2393\n",
      "Epoch 371/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.7209 - val_loss: 173.1980\n",
      "Epoch 372/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.7023 - val_loss: 173.1997\n",
      "Epoch 373/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.6636 - val_loss: 173.1488\n",
      "Epoch 374/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.6131 - val_loss: 173.1502\n",
      "Epoch 375/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5936 - val_loss: 173.0825\n",
      "Epoch 376/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.5590 - val_loss: 173.0505\n",
      "Epoch 377/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.4822 - val_loss: 173.0107\n",
      "Epoch 378/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.4559 - val_loss: 172.9766\n",
      "Epoch 379/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.4359 - val_loss: 172.9182\n",
      "Epoch 380/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.3969 - val_loss: 172.9163\n",
      "Epoch 381/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.3553 - val_loss: 172.9108\n",
      "Epoch 382/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.3256 - val_loss: 172.8665\n",
      "Epoch 383/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.2858 - val_loss: 172.8120\n",
      "Epoch 384/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.2542 - val_loss: 172.7681\n",
      "Epoch 385/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.2593 - val_loss: 172.7073\n",
      "Epoch 386/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.1947 - val_loss: 172.6985\n",
      "Epoch 387/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.1504 - val_loss: 172.7094\n",
      "Epoch 388/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 224.1182 - val_loss: 172.6833\n",
      "Epoch 389/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.1091 - val_loss: 172.7379\n",
      "Epoch 390/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.0534 - val_loss: 172.7405\n",
      "Epoch 391/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.0290 - val_loss: 172.7228\n",
      "Epoch 392/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.0014 - val_loss: 172.6843\n",
      "Epoch 393/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.9655 - val_loss: 172.6853\n",
      "Epoch 394/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.9296 - val_loss: 172.6664\n",
      "Epoch 395/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.9339 - val_loss: 172.6091\n",
      "Epoch 396/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8962 - val_loss: 172.5856\n",
      "Epoch 397/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.8687 - val_loss: 172.5614\n",
      "Epoch 398/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.8044 - val_loss: 172.5843\n",
      "Epoch 399/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.8090 - val_loss: 172.6000\n",
      "Epoch 400/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.7627 - val_loss: 172.6045\n",
      "Epoch 401/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.7456 - val_loss: 172.5853\n",
      "Epoch 402/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7145 - val_loss: 172.5378\n",
      "Epoch 403/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6745 - val_loss: 172.5375\n",
      "Epoch 404/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.6443 - val_loss: 172.5597\n",
      "Epoch 405/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6253 - val_loss: 172.5391\n",
      "Epoch 406/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5964 - val_loss: 172.5356\n",
      "Epoch 407/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5769 - val_loss: 172.5418\n",
      "Epoch 408/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5538 - val_loss: 172.5383\n",
      "Epoch 409/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.5210 - val_loss: 172.5562\n",
      "Epoch 410/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4971 - val_loss: 172.5431\n",
      "Epoch 411/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4692 - val_loss: 172.5357\n",
      "Epoch 412/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4517 - val_loss: 172.5426\n",
      "Epoch 413/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4189 - val_loss: 172.5161\n",
      "Epoch 414/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3885 - val_loss: 172.5323\n",
      "Epoch 415/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3529 - val_loss: 172.5383\n",
      "Epoch 416/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3579 - val_loss: 172.5150\n",
      "Epoch 417/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.3189 - val_loss: 172.5081\n",
      "Epoch 418/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.2830 - val_loss: 172.5156\n",
      "Epoch 419/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2887 - val_loss: 172.5415\n",
      "Epoch 420/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.2513 - val_loss: 172.5115\n",
      "Epoch 421/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.2236 - val_loss: 172.5143\n",
      "Epoch 422/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.2260 - val_loss: 172.5245\n",
      "Epoch 423/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2230 - val_loss: 172.4917\n",
      "Epoch 424/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1708 - val_loss: 172.4752\n",
      "Epoch 425/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1419 - val_loss: 172.4969\n",
      "Epoch 426/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1512 - val_loss: 172.5300\n",
      "Epoch 427/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1084 - val_loss: 172.5228\n",
      "Epoch 428/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1133 - val_loss: 172.4702\n",
      "Epoch 429/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0576 - val_loss: 172.4812\n",
      "Epoch 430/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0482 - val_loss: 172.4610\n",
      "Epoch 431/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0088 - val_loss: 172.4686\n",
      "Epoch 432/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0043 - val_loss: 172.5054\n",
      "Epoch 433/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9786 - val_loss: 172.5173\n",
      "Epoch 434/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9493 - val_loss: 172.5010\n",
      "Epoch 435/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9403 - val_loss: 172.4926\n",
      "Epoch 436/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9315 - val_loss: 172.4655\n",
      "Epoch 437/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9115 - val_loss: 172.4800\n",
      "Epoch 438/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8811 - val_loss: 172.4882\n",
      "Epoch 439/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9004 - val_loss: 172.4422\n",
      "Epoch 440/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8503 - val_loss: 172.4319\n",
      "Epoch 441/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8374 - val_loss: 172.4896\n",
      "Epoch 442/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7975 - val_loss: 172.4843\n",
      "Epoch 443/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7934 - val_loss: 172.4588\n",
      "Epoch 444/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7888 - val_loss: 172.4678\n",
      "Epoch 445/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7529 - val_loss: 172.4878\n",
      "Epoch 446/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7529 - val_loss: 172.4797\n",
      "Epoch 447/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7163 - val_loss: 172.5086\n",
      "Epoch 448/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7059 - val_loss: 172.5461\n",
      "Epoch 449/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6961 - val_loss: 172.5719\n",
      "Epoch 450/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6855 - val_loss: 172.5632\n",
      "Epoch 451/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6635 - val_loss: 172.5357\n",
      "Epoch 452/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6427 - val_loss: 172.5517\n",
      "Epoch 453/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6298 - val_loss: 172.5650\n",
      "Epoch 454/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6179 - val_loss: 172.5474\n",
      "Epoch 455/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5886 - val_loss: 172.5777\n",
      "Epoch 456/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5800 - val_loss: 172.5826\n",
      "Epoch 457/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5605 - val_loss: 172.5820\n",
      "Epoch 458/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5534 - val_loss: 172.5585\n",
      "Epoch 459/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5522 - val_loss: 172.5802\n",
      "Epoch 460/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5266 - val_loss: 172.6016\n",
      "Epoch 461/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4941 - val_loss: 172.5923\n",
      "Epoch 462/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4952 - val_loss: 172.6077\n",
      "Epoch 463/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4965 - val_loss: 172.5709\n",
      "Epoch 464/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4620 - val_loss: 172.5714\n",
      "Epoch 465/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4483 - val_loss: 172.5881\n",
      "Epoch 466/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4453 - val_loss: 172.6041\n",
      "Epoch 467/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4259 - val_loss: 172.6296\n",
      "Epoch 468/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4097 - val_loss: 172.6419\n",
      "Epoch 469/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4238 - val_loss: 172.6205\n",
      "Epoch 470/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4127 - val_loss: 172.6526\n",
      "Epoch 471/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4129 - val_loss: 172.6118\n",
      "Epoch 472/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3742 - val_loss: 172.6325\n",
      "Epoch 473/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3515 - val_loss: 172.6457\n",
      "Epoch 474/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3900 - val_loss: 172.6841\n",
      "Epoch 475/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3312 - val_loss: 172.6986\n",
      "Epoch 476/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3226 - val_loss: 172.6731\n",
      "Epoch 477/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3070 - val_loss: 172.6823\n",
      "Epoch 478/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3045 - val_loss: 172.6531\n",
      "Epoch 479/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2800 - val_loss: 172.6389\n",
      "Epoch 480/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2752 - val_loss: 172.6306\n",
      "Epoch 481/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2735 - val_loss: 172.6216\n",
      "Epoch 482/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3219 - val_loss: 172.5659\n",
      "Epoch 483/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2446 - val_loss: 172.5649\n",
      "Epoch 484/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2406 - val_loss: 172.5847\n",
      "Epoch 485/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2240 - val_loss: 172.6142\n",
      "Epoch 486/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2379 - val_loss: 172.5989\n",
      "Epoch 487/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2061 - val_loss: 172.6309\n",
      "Epoch 488/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2065 - val_loss: 172.6282\n",
      "Epoch 489/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1824 - val_loss: 172.6593\n",
      "Epoch 490/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.1897 - val_loss: 172.7184\n",
      "Epoch 491/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1815 - val_loss: 172.6889\n",
      "Epoch 492/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 222.1517 - val_loss: 172.7033\n",
      "Epoch 493/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1578 - val_loss: 172.7284\n",
      "Epoch 494/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1355 - val_loss: 172.7272\n",
      "Epoch 495/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1271 - val_loss: 172.7340\n",
      "Epoch 496/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1469 - val_loss: 172.7030\n",
      "Epoch 497/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1073 - val_loss: 172.7017\n",
      "Epoch 498/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1089 - val_loss: 172.6925\n",
      "Epoch 499/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1019 - val_loss: 172.7164\n",
      "Epoch 500/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0794 - val_loss: 172.7227\n",
      "Epoch 501/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0921 - val_loss: 172.7133\n",
      "Epoch 502/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 222.0899 - val_loss: 172.7748\n",
      "Epoch 503/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0658 - val_loss: 172.7893\n",
      "Epoch 504/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 222.0528 - val_loss: 172.7766\n",
      "Epoch 505/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0492 - val_loss: 172.7597\n",
      "Epoch 506/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0321 - val_loss: 172.7708\n",
      "Epoch 507/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0830 - val_loss: 172.7378\n",
      "Epoch 508/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0136 - val_loss: 172.7721\n",
      "Epoch 509/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0297 - val_loss: 172.8043\n",
      "Epoch 510/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0152 - val_loss: 172.7972\n",
      "Epoch 511/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0027 - val_loss: 172.8433\n",
      "Epoch 512/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9923 - val_loss: 172.8592\n",
      "Epoch 513/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9800 - val_loss: 172.8743\n",
      "Epoch 514/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9724 - val_loss: 172.8978\n",
      "Epoch 515/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9782 - val_loss: 172.9115\n",
      "Epoch 516/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9609 - val_loss: 172.9196\n",
      "Epoch 517/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9598 - val_loss: 172.9290\n",
      "Epoch 518/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9484 - val_loss: 172.9352\n",
      "Epoch 519/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9580 - val_loss: 172.9493\n",
      "Epoch 520/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9399 - val_loss: 172.9315\n",
      "Epoch 521/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9340 - val_loss: 172.9663\n",
      "Epoch 522/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9387 - val_loss: 173.0020\n",
      "Epoch 523/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9359 - val_loss: 172.9975\n",
      "Epoch 524/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9102 - val_loss: 172.9933\n",
      "Epoch 525/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9052 - val_loss: 172.9704\n",
      "Epoch 526/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9116 - val_loss: 172.9293\n",
      "Epoch 527/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9340 - val_loss: 172.9030\n",
      "Epoch 528/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8696 - val_loss: 172.9263\n",
      "Epoch 529/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8674 - val_loss: 172.9767\n",
      "Epoch 530/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9219 - val_loss: 172.9482\n",
      "Epoch 531/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8693 - val_loss: 173.0132\n",
      "Epoch 532/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8548 - val_loss: 173.0314\n",
      "Epoch 533/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.8872 - val_loss: 173.0771\n",
      "Epoch 534/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8520 - val_loss: 173.0638\n",
      "Epoch 535/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8356 - val_loss: 173.0726\n",
      "Epoch 536/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8258 - val_loss: 173.0988\n",
      "Epoch 537/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8299 - val_loss: 173.1059\n",
      "Epoch 538/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.8189 - val_loss: 173.1124\n",
      "Epoch 539/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8181 - val_loss: 173.1169\n",
      "Epoch 540/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8130 - val_loss: 173.1207\n",
      "Epoch 541/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8287 - val_loss: 173.1025\n",
      "Epoch 542/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7948 - val_loss: 173.0995\n",
      "Epoch 543/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8206 - val_loss: 173.1140\n",
      "Epoch 544/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7940 - val_loss: 173.1010\n",
      "Epoch 545/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7811 - val_loss: 173.1085\n",
      "Epoch 546/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7885 - val_loss: 173.1068\n",
      "Epoch 547/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7657 - val_loss: 173.1347\n",
      "Epoch 548/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7885 - val_loss: 173.1457\n",
      "Epoch 549/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7899 - val_loss: 173.1866\n",
      "Epoch 550/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7572 - val_loss: 173.1903\n",
      "Epoch 551/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7516 - val_loss: 173.2109\n",
      "Epoch 552/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7854 - val_loss: 173.2057\n",
      "Epoch 553/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.7610 - val_loss: 173.2706\n",
      "Epoch 554/750\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 221.7431 - val_loss: 173.2928\n",
      "Epoch 555/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 221.7377 - val_loss: 173.2800\n",
      "Epoch 556/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7349 - val_loss: 173.2681\n",
      "Epoch 557/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7330 - val_loss: 173.2787\n",
      "Epoch 558/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7201 - val_loss: 173.2898\n",
      "Epoch 559/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7119 - val_loss: 173.2996\n",
      "Epoch 560/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7103 - val_loss: 173.3208\n",
      "Epoch 561/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7115 - val_loss: 173.3561\n",
      "Epoch 562/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7546 - val_loss: 173.3811\n",
      "Epoch 563/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6980 - val_loss: 173.3547\n",
      "Epoch 564/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7070 - val_loss: 173.3275\n",
      "Epoch 565/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6901 - val_loss: 173.3236\n",
      "Epoch 566/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6942 - val_loss: 173.3267\n",
      "Epoch 567/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6983 - val_loss: 173.3581\n",
      "Epoch 568/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6834 - val_loss: 173.3639\n",
      "Epoch 569/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6782 - val_loss: 173.3724\n",
      "Epoch 570/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6651 - val_loss: 173.4017\n",
      "Epoch 571/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6751 - val_loss: 173.4604\n",
      "Epoch 572/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6599 - val_loss: 173.4706\n",
      "Epoch 573/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7150 - val_loss: 173.5097\n",
      "Epoch 574/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7004 - val_loss: 173.4877\n",
      "Epoch 575/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6508 - val_loss: 173.5075\n",
      "Epoch 576/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6484 - val_loss: 173.5052\n",
      "Epoch 577/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6474 - val_loss: 173.5063\n",
      "Epoch 578/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6783 - val_loss: 173.4869\n",
      "Epoch 579/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6412 - val_loss: 173.5290\n",
      "Epoch 580/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6341 - val_loss: 173.5452\n",
      "Epoch 581/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6315 - val_loss: 173.5816\n",
      "Epoch 582/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6354 - val_loss: 173.5889\n",
      "Epoch 583/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6317 - val_loss: 173.6442\n",
      "Epoch 584/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6362 - val_loss: 173.6491\n",
      "Epoch 585/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6214 - val_loss: 173.6380\n",
      "Epoch 586/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6183 - val_loss: 173.6418\n",
      "Epoch 587/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6268 - val_loss: 173.6618\n",
      "Epoch 588/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6172 - val_loss: 173.6780\n",
      "Epoch 589/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6010 - val_loss: 173.7078\n",
      "Epoch 590/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6019 - val_loss: 173.7191\n",
      "Epoch 591/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6024 - val_loss: 173.7244\n",
      "Epoch 592/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5965 - val_loss: 173.7022\n",
      "Epoch 593/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6120 - val_loss: 173.7399\n",
      "Epoch 594/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5898 - val_loss: 173.7408\n",
      "Epoch 595/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.6235 - val_loss: 173.7346\n",
      "Epoch 596/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5838 - val_loss: 173.7497\n",
      "Epoch 597/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5925 - val_loss: 173.7761\n",
      "Epoch 598/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5755 - val_loss: 173.7585\n",
      "Epoch 599/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5785 - val_loss: 173.7777\n",
      "Epoch 600/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5917 - val_loss: 173.7587\n",
      "Epoch 601/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5659 - val_loss: 173.7942\n",
      "Epoch 602/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5686 - val_loss: 173.8214\n",
      "Epoch 603/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5950 - val_loss: 173.7892\n",
      "Epoch 604/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5540 - val_loss: 173.8243\n",
      "Epoch 605/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5527 - val_loss: 173.8154\n",
      "Epoch 606/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5714 - val_loss: 173.8281\n",
      "Epoch 607/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.5498 - val_loss: 173.8634\n",
      "Epoch 608/750\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 221.5512 - val_loss: 173.8794\n",
      "Epoch 609/750\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 221.5534 - val_loss: 173.8557\n",
      "Epoch 610/750\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 221.5523 - val_loss: 173.8944\n",
      "Epoch 611/750\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 221.5434 - val_loss: 173.9011\n",
      "Epoch 612/750\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 221.5289 - val_loss: 173.9068\n",
      "Epoch 613/750\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 221.5437 - val_loss: 173.8783\n",
      "Epoch 614/750\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 221.5313 - val_loss: 173.9031\n",
      "Epoch 615/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.5442 - val_loss: 173.9396\n",
      "Epoch 616/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.5236 - val_loss: 173.9424\n",
      "Epoch 617/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.5223 - val_loss: 173.9821\n",
      "Epoch 618/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.5138 - val_loss: 173.9951\n",
      "Epoch 619/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5141 - val_loss: 174.0005\n",
      "Epoch 620/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5097 - val_loss: 173.9798\n",
      "Epoch 621/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5044 - val_loss: 173.9888\n",
      "Epoch 622/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5274 - val_loss: 174.0200\n",
      "Epoch 623/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5093 - val_loss: 174.0425\n",
      "Epoch 624/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5008 - val_loss: 174.0541\n",
      "Epoch 625/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5062 - val_loss: 174.0572\n",
      "Epoch 626/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5137 - val_loss: 174.0136\n",
      "Epoch 627/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4960 - val_loss: 174.0450\n",
      "Epoch 628/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4907 - val_loss: 174.0836\n",
      "Epoch 629/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4868 - val_loss: 174.0940\n",
      "Epoch 630/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4857 - val_loss: 174.1138\n",
      "Epoch 631/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4901 - val_loss: 174.1095\n",
      "Epoch 632/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4765 - val_loss: 174.1290\n",
      "Epoch 633/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4737 - val_loss: 174.1332\n",
      "Epoch 634/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4679 - val_loss: 174.1354\n",
      "Epoch 635/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4722 - val_loss: 174.1545\n",
      "Epoch 636/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4622 - val_loss: 174.1771\n",
      "Epoch 637/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4775 - val_loss: 174.2304\n",
      "Epoch 638/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5475 - val_loss: 174.3028\n",
      "Epoch 639/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4657 - val_loss: 174.2564\n",
      "Epoch 640/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4877 - val_loss: 174.2080\n",
      "Epoch 641/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4476 - val_loss: 174.2071\n",
      "Epoch 642/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4691 - val_loss: 174.1961\n",
      "Epoch 643/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4591 - val_loss: 174.2177\n",
      "Epoch 644/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4479 - val_loss: 174.2610\n",
      "Epoch 645/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4483 - val_loss: 174.3028\n",
      "Epoch 646/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4627 - val_loss: 174.3348\n",
      "Epoch 647/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4455 - val_loss: 174.3524\n",
      "Epoch 648/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4460 - val_loss: 174.3388\n",
      "Epoch 649/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4638 - val_loss: 174.3157\n",
      "Epoch 650/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4397 - val_loss: 174.3058\n",
      "Epoch 651/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4311 - val_loss: 174.3287\n",
      "Epoch 652/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4415 - val_loss: 174.3206\n",
      "Epoch 653/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4236 - val_loss: 174.3563\n",
      "Epoch 654/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4454 - val_loss: 174.4002\n",
      "Epoch 655/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4239 - val_loss: 174.3970\n",
      "Epoch 656/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4699 - val_loss: 174.4297\n",
      "Epoch 657/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4422 - val_loss: 174.4194\n",
      "Epoch 658/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4182 - val_loss: 174.4326\n",
      "Epoch 659/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4225 - val_loss: 174.4378\n",
      "Epoch 660/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4283 - val_loss: 174.4234\n",
      "Epoch 661/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4225 - val_loss: 174.4523\n",
      "Epoch 662/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.4248 - val_loss: 174.4812\n",
      "Epoch 663/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4098 - val_loss: 174.4833\n",
      "Epoch 664/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4102 - val_loss: 174.4733\n",
      "Epoch 665/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4516 - val_loss: 174.4146\n",
      "Epoch 666/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4004 - val_loss: 174.4496\n",
      "Epoch 667/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4243 - val_loss: 174.4875\n",
      "Epoch 668/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4100 - val_loss: 174.4620\n",
      "Epoch 669/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4397 - val_loss: 174.4388\n",
      "Epoch 670/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4041 - val_loss: 174.4799\n",
      "Epoch 671/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3921 - val_loss: 174.4938\n",
      "Epoch 672/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3947 - val_loss: 174.5225\n",
      "Epoch 673/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3957 - val_loss: 174.5426\n",
      "Epoch 674/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3840 - val_loss: 174.5436\n",
      "Epoch 675/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4404 - val_loss: 174.5062\n",
      "Epoch 676/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3961 - val_loss: 174.4988\n",
      "Epoch 677/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3911 - val_loss: 174.5551\n",
      "Epoch 678/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3773 - val_loss: 174.5601\n",
      "Epoch 679/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3820 - val_loss: 174.6067\n",
      "Epoch 680/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3824 - val_loss: 174.6290\n",
      "Epoch 681/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3849 - val_loss: 174.6602\n",
      "Epoch 682/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4077 - val_loss: 174.6391\n",
      "Epoch 683/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3726 - val_loss: 174.6648\n",
      "Epoch 684/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3952 - val_loss: 174.6748\n",
      "Epoch 685/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4000 - val_loss: 174.7058\n",
      "Epoch 686/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3706 - val_loss: 174.6780\n",
      "Epoch 687/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3725 - val_loss: 174.6561\n",
      "Epoch 688/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3613 - val_loss: 174.6443\n",
      "Epoch 689/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3762 - val_loss: 174.6313\n",
      "Epoch 690/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3860 - val_loss: 174.6314\n",
      "Epoch 691/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3654 - val_loss: 174.6523\n",
      "Epoch 692/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3798 - val_loss: 174.6507\n",
      "Epoch 693/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3645 - val_loss: 174.6253\n",
      "Epoch 694/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3694 - val_loss: 174.6401\n",
      "Epoch 695/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3645 - val_loss: 174.6621\n",
      "Epoch 696/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3732 - val_loss: 174.6720\n",
      "Epoch 697/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3531 - val_loss: 174.6710\n",
      "Epoch 698/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3542 - val_loss: 174.6581\n",
      "Epoch 699/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3494 - val_loss: 174.6494\n",
      "Epoch 700/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3715 - val_loss: 174.6748\n",
      "Epoch 701/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4140 - val_loss: 174.6345\n",
      "Epoch 702/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3525 - val_loss: 174.6435\n",
      "Epoch 703/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3588 - val_loss: 174.6758\n",
      "Epoch 704/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3578 - val_loss: 174.6899\n",
      "Epoch 705/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3561 - val_loss: 174.6661\n",
      "Epoch 706/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3441 - val_loss: 174.6741\n",
      "Epoch 707/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3649 - val_loss: 174.6994\n",
      "Epoch 708/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3376 - val_loss: 174.7119\n",
      "Epoch 709/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3455 - val_loss: 174.7042\n",
      "Epoch 710/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3430 - val_loss: 174.7326\n",
      "Epoch 711/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3424 - val_loss: 174.7407\n",
      "Epoch 712/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3339 - val_loss: 174.7312\n",
      "Epoch 713/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3462 - val_loss: 174.7422\n",
      "Epoch 714/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3809 - val_loss: 174.6882\n",
      "Epoch 715/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3416 - val_loss: 174.6967\n",
      "Epoch 716/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3396 - val_loss: 174.6984\n",
      "Epoch 717/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3316 - val_loss: 174.7226\n",
      "Epoch 718/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3352 - val_loss: 174.7409\n",
      "Epoch 719/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3317 - val_loss: 174.7686\n",
      "Epoch 720/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3300 - val_loss: 174.8109\n",
      "Epoch 721/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3553 - val_loss: 174.8691\n",
      "Epoch 722/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3382 - val_loss: 174.8577\n",
      "Epoch 723/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3249 - val_loss: 174.8743\n",
      "Epoch 724/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3240 - val_loss: 174.8879\n",
      "Epoch 725/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3399 - val_loss: 174.9398\n",
      "Epoch 726/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3210 - val_loss: 174.9399\n",
      "Epoch 727/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3181 - val_loss: 174.9706\n",
      "Epoch 728/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3314 - val_loss: 174.9626\n",
      "Epoch 729/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3194 - val_loss: 174.9684\n",
      "Epoch 730/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3223 - val_loss: 174.9852\n",
      "Epoch 731/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3197 - val_loss: 174.9975\n",
      "Epoch 732/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3358 - val_loss: 174.9950\n",
      "Epoch 733/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3206 - val_loss: 175.0236\n",
      "Epoch 734/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3096 - val_loss: 175.0278\n",
      "Epoch 735/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3116 - val_loss: 175.0266\n",
      "Epoch 736/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3264 - val_loss: 175.0603\n",
      "Epoch 737/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3152 - val_loss: 175.0665\n",
      "Epoch 738/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3290 - val_loss: 175.0256\n",
      "Epoch 739/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3222 - val_loss: 175.0708\n",
      "Epoch 740/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3124 - val_loss: 175.0587\n",
      "Epoch 741/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3258 - val_loss: 175.0956\n",
      "Epoch 742/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3268 - val_loss: 175.1103\n",
      "Epoch 743/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3060 - val_loss: 175.0958\n",
      "Epoch 744/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3048 - val_loss: 175.0898\n",
      "Epoch 745/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3158 - val_loss: 175.1214\n",
      "Epoch 746/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3125 - val_loss: 175.1647\n",
      "Epoch 747/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3285 - val_loss: 175.1101\n",
      "Epoch 748/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2898 - val_loss: 175.1180\n",
      "Epoch 749/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3584 - val_loss: 175.1793\n",
      "Epoch 750/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3108 - val_loss: 175.1514\n",
      ">Saved ./models/comp4948_a2_model_8.h5\n",
      "Epoch 1/750\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 4522.0444 - val_loss: 4518.7803\n",
      "Epoch 2/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4515.9336 - val_loss: 4512.7695\n",
      "Epoch 3/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4509.9414 - val_loss: 4506.6919\n",
      "Epoch 4/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4503.7920 - val_loss: 4500.5708\n",
      "Epoch 5/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4497.6924 - val_loss: 4494.3276\n",
      "Epoch 6/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4491.4268 - val_loss: 4488.0771\n",
      "Epoch 7/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4485.0918 - val_loss: 4481.7295\n",
      "Epoch 8/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4478.6606 - val_loss: 4475.2544\n",
      "Epoch 9/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4472.2104 - val_loss: 4468.6655\n",
      "Epoch 10/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4465.6411 - val_loss: 4462.0845\n",
      "Epoch 11/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 4458.9033 - val_loss: 4455.4062\n",
      "Epoch 12/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4452.3164 - val_loss: 4448.6182\n",
      "Epoch 13/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4445.4805 - val_loss: 4441.6187\n",
      "Epoch 14/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4438.3608 - val_loss: 4434.4395\n",
      "Epoch 15/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4431.1318 - val_loss: 4427.1719\n",
      "Epoch 16/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4423.7559 - val_loss: 4419.9180\n",
      "Epoch 17/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4416.4985 - val_loss: 4412.4800\n",
      "Epoch 18/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4408.9873 - val_loss: 4404.8013\n",
      "Epoch 19/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4401.2002 - val_loss: 4397.0620\n",
      "Epoch 20/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4393.5596 - val_loss: 4389.4927\n",
      "Epoch 21/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4385.9170 - val_loss: 4381.6792\n",
      "Epoch 22/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4378.1133 - val_loss: 4373.6113\n",
      "Epoch 23/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4369.9170 - val_loss: 4365.4531\n",
      "Epoch 24/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4361.7310 - val_loss: 4357.1309\n",
      "Epoch 25/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4353.3975 - val_loss: 4348.5908\n",
      "Epoch 26/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4344.7227 - val_loss: 4339.9199\n",
      "Epoch 27/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4336.1685 - val_loss: 4331.3760\n",
      "Epoch 28/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4327.4019 - val_loss: 4322.5894\n",
      "Epoch 29/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4318.5747 - val_loss: 4313.4707\n",
      "Epoch 30/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 4309.2827 - val_loss: 4304.3521\n",
      "Epoch 31/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4300.1841 - val_loss: 4295.0625\n",
      "Epoch 32/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4290.8755 - val_loss: 4285.6636\n",
      "Epoch 33/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4281.4805 - val_loss: 4276.0435\n",
      "Epoch 34/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4271.7358 - val_loss: 4266.2266\n",
      "Epoch 35/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4261.8271 - val_loss: 4256.1411\n",
      "Epoch 36/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4251.6846 - val_loss: 4246.0151\n",
      "Epoch 37/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4241.5137 - val_loss: 4235.9033\n",
      "Epoch 38/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4231.4648 - val_loss: 4225.8579\n",
      "Epoch 39/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4221.2437 - val_loss: 4215.5342\n",
      "Epoch 40/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4210.9854 - val_loss: 4204.8687\n",
      "Epoch 41/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4200.1689 - val_loss: 4193.9185\n",
      "Epoch 42/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4189.1841 - val_loss: 4182.7349\n",
      "Epoch 43/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4178.1055 - val_loss: 4171.6533\n",
      "Epoch 44/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4166.8364 - val_loss: 4160.4111\n",
      "Epoch 45/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4155.5649 - val_loss: 4148.9565\n",
      "Epoch 46/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4144.1436 - val_loss: 4137.4126\n",
      "Epoch 47/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4132.4409 - val_loss: 4125.5698\n",
      "Epoch 48/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4120.4561 - val_loss: 4113.7310\n",
      "Epoch 49/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4108.7695 - val_loss: 4101.8418\n",
      "Epoch 50/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4096.7656 - val_loss: 4089.6072\n",
      "Epoch 51/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4084.4414 - val_loss: 4077.2634\n",
      "Epoch 52/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4072.0188 - val_loss: 4064.7849\n",
      "Epoch 53/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4059.5051 - val_loss: 4052.0430\n",
      "Epoch 54/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4046.7461 - val_loss: 4038.9819\n",
      "Epoch 55/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4033.5767 - val_loss: 4025.6648\n",
      "Epoch 56/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4020.1309 - val_loss: 4012.2529\n",
      "Epoch 57/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4006.8169 - val_loss: 3998.6912\n",
      "Epoch 58/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3993.1353 - val_loss: 3984.9702\n",
      "Epoch 59/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3979.2776 - val_loss: 3971.1179\n",
      "Epoch 60/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3965.5625 - val_loss: 3957.2649\n",
      "Epoch 61/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3951.7554 - val_loss: 3943.1624\n",
      "Epoch 62/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3937.7075 - val_loss: 3928.9751\n",
      "Epoch 63/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3923.3208 - val_loss: 3914.6570\n",
      "Epoch 64/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3909.0046 - val_loss: 3900.1169\n",
      "Epoch 65/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3894.3579 - val_loss: 3885.2009\n",
      "Epoch 66/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3879.3982 - val_loss: 3870.1829\n",
      "Epoch 67/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3864.2878 - val_loss: 3855.0000\n",
      "Epoch 68/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3849.2029 - val_loss: 3839.6470\n",
      "Epoch 69/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3834.0623 - val_loss: 3824.7515\n",
      "Epoch 70/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3819.1985 - val_loss: 3809.7947\n",
      "Epoch 71/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3804.0469 - val_loss: 3794.6313\n",
      "Epoch 72/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3788.8511 - val_loss: 3779.0210\n",
      "Epoch 73/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3773.0803 - val_loss: 3763.1108\n",
      "Epoch 74/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3757.0251 - val_loss: 3746.9109\n",
      "Epoch 75/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3740.9851 - val_loss: 3730.5703\n",
      "Epoch 76/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3724.7998 - val_loss: 3714.4006\n",
      "Epoch 77/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3708.5002 - val_loss: 3698.1094\n",
      "Epoch 78/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3691.9763 - val_loss: 3681.4736\n",
      "Epoch 79/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3675.3313 - val_loss: 3664.5603\n",
      "Epoch 80/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3658.3264 - val_loss: 3647.4814\n",
      "Epoch 81/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3641.3191 - val_loss: 3630.2437\n",
      "Epoch 82/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3624.2925 - val_loss: 3612.9072\n",
      "Epoch 83/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3606.8271 - val_loss: 3595.4343\n",
      "Epoch 84/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3589.3540 - val_loss: 3577.8118\n",
      "Epoch 85/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3571.9343 - val_loss: 3560.1907\n",
      "Epoch 86/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3554.0103 - val_loss: 3542.2993\n",
      "Epoch 87/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3536.0996 - val_loss: 3523.9260\n",
      "Epoch 88/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3517.7009 - val_loss: 3505.5032\n",
      "Epoch 89/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3498.8960 - val_loss: 3487.2712\n",
      "Epoch 90/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3481.2959 - val_loss: 3469.2727\n",
      "Epoch 91/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3463.0076 - val_loss: 3451.0315\n",
      "Epoch 92/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3444.6091 - val_loss: 3432.3870\n",
      "Epoch 93/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3426.0303 - val_loss: 3413.8821\n",
      "Epoch 94/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3407.7903 - val_loss: 3395.8608\n",
      "Epoch 95/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3389.8398 - val_loss: 3377.8643\n",
      "Epoch 96/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3371.8533 - val_loss: 3359.7810\n",
      "Epoch 97/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3353.7290 - val_loss: 3341.4539\n",
      "Epoch 98/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3335.1951 - val_loss: 3322.8384\n",
      "Epoch 99/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3316.5068 - val_loss: 3303.8799\n",
      "Epoch 100/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3297.4727 - val_loss: 3284.5615\n",
      "Epoch 101/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3277.9077 - val_loss: 3265.0115\n",
      "Epoch 102/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3258.3704 - val_loss: 3245.1924\n",
      "Epoch 103/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3238.5833 - val_loss: 3225.0588\n",
      "Epoch 104/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3218.3022 - val_loss: 3204.7615\n",
      "Epoch 105/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3197.7151 - val_loss: 3184.8049\n",
      "Epoch 106/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3178.2217 - val_loss: 3165.0271\n",
      "Epoch 107/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3158.4539 - val_loss: 3144.9412\n",
      "Epoch 108/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3138.3909 - val_loss: 3124.7617\n",
      "Epoch 109/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3118.3042 - val_loss: 3104.4751\n",
      "Epoch 110/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3098.0173 - val_loss: 3084.0649\n",
      "Epoch 111/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3077.6055 - val_loss: 3063.2991\n",
      "Epoch 112/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3056.6011 - val_loss: 3042.8499\n",
      "Epoch 113/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3036.4187 - val_loss: 3022.2854\n",
      "Epoch 114/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3015.8794 - val_loss: 3001.6565\n",
      "Epoch 115/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2995.3201 - val_loss: 2980.6172\n",
      "Epoch 116/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2974.0530 - val_loss: 2959.5422\n",
      "Epoch 117/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2953.1770 - val_loss: 2938.3201\n",
      "Epoch 118/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2931.8960 - val_loss: 2917.0320\n",
      "Epoch 119/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2910.8406 - val_loss: 2895.7217\n",
      "Epoch 120/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2889.2966 - val_loss: 2874.2458\n",
      "Epoch 121/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2867.8987 - val_loss: 2852.4465\n",
      "Epoch 122/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2845.8667 - val_loss: 2830.6860\n",
      "Epoch 123/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2824.2900 - val_loss: 2808.8926\n",
      "Epoch 124/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2802.1157 - val_loss: 2787.0146\n",
      "Epoch 125/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2780.5208 - val_loss: 2764.5950\n",
      "Epoch 126/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2757.7876 - val_loss: 2742.1899\n",
      "Epoch 127/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2735.7581 - val_loss: 2720.0647\n",
      "Epoch 128/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2713.7524 - val_loss: 2697.6541\n",
      "Epoch 129/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2691.1736 - val_loss: 2675.1343\n",
      "Epoch 130/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2668.8328 - val_loss: 2652.7429\n",
      "Epoch 131/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2646.7595 - val_loss: 2630.4597\n",
      "Epoch 132/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2624.7971 - val_loss: 2608.2649\n",
      "Epoch 133/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2602.5750 - val_loss: 2585.9570\n",
      "Epoch 134/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2580.1121 - val_loss: 2563.4805\n",
      "Epoch 135/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2557.6519 - val_loss: 2540.7756\n",
      "Epoch 136/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2535.0835 - val_loss: 2518.1091\n",
      "Epoch 137/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2512.6538 - val_loss: 2495.3015\n",
      "Epoch 138/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2489.6899 - val_loss: 2472.4570\n",
      "Epoch 139/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2467.0957 - val_loss: 2449.3210\n",
      "Epoch 140/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2443.9717 - val_loss: 2426.5305\n",
      "Epoch 141/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2421.5764 - val_loss: 2404.0576\n",
      "Epoch 142/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2399.2957 - val_loss: 2381.4224\n",
      "Epoch 143/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2376.6025 - val_loss: 2358.7017\n",
      "Epoch 144/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2354.0654 - val_loss: 2336.1248\n",
      "Epoch 145/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2331.8540 - val_loss: 2313.8196\n",
      "Epoch 146/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2309.7205 - val_loss: 2291.3909\n",
      "Epoch 147/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2287.4133 - val_loss: 2268.7556\n",
      "Epoch 148/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2265.0662 - val_loss: 2245.9329\n",
      "Epoch 149/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2241.9302 - val_loss: 2223.2542\n",
      "Epoch 150/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2219.3394 - val_loss: 2200.3831\n",
      "Epoch 151/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2196.5159 - val_loss: 2177.3752\n",
      "Epoch 152/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2173.5068 - val_loss: 2154.6365\n",
      "Epoch 153/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2151.1118 - val_loss: 2132.5444\n",
      "Epoch 154/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2129.1565 - val_loss: 2110.2793\n",
      "Epoch 155/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2107.2654 - val_loss: 2087.7268\n",
      "Epoch 156/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2084.5320 - val_loss: 2065.3127\n",
      "Epoch 157/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2062.1978 - val_loss: 2042.7017\n",
      "Epoch 158/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2040.0272 - val_loss: 2019.7548\n",
      "Epoch 159/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2017.1075 - val_loss: 1996.8599\n",
      "Epoch 160/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1994.4415 - val_loss: 1973.8712\n",
      "Epoch 161/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1971.7102 - val_loss: 1950.7379\n",
      "Epoch 162/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1948.5364 - val_loss: 1927.6431\n",
      "Epoch 163/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1926.4230 - val_loss: 1905.1202\n",
      "Epoch 164/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1903.8038 - val_loss: 1882.9471\n",
      "Epoch 165/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1881.7676 - val_loss: 1860.7269\n",
      "Epoch 166/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1859.4434 - val_loss: 1838.5887\n",
      "Epoch 167/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1837.6766 - val_loss: 1816.3839\n",
      "Epoch 168/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1815.7736 - val_loss: 1794.0938\n",
      "Epoch 169/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1793.3645 - val_loss: 1771.9059\n",
      "Epoch 170/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1771.3605 - val_loss: 1749.6154\n",
      "Epoch 171/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1749.2838 - val_loss: 1727.3071\n",
      "Epoch 172/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1727.2947 - val_loss: 1704.7993\n",
      "Epoch 173/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1704.9041 - val_loss: 1682.3781\n",
      "Epoch 174/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1682.4628 - val_loss: 1659.9965\n",
      "Epoch 175/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1660.7035 - val_loss: 1637.3400\n",
      "Epoch 176/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1638.3240 - val_loss: 1614.8221\n",
      "Epoch 177/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1615.7198 - val_loss: 1592.5164\n",
      "Epoch 178/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1593.6941 - val_loss: 1570.0355\n",
      "Epoch 179/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1571.3151 - val_loss: 1547.5574\n",
      "Epoch 180/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1549.2306 - val_loss: 1525.2599\n",
      "Epoch 181/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1527.1031 - val_loss: 1502.9636\n",
      "Epoch 182/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1505.1257 - val_loss: 1480.5389\n",
      "Epoch 183/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1483.2665 - val_loss: 1458.9738\n",
      "Epoch 184/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1461.6777 - val_loss: 1437.8612\n",
      "Epoch 185/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1441.3126 - val_loss: 1416.6202\n",
      "Epoch 186/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1420.0874 - val_loss: 1395.7345\n",
      "Epoch 187/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1399.6764 - val_loss: 1374.6708\n",
      "Epoch 188/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1378.8407 - val_loss: 1353.7949\n",
      "Epoch 189/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1358.4166 - val_loss: 1332.8660\n",
      "Epoch 190/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1337.7374 - val_loss: 1312.1495\n",
      "Epoch 191/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1317.2494 - val_loss: 1291.7086\n",
      "Epoch 192/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1297.0203 - val_loss: 1271.2091\n",
      "Epoch 193/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1276.9952 - val_loss: 1250.6776\n",
      "Epoch 194/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1256.6276 - val_loss: 1230.3699\n",
      "Epoch 195/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1236.8262 - val_loss: 1209.9852\n",
      "Epoch 196/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1216.5162 - val_loss: 1190.1718\n",
      "Epoch 197/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1197.3545 - val_loss: 1170.3641\n",
      "Epoch 198/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1177.7484 - val_loss: 1150.7909\n",
      "Epoch 199/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1158.1211 - val_loss: 1131.5881\n",
      "Epoch 200/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1139.7360 - val_loss: 1112.1754\n",
      "Epoch 201/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1120.3646 - val_loss: 1093.1637\n",
      "Epoch 202/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1101.8788 - val_loss: 1074.0887\n",
      "Epoch 203/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1083.0952 - val_loss: 1055.1897\n",
      "Epoch 204/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1064.3521 - val_loss: 1036.4708\n",
      "Epoch 205/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1046.0918 - val_loss: 1017.8187\n",
      "Epoch 206/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1027.7368 - val_loss: 999.3848\n",
      "Epoch 207/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1009.6544 - val_loss: 981.0648\n",
      "Epoch 208/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 991.7429 - val_loss: 962.8391\n",
      "Epoch 209/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 973.8343 - val_loss: 944.9390\n",
      "Epoch 210/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 955.9539 - val_loss: 927.4522\n",
      "Epoch 211/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 938.7469 - val_loss: 909.9633\n",
      "Epoch 212/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 921.3376 - val_loss: 892.8484\n",
      "Epoch 213/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 904.6821 - val_loss: 875.7238\n",
      "Epoch 214/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 888.0385 - val_loss: 858.6486\n",
      "Epoch 215/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 871.2006 - val_loss: 841.9182\n",
      "Epoch 216/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 855.0178 - val_loss: 825.2363\n",
      "Epoch 217/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 838.8405 - val_loss: 808.7754\n",
      "Epoch 218/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 822.8544 - val_loss: 792.5844\n",
      "Epoch 219/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 806.8250 - val_loss: 776.7805\n",
      "Epoch 220/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 791.4348 - val_loss: 761.0606\n",
      "Epoch 221/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 776.0250 - val_loss: 745.6223\n",
      "Epoch 222/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 760.8350 - val_loss: 730.4099\n",
      "Epoch 223/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 746.3715 - val_loss: 715.1033\n",
      "Epoch 224/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 731.5853 - val_loss: 700.0723\n",
      "Epoch 225/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 716.9355 - val_loss: 685.3827\n",
      "Epoch 226/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 702.8271 - val_loss: 670.8399\n",
      "Epoch 227/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 688.4001 - val_loss: 656.7759\n",
      "Epoch 228/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 674.9919 - val_loss: 642.7202\n",
      "Epoch 229/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 661.1511 - val_loss: 629.0969\n",
      "Epoch 230/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 648.1033 - val_loss: 615.5550\n",
      "Epoch 231/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 634.9476 - val_loss: 602.3013\n",
      "Epoch 232/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 621.6782 - val_loss: 589.5107\n",
      "Epoch 233/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 609.4446 - val_loss: 576.6458\n",
      "Epoch 234/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 597.2205 - val_loss: 563.9076\n",
      "Epoch 235/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 584.8526 - val_loss: 551.5267\n",
      "Epoch 236/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 573.1682 - val_loss: 539.3140\n",
      "Epoch 237/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 561.4657 - val_loss: 527.4612\n",
      "Epoch 238/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 549.8076 - val_loss: 516.0369\n",
      "Epoch 239/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 539.1142 - val_loss: 504.6884\n",
      "Epoch 240/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 527.8747 - val_loss: 493.8292\n",
      "Epoch 241/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 517.3513 - val_loss: 483.1457\n",
      "Epoch 242/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 507.0810 - val_loss: 472.5589\n",
      "Epoch 243/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 497.0810 - val_loss: 462.0957\n",
      "Epoch 244/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 487.0534 - val_loss: 451.9548\n",
      "Epoch 245/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 477.4039 - val_loss: 442.0959\n",
      "Epoch 246/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 467.7850 - val_loss: 432.6263\n",
      "Epoch 247/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 458.7263 - val_loss: 423.3392\n",
      "Epoch 248/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 449.8812 - val_loss: 414.2518\n",
      "Epoch 249/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 441.1782 - val_loss: 405.4329\n",
      "Epoch 250/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 432.9297 - val_loss: 396.7841\n",
      "Epoch 251/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 424.9961 - val_loss: 388.3027\n",
      "Epoch 252/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 416.8321 - val_loss: 380.2843\n",
      "Epoch 253/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 409.1547 - val_loss: 372.4668\n",
      "Epoch 254/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 401.7710 - val_loss: 364.8334\n",
      "Epoch 255/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 394.4056 - val_loss: 357.4717\n",
      "Epoch 256/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 387.5661 - val_loss: 350.2666\n",
      "Epoch 257/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 380.4551 - val_loss: 343.4999\n",
      "Epoch 258/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 374.1994 - val_loss: 336.7017\n",
      "Epoch 259/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 368.1099 - val_loss: 329.9773\n",
      "Epoch 260/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 361.4632 - val_loss: 323.7523\n",
      "Epoch 261/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 355.7156 - val_loss: 317.5844\n",
      "Epoch 262/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 349.7310 - val_loss: 311.7635\n",
      "Epoch 263/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 344.4660 - val_loss: 305.9555\n",
      "Epoch 264/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 339.1087 - val_loss: 300.3698\n",
      "Epoch 265/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 333.7946 - val_loss: 295.0602\n",
      "Epoch 266/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 329.1101 - val_loss: 289.8123\n",
      "Epoch 267/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 324.0804 - val_loss: 284.9153\n",
      "Epoch 268/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 319.5763 - val_loss: 280.1668\n",
      "Epoch 269/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 315.2297 - val_loss: 275.5864\n",
      "Epoch 270/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 310.6762 - val_loss: 271.3827\n",
      "Epoch 271/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 307.0363 - val_loss: 267.0202\n",
      "Epoch 272/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 303.0234 - val_loss: 262.8882\n",
      "Epoch 273/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 299.2987 - val_loss: 258.9222\n",
      "Epoch 274/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 295.5385 - val_loss: 255.2128\n",
      "Epoch 275/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 292.3880 - val_loss: 251.5057\n",
      "Epoch 276/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 288.7992 - val_loss: 248.1297\n",
      "Epoch 277/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 285.7086 - val_loss: 244.8373\n",
      "Epoch 278/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 282.6370 - val_loss: 241.7254\n",
      "Epoch 279/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 279.9499 - val_loss: 238.5976\n",
      "Epoch 280/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 276.8539 - val_loss: 235.8287\n",
      "Epoch 281/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 274.4759 - val_loss: 232.9579\n",
      "Epoch 282/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 271.7975 - val_loss: 230.2919\n",
      "Epoch 283/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 269.6070 - val_loss: 227.5935\n",
      "Epoch 284/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 267.2542 - val_loss: 225.0414\n",
      "Epoch 285/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 264.9536 - val_loss: 222.6861\n",
      "Epoch 286/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 262.8724 - val_loss: 220.4206\n",
      "Epoch 287/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 260.6511 - val_loss: 218.3978\n",
      "Epoch 288/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 259.0491 - val_loss: 216.2505\n",
      "Epoch 289/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 257.1829 - val_loss: 214.2658\n",
      "Epoch 290/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 255.3819 - val_loss: 212.4494\n",
      "Epoch 291/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 253.8966 - val_loss: 210.6449\n",
      "Epoch 292/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 252.0623 - val_loss: 209.1343\n",
      "Epoch 293/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 250.8307 - val_loss: 207.4784\n",
      "Epoch 294/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 249.4035 - val_loss: 205.9107\n",
      "Epoch 295/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 248.1336 - val_loss: 204.3651\n",
      "Epoch 296/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 246.7992 - val_loss: 202.9539\n",
      "Epoch 297/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 245.6803 - val_loss: 201.5809\n",
      "Epoch 298/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 244.4488 - val_loss: 200.3648\n",
      "Epoch 299/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 243.3843 - val_loss: 199.1888\n",
      "Epoch 300/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 242.3362 - val_loss: 198.0750\n",
      "Epoch 301/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 241.4221 - val_loss: 196.9879\n",
      "Epoch 302/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 240.5185 - val_loss: 195.9402\n",
      "Epoch 303/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 239.6281 - val_loss: 194.9504\n",
      "Epoch 304/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 238.8176 - val_loss: 193.9988\n",
      "Epoch 305/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 238.0235 - val_loss: 193.1302\n",
      "Epoch 306/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 237.3396 - val_loss: 192.2860\n",
      "Epoch 307/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 236.6937 - val_loss: 191.4854\n",
      "Epoch 308/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 236.0522 - val_loss: 190.7395\n",
      "Epoch 309/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 235.3891 - val_loss: 190.0804\n",
      "Epoch 310/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 234.9580 - val_loss: 189.3598\n",
      "Epoch 311/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 234.3594 - val_loss: 188.7576\n",
      "Epoch 312/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 233.8850 - val_loss: 188.2005\n",
      "Epoch 313/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 233.3969 - val_loss: 187.6690\n",
      "Epoch 314/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232.9611 - val_loss: 187.1784\n",
      "Epoch 315/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 232.6105 - val_loss: 186.6659\n",
      "Epoch 316/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 232.2588 - val_loss: 186.1439\n",
      "Epoch 317/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.9227 - val_loss: 185.6523\n",
      "Epoch 318/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.4773 - val_loss: 185.2679\n",
      "Epoch 319/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.1637 - val_loss: 184.9096\n",
      "Epoch 320/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 230.9071 - val_loss: 184.5279\n",
      "Epoch 321/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.6839 - val_loss: 184.1475\n",
      "Epoch 322/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.3019 - val_loss: 183.8659\n",
      "Epoch 323/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.0405 - val_loss: 183.6031\n",
      "Epoch 324/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.8804 - val_loss: 183.2374\n",
      "Epoch 325/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.5793 - val_loss: 182.9336\n",
      "Epoch 326/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.5289 - val_loss: 182.5561\n",
      "Epoch 327/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.1362 - val_loss: 182.3342\n",
      "Epoch 328/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.9851 - val_loss: 182.0798\n",
      "Epoch 329/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.7996 - val_loss: 181.8489\n",
      "Epoch 330/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.6165 - val_loss: 181.6447\n",
      "Epoch 331/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.4383 - val_loss: 181.4642\n",
      "Epoch 332/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.3006 - val_loss: 181.2940\n",
      "Epoch 333/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.1979 - val_loss: 181.0632\n",
      "Epoch 334/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.9965 - val_loss: 180.8863\n",
      "Epoch 335/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.8914 - val_loss: 180.6901\n",
      "Epoch 336/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.7562 - val_loss: 180.5387\n",
      "Epoch 337/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.6273 - val_loss: 180.3762\n",
      "Epoch 338/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.5170 - val_loss: 180.2258\n",
      "Epoch 339/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.4233 - val_loss: 180.0540\n",
      "Epoch 340/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.2613 - val_loss: 179.9126\n",
      "Epoch 341/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.2090 - val_loss: 179.7402\n",
      "Epoch 342/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.0530 - val_loss: 179.6468\n",
      "Epoch 343/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.9748 - val_loss: 179.5257\n",
      "Epoch 344/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.9429 - val_loss: 179.3718\n",
      "Epoch 345/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.8158 - val_loss: 179.2578\n",
      "Epoch 346/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.7116 - val_loss: 179.1749\n",
      "Epoch 347/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.6377 - val_loss: 179.0878\n",
      "Epoch 348/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.5476 - val_loss: 179.0067\n",
      "Epoch 349/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.4814 - val_loss: 178.9159\n",
      "Epoch 350/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.4053 - val_loss: 178.7937\n",
      "Epoch 351/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.3471 - val_loss: 178.6925\n",
      "Epoch 352/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.2605 - val_loss: 178.5931\n",
      "Epoch 353/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.1753 - val_loss: 178.5199\n",
      "Epoch 354/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.1161 - val_loss: 178.4463\n",
      "Epoch 355/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.0635 - val_loss: 178.3438\n",
      "Epoch 356/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.9825 - val_loss: 178.2893\n",
      "Epoch 357/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.9168 - val_loss: 178.2003\n",
      "Epoch 358/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.8666 - val_loss: 178.1248\n",
      "Epoch 359/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.8021 - val_loss: 178.0408\n",
      "Epoch 360/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.7444 - val_loss: 177.9857\n",
      "Epoch 361/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.6775 - val_loss: 177.9331\n",
      "Epoch 362/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.6286 - val_loss: 177.9129\n",
      "Epoch 363/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.5851 - val_loss: 177.8243\n",
      "Epoch 364/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.5049 - val_loss: 177.7675\n",
      "Epoch 365/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.4442 - val_loss: 177.7118\n",
      "Epoch 366/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.3913 - val_loss: 177.6466\n",
      "Epoch 367/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.3269 - val_loss: 177.5934\n",
      "Epoch 368/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.2815 - val_loss: 177.5396\n",
      "Epoch 369/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.2246 - val_loss: 177.4911\n",
      "Epoch 370/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.1742 - val_loss: 177.4462\n",
      "Epoch 371/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.1385 - val_loss: 177.3842\n",
      "Epoch 372/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.0619 - val_loss: 177.3381\n",
      "Epoch 373/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.0154 - val_loss: 177.3047\n",
      "Epoch 374/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.9812 - val_loss: 177.2653\n",
      "Epoch 375/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.9335 - val_loss: 177.1927\n",
      "Epoch 376/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.8786 - val_loss: 177.1309\n",
      "Epoch 377/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.8596 - val_loss: 177.1251\n",
      "Epoch 378/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.7936 - val_loss: 177.0858\n",
      "Epoch 379/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.7348 - val_loss: 177.0505\n",
      "Epoch 380/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.6910 - val_loss: 177.0229\n",
      "Epoch 381/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.6550 - val_loss: 176.9912\n",
      "Epoch 382/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.6046 - val_loss: 176.9701\n",
      "Epoch 383/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5763 - val_loss: 176.9611\n",
      "Epoch 384/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.5383 - val_loss: 176.8885\n",
      "Epoch 385/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.4913 - val_loss: 176.8331\n",
      "Epoch 386/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.4328 - val_loss: 176.8244\n",
      "Epoch 387/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3888 - val_loss: 176.7956\n",
      "Epoch 388/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3515 - val_loss: 176.7754\n",
      "Epoch 389/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.3027 - val_loss: 176.7337\n",
      "Epoch 390/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2719 - val_loss: 176.7093\n",
      "Epoch 391/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2239 - val_loss: 176.6517\n",
      "Epoch 392/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.2001 - val_loss: 176.6121\n",
      "Epoch 393/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1392 - val_loss: 176.5735\n",
      "Epoch 394/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.1007 - val_loss: 176.5429\n",
      "Epoch 395/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0760 - val_loss: 176.5286\n",
      "Epoch 396/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0257 - val_loss: 176.5162\n",
      "Epoch 397/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9870 - val_loss: 176.5206\n",
      "Epoch 398/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9584 - val_loss: 176.4856\n",
      "Epoch 399/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9119 - val_loss: 176.4400\n",
      "Epoch 400/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8795 - val_loss: 176.4299\n",
      "Epoch 401/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8767 - val_loss: 176.3650\n",
      "Epoch 402/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8234 - val_loss: 176.3421\n",
      "Epoch 403/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7725 - val_loss: 176.3177\n",
      "Epoch 404/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.7473 - val_loss: 176.3293\n",
      "Epoch 405/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6970 - val_loss: 176.3034\n",
      "Epoch 406/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6615 - val_loss: 176.2584\n",
      "Epoch 407/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.6448 - val_loss: 176.2266\n",
      "Epoch 408/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6182 - val_loss: 176.1693\n",
      "Epoch 409/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6010 - val_loss: 176.1172\n",
      "Epoch 410/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5468 - val_loss: 176.1116\n",
      "Epoch 411/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4978 - val_loss: 176.1018\n",
      "Epoch 412/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4960 - val_loss: 176.0644\n",
      "Epoch 413/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4556 - val_loss: 176.0768\n",
      "Epoch 414/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4156 - val_loss: 176.0584\n",
      "Epoch 415/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3831 - val_loss: 176.0546\n",
      "Epoch 416/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3546 - val_loss: 176.0397\n",
      "Epoch 417/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3530 - val_loss: 176.0605\n",
      "Epoch 418/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2868 - val_loss: 176.0176\n",
      "Epoch 419/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2414 - val_loss: 175.9643\n",
      "Epoch 420/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2206 - val_loss: 175.9397\n",
      "Epoch 421/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1994 - val_loss: 175.8915\n",
      "Epoch 422/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1766 - val_loss: 175.8688\n",
      "Epoch 423/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1524 - val_loss: 175.8215\n",
      "Epoch 424/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1128 - val_loss: 175.8036\n",
      "Epoch 425/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0815 - val_loss: 175.7825\n",
      "Epoch 426/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0522 - val_loss: 175.7955\n",
      "Epoch 427/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0899 - val_loss: 175.7559\n",
      "Epoch 428/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0071 - val_loss: 175.7843\n",
      "Epoch 429/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9758 - val_loss: 175.7700\n",
      "Epoch 430/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9553 - val_loss: 175.7843\n",
      "Epoch 431/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9203 - val_loss: 175.7800\n",
      "Epoch 432/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8963 - val_loss: 175.7738\n",
      "Epoch 433/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9004 - val_loss: 175.7974\n",
      "Epoch 434/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8413 - val_loss: 175.7799\n",
      "Epoch 435/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.8396 - val_loss: 175.7659\n",
      "Epoch 436/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8131 - val_loss: 175.7360\n",
      "Epoch 437/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7922 - val_loss: 175.7222\n",
      "Epoch 438/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7668 - val_loss: 175.7255\n",
      "Epoch 439/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7578 - val_loss: 175.7048\n",
      "Epoch 440/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7226 - val_loss: 175.6940\n",
      "Epoch 441/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7377 - val_loss: 175.6680\n",
      "Epoch 442/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6970 - val_loss: 175.6728\n",
      "Epoch 443/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7433 - val_loss: 175.7177\n",
      "Epoch 444/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6564 - val_loss: 175.6995\n",
      "Epoch 445/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6570 - val_loss: 175.6882\n",
      "Epoch 446/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6400 - val_loss: 175.6813\n",
      "Epoch 447/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6015 - val_loss: 175.6979\n",
      "Epoch 448/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5828 - val_loss: 175.6905\n",
      "Epoch 449/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5684 - val_loss: 175.6818\n",
      "Epoch 450/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5494 - val_loss: 175.6704\n",
      "Epoch 451/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5275 - val_loss: 175.6808\n",
      "Epoch 452/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5244 - val_loss: 175.7053\n",
      "Epoch 453/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4984 - val_loss: 175.6956\n",
      "Epoch 454/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4829 - val_loss: 175.6683\n",
      "Epoch 455/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4703 - val_loss: 175.6667\n",
      "Epoch 456/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4487 - val_loss: 175.6422\n",
      "Epoch 457/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4251 - val_loss: 175.6275\n",
      "Epoch 458/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4090 - val_loss: 175.6192\n",
      "Epoch 459/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4075 - val_loss: 175.6121\n",
      "Epoch 460/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3956 - val_loss: 175.6192\n",
      "Epoch 461/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3696 - val_loss: 175.6164\n",
      "Epoch 462/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 222.3562 - val_loss: 175.6404\n",
      "Epoch 463/750\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 222.3457 - val_loss: 175.6800\n",
      "Epoch 464/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3252 - val_loss: 175.6673\n",
      "Epoch 465/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3127 - val_loss: 175.6655\n",
      "Epoch 466/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.3453 - val_loss: 175.6232\n",
      "Epoch 467/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.2989 - val_loss: 175.5921\n",
      "Epoch 468/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.2577 - val_loss: 175.6105\n",
      "Epoch 469/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.2557 - val_loss: 175.6300\n",
      "Epoch 470/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.2498 - val_loss: 175.6148\n",
      "Epoch 471/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2211 - val_loss: 175.6083\n",
      "Epoch 472/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 222.2035 - val_loss: 175.6036\n",
      "Epoch 473/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.1970 - val_loss: 175.6387\n",
      "Epoch 474/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1926 - val_loss: 175.6310\n",
      "Epoch 475/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1609 - val_loss: 175.6602\n",
      "Epoch 476/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.1434 - val_loss: 175.6494\n",
      "Epoch 477/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 222.1348 - val_loss: 175.6629\n",
      "Epoch 478/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1446 - val_loss: 175.7133\n",
      "Epoch 479/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.1364 - val_loss: 175.6936\n",
      "Epoch 480/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.1034 - val_loss: 175.6900\n",
      "Epoch 481/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0924 - val_loss: 175.7070\n",
      "Epoch 482/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0692 - val_loss: 175.7387\n",
      "Epoch 483/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.0547 - val_loss: 175.7591\n",
      "Epoch 484/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0483 - val_loss: 175.7630\n",
      "Epoch 485/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0588 - val_loss: 175.7590\n",
      "Epoch 486/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.0386 - val_loss: 175.7450\n",
      "Epoch 487/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0307 - val_loss: 175.7757\n",
      "Epoch 488/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0300 - val_loss: 175.7676\n",
      "Epoch 489/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.0267 - val_loss: 175.7148\n",
      "Epoch 490/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.0002 - val_loss: 175.7071\n",
      "Epoch 491/750\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 221.9912 - val_loss: 175.6974\n",
      "Epoch 492/750\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 221.9713 - val_loss: 175.6956\n",
      "Epoch 493/750\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 221.9546 - val_loss: 175.7227\n",
      "Epoch 494/750\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 221.9507 - val_loss: 175.7245\n",
      "Epoch 495/750\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 221.9417 - val_loss: 175.6930\n",
      "Epoch 496/750\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 221.9313 - val_loss: 175.6819\n",
      "Epoch 497/750\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 221.9226 - val_loss: 175.6616\n",
      "Epoch 498/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.9067 - val_loss: 175.6433\n",
      "Epoch 499/750\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 221.9234 - val_loss: 175.6806\n",
      "Epoch 500/750\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 221.9260 - val_loss: 175.6411\n",
      "Epoch 501/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8942 - val_loss: 175.6750\n",
      "Epoch 502/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.8700 - val_loss: 175.6711\n",
      "Epoch 503/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8634 - val_loss: 175.6508\n",
      "Epoch 504/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8597 - val_loss: 175.6594\n",
      "Epoch 505/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8788 - val_loss: 175.6365\n",
      "Epoch 506/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8349 - val_loss: 175.6440\n",
      "Epoch 507/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8236 - val_loss: 175.6615\n",
      "Epoch 508/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.8106 - val_loss: 175.6644\n",
      "Epoch 509/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8129 - val_loss: 175.6536\n",
      "Epoch 510/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7943 - val_loss: 175.6607\n",
      "Epoch 511/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7869 - val_loss: 175.6616\n",
      "Epoch 512/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7927 - val_loss: 175.6428\n",
      "Epoch 513/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7791 - val_loss: 175.6374\n",
      "Epoch 514/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7996 - val_loss: 175.6802\n",
      "Epoch 515/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7516 - val_loss: 175.6675\n",
      "Epoch 516/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7533 - val_loss: 175.6744\n",
      "Epoch 517/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7527 - val_loss: 175.6491\n",
      "Epoch 518/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7472 - val_loss: 175.6331\n",
      "Epoch 519/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7261 - val_loss: 175.6281\n",
      "Epoch 520/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7316 - val_loss: 175.6402\n",
      "Epoch 521/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7108 - val_loss: 175.6251\n",
      "Epoch 522/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7421 - val_loss: 175.5901\n",
      "Epoch 523/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7090 - val_loss: 175.6011\n",
      "Epoch 524/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7285 - val_loss: 175.6254\n",
      "Epoch 525/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.7041 - val_loss: 175.6205\n",
      "Epoch 526/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6866 - val_loss: 175.6276\n",
      "Epoch 527/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6781 - val_loss: 175.6303\n",
      "Epoch 528/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6786 - val_loss: 175.6528\n",
      "Epoch 529/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6919 - val_loss: 175.6291\n",
      "Epoch 530/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6621 - val_loss: 175.6482\n",
      "Epoch 531/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6619 - val_loss: 175.6596\n",
      "Epoch 532/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6518 - val_loss: 175.6569\n",
      "Epoch 533/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6538 - val_loss: 175.6678\n",
      "Epoch 534/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6420 - val_loss: 175.6644\n",
      "Epoch 535/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6474 - val_loss: 175.6740\n",
      "Epoch 536/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6565 - val_loss: 175.6503\n",
      "Epoch 537/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6294 - val_loss: 175.6475\n",
      "Epoch 538/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6342 - val_loss: 175.6353\n",
      "Epoch 539/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6759 - val_loss: 175.6691\n",
      "Epoch 540/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6055 - val_loss: 175.6455\n",
      "Epoch 541/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6026 - val_loss: 175.6277\n",
      "Epoch 542/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6129 - val_loss: 175.5965\n",
      "Epoch 543/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6192 - val_loss: 175.5812\n",
      "Epoch 544/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5885 - val_loss: 175.5687\n",
      "Epoch 545/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6497 - val_loss: 175.5995\n",
      "Epoch 546/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6057 - val_loss: 175.5669\n",
      "Epoch 547/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5917 - val_loss: 175.5364\n",
      "Epoch 548/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5942 - val_loss: 175.5616\n",
      "Epoch 549/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5760 - val_loss: 175.5612\n",
      "Epoch 550/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5789 - val_loss: 175.5639\n",
      "Epoch 551/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6029 - val_loss: 175.6302\n",
      "Epoch 552/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5623 - val_loss: 175.5893\n",
      "Epoch 553/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5547 - val_loss: 175.5941\n",
      "Epoch 554/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5459 - val_loss: 175.6100\n",
      "Epoch 555/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5541 - val_loss: 175.6048\n",
      "Epoch 556/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5389 - val_loss: 175.6334\n",
      "Epoch 557/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5302 - val_loss: 175.6318\n",
      "Epoch 558/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5380 - val_loss: 175.6284\n",
      "Epoch 559/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5432 - val_loss: 175.6477\n",
      "Epoch 560/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5153 - val_loss: 175.6381\n",
      "Epoch 561/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5158 - val_loss: 175.6373\n",
      "Epoch 562/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5162 - val_loss: 175.6256\n",
      "Epoch 563/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5142 - val_loss: 175.6106\n",
      "Epoch 564/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5150 - val_loss: 175.5991\n",
      "Epoch 565/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4932 - val_loss: 175.6334\n",
      "Epoch 566/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5209 - val_loss: 175.6636\n",
      "Epoch 567/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4802 - val_loss: 175.6550\n",
      "Epoch 568/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5000 - val_loss: 175.6231\n",
      "Epoch 569/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4755 - val_loss: 175.6115\n",
      "Epoch 570/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4738 - val_loss: 175.6123\n",
      "Epoch 571/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5112 - val_loss: 175.6513\n",
      "Epoch 572/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4625 - val_loss: 175.6293\n",
      "Epoch 573/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4678 - val_loss: 175.6351\n",
      "Epoch 574/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4817 - val_loss: 175.6112\n",
      "Epoch 575/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4571 - val_loss: 175.6339\n",
      "Epoch 576/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4971 - val_loss: 175.6819\n",
      "Epoch 577/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4575 - val_loss: 175.6567\n",
      "Epoch 578/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4699 - val_loss: 175.6532\n",
      "Epoch 579/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4554 - val_loss: 175.6554\n",
      "Epoch 580/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4432 - val_loss: 175.6942\n",
      "Epoch 581/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4329 - val_loss: 175.7159\n",
      "Epoch 582/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4444 - val_loss: 175.7126\n",
      "Epoch 583/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.4441 - val_loss: 175.7495\n",
      "Epoch 584/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4244 - val_loss: 175.7856\n",
      "Epoch 585/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4196 - val_loss: 175.7971\n",
      "Epoch 586/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4176 - val_loss: 175.8061\n",
      "Epoch 587/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4221 - val_loss: 175.7965\n",
      "Epoch 588/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4027 - val_loss: 175.7975\n",
      "Epoch 589/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4250 - val_loss: 175.7992\n",
      "Epoch 590/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3973 - val_loss: 175.7942\n",
      "Epoch 591/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3980 - val_loss: 175.7982\n",
      "Epoch 592/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3944 - val_loss: 175.8091\n",
      "Epoch 593/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.4269 - val_loss: 175.8462\n",
      "Epoch 594/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3844 - val_loss: 175.8427\n",
      "Epoch 595/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.4413 - val_loss: 175.7961\n",
      "Epoch 596/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3928 - val_loss: 175.8345\n",
      "Epoch 597/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4060 - val_loss: 175.8154\n",
      "Epoch 598/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3784 - val_loss: 175.8230\n",
      "Epoch 599/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3809 - val_loss: 175.8415\n",
      "Epoch 600/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3726 - val_loss: 175.8355\n",
      "Epoch 601/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3794 - val_loss: 175.8266\n",
      "Epoch 602/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3981 - val_loss: 175.8549\n",
      "Epoch 603/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3997 - val_loss: 175.8155\n",
      "Epoch 604/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3849 - val_loss: 175.8627\n",
      "Epoch 605/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3499 - val_loss: 175.8397\n",
      "Epoch 606/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3533 - val_loss: 175.8453\n",
      "Epoch 607/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3737 - val_loss: 175.8661\n",
      "Epoch 608/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3498 - val_loss: 175.8511\n",
      "Epoch 609/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3424 - val_loss: 175.8254\n",
      "Epoch 610/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3771 - val_loss: 175.8568\n",
      "Epoch 611/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3726 - val_loss: 175.8435\n",
      "Epoch 612/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3308 - val_loss: 175.8708\n",
      "Epoch 613/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3436 - val_loss: 175.9013\n",
      "Epoch 614/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3298 - val_loss: 175.9290\n",
      "Epoch 615/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3249 - val_loss: 175.9300\n",
      "Epoch 616/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3495 - val_loss: 175.9560\n",
      "Epoch 617/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3256 - val_loss: 175.9612\n",
      "Epoch 618/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3280 - val_loss: 175.9590\n",
      "Epoch 619/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3178 - val_loss: 175.9547\n",
      "Epoch 620/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3214 - val_loss: 175.9341\n",
      "Epoch 621/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3158 - val_loss: 175.9482\n",
      "Epoch 622/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3069 - val_loss: 175.9445\n",
      "Epoch 623/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3073 - val_loss: 175.9447\n",
      "Epoch 624/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3240 - val_loss: 175.9361\n",
      "Epoch 625/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3017 - val_loss: 175.9341\n",
      "Epoch 626/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3117 - val_loss: 175.9135\n",
      "Epoch 627/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3621 - val_loss: 175.9790\n",
      "Epoch 628/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3417 - val_loss: 176.0238\n",
      "Epoch 629/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2922 - val_loss: 176.0249\n",
      "Epoch 630/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3186 - val_loss: 176.0586\n",
      "Epoch 631/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3175 - val_loss: 176.0199\n",
      "Epoch 632/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2833 - val_loss: 176.0095\n",
      "Epoch 633/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2873 - val_loss: 175.9921\n",
      "Epoch 634/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2961 - val_loss: 175.9776\n",
      "Epoch 635/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3188 - val_loss: 175.9395\n",
      "Epoch 636/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2850 - val_loss: 175.9459\n",
      "Epoch 637/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2867 - val_loss: 175.9385\n",
      "Epoch 638/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2883 - val_loss: 175.9121\n",
      "Epoch 639/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2942 - val_loss: 175.9406\n",
      "Epoch 640/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3341 - val_loss: 175.9844\n",
      "Epoch 641/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2805 - val_loss: 175.9636\n",
      "Epoch 642/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2764 - val_loss: 175.9351\n",
      "Epoch 643/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2687 - val_loss: 175.9395\n",
      "Epoch 644/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2956 - val_loss: 175.9806\n",
      "Epoch 645/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2664 - val_loss: 175.9685\n",
      "Epoch 646/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2989 - val_loss: 175.9421\n",
      "Epoch 647/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2896 - val_loss: 175.9987\n",
      "Epoch 648/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2659 - val_loss: 176.0098\n",
      "Epoch 649/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2716 - val_loss: 176.0038\n",
      "Epoch 650/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2649 - val_loss: 175.9961\n",
      "Epoch 651/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2618 - val_loss: 176.0268\n",
      "Epoch 652/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2845 - val_loss: 176.0657\n",
      "Epoch 653/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2818 - val_loss: 176.0501\n",
      "Epoch 654/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2942 - val_loss: 176.0332\n",
      "Epoch 655/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2825 - val_loss: 176.0273\n",
      "Epoch 656/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2554 - val_loss: 176.0685\n",
      "Epoch 657/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2678 - val_loss: 176.1142\n",
      "Epoch 658/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2597 - val_loss: 176.1030\n",
      "Epoch 659/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2650 - val_loss: 176.1018\n",
      "Epoch 660/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2431 - val_loss: 176.0915\n",
      "Epoch 661/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2580 - val_loss: 176.1395\n",
      "Epoch 662/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2335 - val_loss: 176.1580\n",
      "Epoch 663/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2443 - val_loss: 176.1839\n",
      "Epoch 664/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2341 - val_loss: 176.2106\n",
      "Epoch 665/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2486 - val_loss: 176.2154\n",
      "Epoch 666/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2346 - val_loss: 176.2528\n",
      "Epoch 667/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2393 - val_loss: 176.2873\n",
      "Epoch 668/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2332 - val_loss: 176.2798\n",
      "Epoch 669/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2343 - val_loss: 176.3018\n",
      "Epoch 670/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2462 - val_loss: 176.2942\n",
      "Epoch 671/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2307 - val_loss: 176.2984\n",
      "Epoch 672/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2236 - val_loss: 176.2877\n",
      "Epoch 673/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2238 - val_loss: 176.2879\n",
      "Epoch 674/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2297 - val_loss: 176.2816\n",
      "Epoch 675/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2290 - val_loss: 176.2798\n",
      "Epoch 676/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.2261 - val_loss: 176.2657\n",
      "Epoch 677/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.2278 - val_loss: 176.2788\n",
      "Epoch 678/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2353 - val_loss: 176.3111\n",
      "Epoch 679/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2160 - val_loss: 176.2847\n",
      "Epoch 680/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2416 - val_loss: 176.2478\n",
      "Epoch 681/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2129 - val_loss: 176.2617\n",
      "Epoch 682/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2178 - val_loss: 176.2689\n",
      "Epoch 683/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2070 - val_loss: 176.2630\n",
      "Epoch 684/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2081 - val_loss: 176.2750\n",
      "Epoch 685/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2160 - val_loss: 176.3132\n",
      "Epoch 686/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2080 - val_loss: 176.3433\n",
      "Epoch 687/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2002 - val_loss: 176.3670\n",
      "Epoch 688/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2048 - val_loss: 176.3547\n",
      "Epoch 689/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2039 - val_loss: 176.3490\n",
      "Epoch 690/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2002 - val_loss: 176.3423\n",
      "Epoch 691/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1982 - val_loss: 176.3616\n",
      "Epoch 692/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1976 - val_loss: 176.3595\n",
      "Epoch 693/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1936 - val_loss: 176.3764\n",
      "Epoch 694/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1913 - val_loss: 176.3752\n",
      "Epoch 695/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2068 - val_loss: 176.3391\n",
      "Epoch 696/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2029 - val_loss: 176.3606\n",
      "Epoch 697/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1907 - val_loss: 176.3688\n",
      "Epoch 698/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2054 - val_loss: 176.3883\n",
      "Epoch 699/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1904 - val_loss: 176.3684\n",
      "Epoch 700/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1915 - val_loss: 176.3481\n",
      "Epoch 701/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2061 - val_loss: 176.3378\n",
      "Epoch 702/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1904 - val_loss: 176.3456\n",
      "Epoch 703/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2000 - val_loss: 176.3837\n",
      "Epoch 704/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1858 - val_loss: 176.3751\n",
      "Epoch 705/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2032 - val_loss: 176.3521\n",
      "Epoch 706/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2013 - val_loss: 176.3641\n",
      "Epoch 707/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1830 - val_loss: 176.3800\n",
      "Epoch 708/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1815 - val_loss: 176.3728\n",
      "Epoch 709/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1959 - val_loss: 176.3558\n",
      "Epoch 710/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1763 - val_loss: 176.3698\n",
      "Epoch 711/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1899 - val_loss: 176.3808\n",
      "Epoch 712/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1771 - val_loss: 176.3854\n",
      "Epoch 713/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1938 - val_loss: 176.4209\n",
      "Epoch 714/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1865 - val_loss: 176.4461\n",
      "Epoch 715/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1869 - val_loss: 176.4579\n",
      "Epoch 716/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1721 - val_loss: 176.4625\n",
      "Epoch 717/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1784 - val_loss: 176.4552\n",
      "Epoch 718/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1815 - val_loss: 176.4693\n",
      "Epoch 719/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1684 - val_loss: 176.4687\n",
      "Epoch 720/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2066 - val_loss: 176.4499\n",
      "Epoch 721/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1691 - val_loss: 176.4824\n",
      "Epoch 722/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2045 - val_loss: 176.4686\n",
      "Epoch 723/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2248 - val_loss: 176.5325\n",
      "Epoch 724/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1788 - val_loss: 176.5311\n",
      "Epoch 725/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1778 - val_loss: 176.5157\n",
      "Epoch 726/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1776 - val_loss: 176.5315\n",
      "Epoch 727/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1681 - val_loss: 176.5276\n",
      "Epoch 728/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1622 - val_loss: 176.5249\n",
      "Epoch 729/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1759 - val_loss: 176.4939\n",
      "Epoch 730/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1626 - val_loss: 176.5069\n",
      "Epoch 731/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1768 - val_loss: 176.4953\n",
      "Epoch 732/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1546 - val_loss: 176.5159\n",
      "Epoch 733/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2121 - val_loss: 176.5866\n",
      "Epoch 734/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1562 - val_loss: 176.5847\n",
      "Epoch 735/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1638 - val_loss: 176.6014\n",
      "Epoch 736/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1678 - val_loss: 176.5778\n",
      "Epoch 737/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1572 - val_loss: 176.5853\n",
      "Epoch 738/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1604 - val_loss: 176.5909\n",
      "Epoch 739/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1673 - val_loss: 176.6126\n",
      "Epoch 740/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1660 - val_loss: 176.6110\n",
      "Epoch 741/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1516 - val_loss: 176.6087\n",
      "Epoch 742/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1469 - val_loss: 176.6321\n",
      "Epoch 743/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1544 - val_loss: 176.6221\n",
      "Epoch 744/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1461 - val_loss: 176.6368\n",
      "Epoch 745/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1575 - val_loss: 176.6550\n",
      "Epoch 746/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1830 - val_loss: 176.6307\n",
      "Epoch 747/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1942 - val_loss: 176.7084\n",
      "Epoch 748/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1580 - val_loss: 176.7280\n",
      "Epoch 749/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1815 - val_loss: 176.7430\n",
      "Epoch 750/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1661 - val_loss: 176.7214\n",
      ">Saved ./models/comp4948_a2_model_9.h5\n",
      "Epoch 1/750\n",
      "6/6 [==============================] - 0s 30ms/step - loss: 4533.6328 - val_loss: 4530.3984\n",
      "Epoch 2/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4527.5088 - val_loss: 4524.3784\n",
      "Epoch 3/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4521.4795 - val_loss: 4518.3765\n",
      "Epoch 4/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4515.5635 - val_loss: 4512.3628\n",
      "Epoch 5/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4509.4868 - val_loss: 4506.3472\n",
      "Epoch 6/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4503.4971 - val_loss: 4500.2305\n",
      "Epoch 7/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4497.3213 - val_loss: 4494.1123\n",
      "Epoch 8/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4491.2310 - val_loss: 4487.9751\n",
      "Epoch 9/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4485.0869 - val_loss: 4481.7090\n",
      "Epoch 10/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4478.7339 - val_loss: 4475.3345\n",
      "Epoch 11/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4472.3457 - val_loss: 4468.8706\n",
      "Epoch 12/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4465.9463 - val_loss: 4462.4038\n",
      "Epoch 13/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4459.3618 - val_loss: 4455.7925\n",
      "Epoch 14/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4452.7256 - val_loss: 4448.9395\n",
      "Epoch 15/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4445.8330 - val_loss: 4441.8813\n",
      "Epoch 16/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4438.7227 - val_loss: 4434.7754\n",
      "Epoch 17/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4431.5425 - val_loss: 4427.5532\n",
      "Epoch 18/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4424.3472 - val_loss: 4420.2871\n",
      "Epoch 19/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4416.9995 - val_loss: 4412.7778\n",
      "Epoch 20/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4409.5122 - val_loss: 4405.2451\n",
      "Epoch 21/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4401.8813 - val_loss: 4397.5400\n",
      "Epoch 22/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4394.0552 - val_loss: 4389.6226\n",
      "Epoch 23/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4386.2622 - val_loss: 4381.7065\n",
      "Epoch 24/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4378.2656 - val_loss: 4373.6411\n",
      "Epoch 25/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4370.1401 - val_loss: 4365.3027\n",
      "Epoch 26/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4361.6655 - val_loss: 4356.8184\n",
      "Epoch 27/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4353.2056 - val_loss: 4348.2192\n",
      "Epoch 28/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4344.4985 - val_loss: 4339.3706\n",
      "Epoch 29/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4335.5278 - val_loss: 4330.3745\n",
      "Epoch 30/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4326.6841 - val_loss: 4321.2402\n",
      "Epoch 31/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4317.3882 - val_loss: 4312.0420\n",
      "Epoch 32/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4308.2056 - val_loss: 4302.7285\n",
      "Epoch 33/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4298.9141 - val_loss: 4293.3813\n",
      "Epoch 34/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4289.5498 - val_loss: 4283.8276\n",
      "Epoch 35/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4279.9272 - val_loss: 4274.0454\n",
      "Epoch 36/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4270.0894 - val_loss: 4264.1323\n",
      "Epoch 37/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4260.1641 - val_loss: 4254.0845\n",
      "Epoch 38/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4249.9966 - val_loss: 4243.9170\n",
      "Epoch 39/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4239.8027 - val_loss: 4233.4980\n",
      "Epoch 40/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4229.3379 - val_loss: 4223.1470\n",
      "Epoch 41/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4219.1470 - val_loss: 4212.6567\n",
      "Epoch 42/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4208.5400 - val_loss: 4202.1431\n",
      "Epoch 43/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4198.0020 - val_loss: 4191.5303\n",
      "Epoch 44/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4187.2993 - val_loss: 4180.5723\n",
      "Epoch 45/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4176.2803 - val_loss: 4169.2817\n",
      "Epoch 46/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4164.9023 - val_loss: 4157.7100\n",
      "Epoch 47/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4153.3188 - val_loss: 4146.0557\n",
      "Epoch 48/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4141.4067 - val_loss: 4134.3047\n",
      "Epoch 49/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4129.8628 - val_loss: 4122.4531\n",
      "Epoch 50/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4117.8511 - val_loss: 4110.3784\n",
      "Epoch 51/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4105.8154 - val_loss: 4098.3472\n",
      "Epoch 52/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4093.7717 - val_loss: 4086.3057\n",
      "Epoch 53/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4081.8057 - val_loss: 4073.8132\n",
      "Epoch 54/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4069.1404 - val_loss: 4061.1775\n",
      "Epoch 55/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4056.5601 - val_loss: 4048.4009\n",
      "Epoch 56/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4043.7791 - val_loss: 4035.4709\n",
      "Epoch 57/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 4030.6580 - val_loss: 4022.3140\n",
      "Epoch 58/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4017.5913 - val_loss: 4008.9688\n",
      "Epoch 59/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 4004.0886 - val_loss: 3995.6992\n",
      "Epoch 60/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3991.1570 - val_loss: 3982.7627\n",
      "Epoch 61/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3977.9949 - val_loss: 3969.5603\n",
      "Epoch 62/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3964.7354 - val_loss: 3956.0100\n",
      "Epoch 63/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3951.2002 - val_loss: 3942.1064\n",
      "Epoch 64/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3937.0857 - val_loss: 3927.9475\n",
      "Epoch 65/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3922.8154 - val_loss: 3913.4727\n",
      "Epoch 66/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3908.2275 - val_loss: 3898.6470\n",
      "Epoch 67/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3893.3774 - val_loss: 3883.5732\n",
      "Epoch 68/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3878.1667 - val_loss: 3868.3635\n",
      "Epoch 69/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3863.1118 - val_loss: 3853.1506\n",
      "Epoch 70/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3847.9121 - val_loss: 3837.8389\n",
      "Epoch 71/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3832.3584 - val_loss: 3822.6057\n",
      "Epoch 72/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3817.5657 - val_loss: 3807.6682\n",
      "Epoch 73/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3802.5657 - val_loss: 3792.4109\n",
      "Epoch 74/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3787.1094 - val_loss: 3776.8892\n",
      "Epoch 75/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3771.6177 - val_loss: 3761.1514\n",
      "Epoch 76/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3755.7659 - val_loss: 3745.0298\n",
      "Epoch 77/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3739.7263 - val_loss: 3728.6946\n",
      "Epoch 78/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3723.3569 - val_loss: 3712.0461\n",
      "Epoch 79/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3706.8208 - val_loss: 3695.5593\n",
      "Epoch 80/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3690.2664 - val_loss: 3679.2219\n",
      "Epoch 81/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3674.0813 - val_loss: 3662.4958\n",
      "Epoch 82/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3657.2688 - val_loss: 3645.6279\n",
      "Epoch 83/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3640.3198 - val_loss: 3628.6592\n",
      "Epoch 84/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3623.3481 - val_loss: 3611.3977\n",
      "Epoch 85/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3606.3870 - val_loss: 3594.2180\n",
      "Epoch 86/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3588.7979 - val_loss: 3577.1787\n",
      "Epoch 87/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3571.8250 - val_loss: 3559.9517\n",
      "Epoch 88/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3554.8438 - val_loss: 3542.8079\n",
      "Epoch 89/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3537.6560 - val_loss: 3525.3311\n",
      "Epoch 90/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3520.1062 - val_loss: 3507.6826\n",
      "Epoch 91/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3502.4382 - val_loss: 3489.6697\n",
      "Epoch 92/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3484.2166 - val_loss: 3471.5115\n",
      "Epoch 93/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3466.2170 - val_loss: 3453.4404\n",
      "Epoch 94/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3448.2500 - val_loss: 3434.9702\n",
      "Epoch 95/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3429.7224 - val_loss: 3416.5029\n",
      "Epoch 96/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3411.2551 - val_loss: 3398.0830\n",
      "Epoch 97/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3392.6443 - val_loss: 3379.5640\n",
      "Epoch 98/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3374.9094 - val_loss: 3361.7922\n",
      "Epoch 99/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3357.0479 - val_loss: 3343.8867\n",
      "Epoch 100/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3339.1987 - val_loss: 3325.6228\n",
      "Epoch 101/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3320.7097 - val_loss: 3307.2314\n",
      "Epoch 102/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3302.3037 - val_loss: 3288.3557\n",
      "Epoch 103/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3283.3201 - val_loss: 3269.1758\n",
      "Epoch 104/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3264.0581 - val_loss: 3249.6057\n",
      "Epoch 105/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3244.3765 - val_loss: 3229.8743\n",
      "Epoch 106/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3224.8420 - val_loss: 3210.2000\n",
      "Epoch 107/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3205.1353 - val_loss: 3190.4915\n",
      "Epoch 108/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3185.6057 - val_loss: 3170.3140\n",
      "Epoch 109/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3165.5066 - val_loss: 3150.1555\n",
      "Epoch 110/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3145.4629 - val_loss: 3130.1624\n",
      "Epoch 111/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3125.4172 - val_loss: 3109.8657\n",
      "Epoch 112/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3105.0422 - val_loss: 3089.2703\n",
      "Epoch 113/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3084.4485 - val_loss: 3068.3062\n",
      "Epoch 114/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3063.1650 - val_loss: 3047.2031\n",
      "Epoch 115/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 3042.3142 - val_loss: 3026.2212\n",
      "Epoch 116/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3021.4072 - val_loss: 3005.1929\n",
      "Epoch 117/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 3000.7097 - val_loss: 2984.5959\n",
      "Epoch 118/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2979.6538 - val_loss: 2964.0015\n",
      "Epoch 119/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2959.3157 - val_loss: 2942.8313\n",
      "Epoch 120/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2938.1938 - val_loss: 2921.7432\n",
      "Epoch 121/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2917.1035 - val_loss: 2900.6431\n",
      "Epoch 122/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2895.7336 - val_loss: 2879.3262\n",
      "Epoch 123/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2874.3955 - val_loss: 2857.6414\n",
      "Epoch 124/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2852.7346 - val_loss: 2835.6812\n",
      "Epoch 125/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2831.2290 - val_loss: 2814.4309\n",
      "Epoch 126/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2810.2485 - val_loss: 2792.8633\n",
      "Epoch 127/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2788.2632 - val_loss: 2771.4475\n",
      "Epoch 128/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2766.9875 - val_loss: 2749.7830\n",
      "Epoch 129/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2745.5344 - val_loss: 2727.8733\n",
      "Epoch 130/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2723.8345 - val_loss: 2705.6877\n",
      "Epoch 131/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2701.5227 - val_loss: 2683.4978\n",
      "Epoch 132/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2679.2239 - val_loss: 2661.4685\n",
      "Epoch 133/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2657.7954 - val_loss: 2639.8015\n",
      "Epoch 134/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2636.0222 - val_loss: 2618.1143\n",
      "Epoch 135/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2614.6360 - val_loss: 2596.0615\n",
      "Epoch 136/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2592.6069 - val_loss: 2573.8135\n",
      "Epoch 137/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2570.3792 - val_loss: 2551.4026\n",
      "Epoch 138/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2547.9749 - val_loss: 2528.7441\n",
      "Epoch 139/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2525.2732 - val_loss: 2506.0283\n",
      "Epoch 140/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 2503.1226 - val_loss: 2483.6619\n",
      "Epoch 141/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 2481.0059 - val_loss: 2461.1360\n",
      "Epoch 142/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2458.3909 - val_loss: 2438.6289\n",
      "Epoch 143/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2435.6802 - val_loss: 2416.2876\n",
      "Epoch 144/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2413.6624 - val_loss: 2393.8425\n",
      "Epoch 145/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2391.5120 - val_loss: 2371.2312\n",
      "Epoch 146/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2368.8291 - val_loss: 2348.6345\n",
      "Epoch 147/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2346.7981 - val_loss: 2326.3960\n",
      "Epoch 148/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2324.6863 - val_loss: 2303.9998\n",
      "Epoch 149/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2302.5591 - val_loss: 2281.3530\n",
      "Epoch 150/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2279.8296 - val_loss: 2258.8459\n",
      "Epoch 151/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2257.7178 - val_loss: 2236.1033\n",
      "Epoch 152/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2235.0095 - val_loss: 2213.3193\n",
      "Epoch 153/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2212.8582 - val_loss: 2190.7131\n",
      "Epoch 154/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2190.4636 - val_loss: 2168.0940\n",
      "Epoch 155/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2167.8994 - val_loss: 2145.5757\n",
      "Epoch 156/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2145.5474 - val_loss: 2122.8865\n",
      "Epoch 157/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2122.8359 - val_loss: 2100.1172\n",
      "Epoch 158/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2100.6216 - val_loss: 2076.9229\n",
      "Epoch 159/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 2077.5432 - val_loss: 2053.8594\n",
      "Epoch 160/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2054.8958 - val_loss: 2030.6851\n",
      "Epoch 161/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2031.7871 - val_loss: 2007.4927\n",
      "Epoch 162/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 2009.1221 - val_loss: 1984.2303\n",
      "Epoch 163/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1985.7358 - val_loss: 1961.2410\n",
      "Epoch 164/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1962.9944 - val_loss: 1938.9371\n",
      "Epoch 165/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1941.5730 - val_loss: 1916.5149\n",
      "Epoch 166/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1919.2129 - val_loss: 1894.2625\n",
      "Epoch 167/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1896.9773 - val_loss: 1872.1614\n",
      "Epoch 168/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1875.1024 - val_loss: 1850.0194\n",
      "Epoch 169/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1853.3013 - val_loss: 1827.7278\n",
      "Epoch 170/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1831.2694 - val_loss: 1805.4070\n",
      "Epoch 171/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1808.9519 - val_loss: 1783.1538\n",
      "Epoch 172/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1787.3496 - val_loss: 1760.7773\n",
      "Epoch 173/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1765.3297 - val_loss: 1738.6512\n",
      "Epoch 174/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1743.1298 - val_loss: 1716.7445\n",
      "Epoch 175/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1721.3925 - val_loss: 1694.7616\n",
      "Epoch 176/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1699.4869 - val_loss: 1672.7726\n",
      "Epoch 177/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1677.9119 - val_loss: 1650.6555\n",
      "Epoch 178/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1656.2578 - val_loss: 1628.8927\n",
      "Epoch 179/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1634.5087 - val_loss: 1607.4812\n",
      "Epoch 180/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1613.4982 - val_loss: 1585.8333\n",
      "Epoch 181/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1591.5978 - val_loss: 1564.6202\n",
      "Epoch 182/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1571.0762 - val_loss: 1543.9208\n",
      "Epoch 183/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1550.8628 - val_loss: 1523.1770\n",
      "Epoch 184/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1530.4740 - val_loss: 1502.4559\n",
      "Epoch 185/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1509.9323 - val_loss: 1481.9032\n",
      "Epoch 186/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1489.8573 - val_loss: 1461.1913\n",
      "Epoch 187/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1469.1840 - val_loss: 1440.6652\n",
      "Epoch 188/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1449.0308 - val_loss: 1419.9843\n",
      "Epoch 189/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1428.5719 - val_loss: 1399.3805\n",
      "Epoch 190/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1407.9387 - val_loss: 1378.9413\n",
      "Epoch 191/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1387.6882 - val_loss: 1358.4526\n",
      "Epoch 192/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1367.4908 - val_loss: 1337.9139\n",
      "Epoch 193/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1347.1543 - val_loss: 1317.5597\n",
      "Epoch 194/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1327.3854 - val_loss: 1297.2025\n",
      "Epoch 195/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1307.1320 - val_loss: 1277.0667\n",
      "Epoch 196/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1287.2546 - val_loss: 1256.9169\n",
      "Epoch 197/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 1267.6550 - val_loss: 1236.7356\n",
      "Epoch 198/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1247.4391 - val_loss: 1216.9218\n",
      "Epoch 199/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1227.8976 - val_loss: 1197.0947\n",
      "Epoch 200/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1208.7058 - val_loss: 1177.3835\n",
      "Epoch 201/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1189.3784 - val_loss: 1157.9083\n",
      "Epoch 202/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1170.1001 - val_loss: 1138.5811\n",
      "Epoch 203/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1151.0165 - val_loss: 1119.3325\n",
      "Epoch 204/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1131.6454 - val_loss: 1100.4110\n",
      "Epoch 205/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1113.1516 - val_loss: 1081.3212\n",
      "Epoch 206/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1094.3798 - val_loss: 1062.3075\n",
      "Epoch 207/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1075.8104 - val_loss: 1043.3564\n",
      "Epoch 208/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1056.8250 - val_loss: 1024.7434\n",
      "Epoch 209/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1039.0165 - val_loss: 1005.8777\n",
      "Epoch 210/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 1020.5276 - val_loss: 987.3889\n",
      "Epoch 211/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 1002.1477 - val_loss: 969.2387\n",
      "Epoch 212/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 984.7223 - val_loss: 951.0039\n",
      "Epoch 213/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 966.7132 - val_loss: 933.1252\n",
      "Epoch 214/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 948.9714 - val_loss: 915.4643\n",
      "Epoch 215/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 931.4568 - val_loss: 898.0186\n",
      "Epoch 216/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 914.6224 - val_loss: 880.6933\n",
      "Epoch 217/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 897.4468 - val_loss: 863.6774\n",
      "Epoch 218/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 880.8380 - val_loss: 846.7236\n",
      "Epoch 219/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 864.5871 - val_loss: 829.7332\n",
      "Epoch 220/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 847.5547 - val_loss: 813.2838\n",
      "Epoch 221/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 831.6888 - val_loss: 796.8177\n",
      "Epoch 222/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 816.2867 - val_loss: 780.7725\n",
      "Epoch 223/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 800.3959 - val_loss: 765.3737\n",
      "Epoch 224/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 785.0557 - val_loss: 750.2369\n",
      "Epoch 225/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 770.2068 - val_loss: 735.1317\n",
      "Epoch 226/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 755.6780 - val_loss: 720.0480\n",
      "Epoch 227/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 740.6654 - val_loss: 705.4066\n",
      "Epoch 228/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 726.6232 - val_loss: 690.7213\n",
      "Epoch 229/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 712.3028 - val_loss: 676.3057\n",
      "Epoch 230/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 698.1382 - val_loss: 662.1422\n",
      "Epoch 231/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 684.4951 - val_loss: 648.0911\n",
      "Epoch 232/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 670.9016 - val_loss: 634.2748\n",
      "Epoch 233/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 657.3742 - val_loss: 620.8807\n",
      "Epoch 234/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 644.2626 - val_loss: 607.6726\n",
      "Epoch 235/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 631.6430 - val_loss: 594.4666\n",
      "Epoch 236/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 618.8951 - val_loss: 581.5467\n",
      "Epoch 237/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 606.3773 - val_loss: 568.8914\n",
      "Epoch 238/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 594.1468 - val_loss: 556.4668\n",
      "Epoch 239/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 582.1406 - val_loss: 544.3156\n",
      "Epoch 240/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 570.2034 - val_loss: 532.5152\n",
      "Epoch 241/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 559.1032 - val_loss: 520.7794\n",
      "Epoch 242/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 547.5759 - val_loss: 509.4726\n",
      "Epoch 243/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 536.7609 - val_loss: 498.2456\n",
      "Epoch 244/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 526.1669 - val_loss: 487.1946\n",
      "Epoch 245/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 515.1345 - val_loss: 476.7248\n",
      "Epoch 246/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 505.2226 - val_loss: 466.2809\n",
      "Epoch 247/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 495.2558 - val_loss: 456.0628\n",
      "Epoch 248/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 485.2349 - val_loss: 446.2561\n",
      "Epoch 249/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 475.8718 - val_loss: 436.5247\n",
      "Epoch 250/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 466.5991 - val_loss: 426.9918\n",
      "Epoch 251/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 457.3821 - val_loss: 417.7672\n",
      "Epoch 252/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 448.8238 - val_loss: 408.6081\n",
      "Epoch 253/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 439.9685 - val_loss: 399.8381\n",
      "Epoch 254/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 431.5032 - val_loss: 391.3801\n",
      "Epoch 255/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 423.5220 - val_loss: 383.0811\n",
      "Epoch 256/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 415.5780 - val_loss: 374.9705\n",
      "Epoch 257/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 407.9777 - val_loss: 367.0922\n",
      "Epoch 258/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 400.5703 - val_loss: 359.5511\n",
      "Epoch 259/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 393.5566 - val_loss: 352.2085\n",
      "Epoch 260/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 386.5770 - val_loss: 345.1309\n",
      "Epoch 261/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 379.7567 - val_loss: 338.3152\n",
      "Epoch 262/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 373.3252 - val_loss: 331.7043\n",
      "Epoch 263/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 367.1263 - val_loss: 325.3142\n",
      "Epoch 264/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 361.1906 - val_loss: 319.0378\n",
      "Epoch 265/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 355.0877 - val_loss: 313.0477\n",
      "Epoch 266/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 349.5717 - val_loss: 307.1489\n",
      "Epoch 267/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 344.1280 - val_loss: 301.4291\n",
      "Epoch 268/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 338.9055 - val_loss: 295.9254\n",
      "Epoch 269/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 333.7211 - val_loss: 290.6996\n",
      "Epoch 270/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 328.5359 - val_loss: 285.7923\n",
      "Epoch 271/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 324.1654 - val_loss: 280.7988\n",
      "Epoch 272/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 319.3940 - val_loss: 276.0914\n",
      "Epoch 273/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 315.3120 - val_loss: 271.3453\n",
      "Epoch 274/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 310.7007 - val_loss: 267.0051\n",
      "Epoch 275/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 306.6824 - val_loss: 262.7928\n",
      "Epoch 276/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 302.6288 - val_loss: 258.7946\n",
      "Epoch 277/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 299.1364 - val_loss: 254.7741\n",
      "Epoch 278/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 295.4426 - val_loss: 250.9756\n",
      "Epoch 279/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 292.0220 - val_loss: 247.3109\n",
      "Epoch 280/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 288.7396 - val_loss: 243.8372\n",
      "Epoch 281/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 285.3494 - val_loss: 240.6882\n",
      "Epoch 282/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 282.3155 - val_loss: 237.6261\n",
      "Epoch 283/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 279.6570 - val_loss: 234.4991\n",
      "Epoch 284/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 277.1133 - val_loss: 231.3678\n",
      "Epoch 285/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 274.0920 - val_loss: 228.5716\n",
      "Epoch 286/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 271.7086 - val_loss: 225.8591\n",
      "Epoch 287/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 269.0968 - val_loss: 223.4165\n",
      "Epoch 288/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 267.1118 - val_loss: 220.8992\n",
      "Epoch 289/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 264.7914 - val_loss: 218.6155\n",
      "Epoch 290/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 262.6309 - val_loss: 216.4837\n",
      "Epoch 291/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 260.9064 - val_loss: 214.2917\n",
      "Epoch 292/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 258.7653 - val_loss: 212.4087\n",
      "Epoch 293/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 257.0307 - val_loss: 210.5596\n",
      "Epoch 294/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 255.4110 - val_loss: 208.7074\n",
      "Epoch 295/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 253.8762 - val_loss: 206.8936\n",
      "Epoch 296/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 252.1879 - val_loss: 205.2595\n",
      "Epoch 297/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 250.7267 - val_loss: 203.6598\n",
      "Epoch 298/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 249.4822 - val_loss: 202.0472\n",
      "Epoch 299/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 247.8885 - val_loss: 200.6671\n",
      "Epoch 300/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 246.6700 - val_loss: 199.2778\n",
      "Epoch 301/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 245.6728 - val_loss: 197.8141\n",
      "Epoch 302/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 244.3697 - val_loss: 196.5289\n",
      "Epoch 303/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 243.2174 - val_loss: 195.3338\n",
      "Epoch 304/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 242.1994 - val_loss: 194.2161\n",
      "Epoch 305/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 241.3322 - val_loss: 193.0729\n",
      "Epoch 306/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 240.3450 - val_loss: 192.0242\n",
      "Epoch 307/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 239.5430 - val_loss: 191.0310\n",
      "Epoch 308/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 238.5636 - val_loss: 190.2263\n",
      "Epoch 309/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 237.9139 - val_loss: 189.3767\n",
      "Epoch 310/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 237.2666 - val_loss: 188.5358\n",
      "Epoch 311/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 236.4647 - val_loss: 187.8416\n",
      "Epoch 312/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 235.9214 - val_loss: 187.0618\n",
      "Epoch 313/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 235.2802 - val_loss: 186.3765\n",
      "Epoch 314/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 234.7756 - val_loss: 185.6719\n",
      "Epoch 315/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 234.2090 - val_loss: 185.0144\n",
      "Epoch 316/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 233.6989 - val_loss: 184.4197\n",
      "Epoch 317/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 233.2208 - val_loss: 183.8566\n",
      "Epoch 318/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 232.7902 - val_loss: 183.3212\n",
      "Epoch 319/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 232.3588 - val_loss: 182.8386\n",
      "Epoch 320/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.8949 - val_loss: 182.4254\n",
      "Epoch 321/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.5769 - val_loss: 181.9718\n",
      "Epoch 322/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 231.2301 - val_loss: 181.5112\n",
      "Epoch 323/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.8889 - val_loss: 181.1062\n",
      "Epoch 324/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.6457 - val_loss: 180.6813\n",
      "Epoch 325/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 230.2624 - val_loss: 180.3340\n",
      "Epoch 326/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.9977 - val_loss: 179.9966\n",
      "Epoch 327/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 229.7638 - val_loss: 179.6556\n",
      "Epoch 328/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.4275 - val_loss: 179.3880\n",
      "Epoch 329/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 229.2048 - val_loss: 179.1432\n",
      "Epoch 330/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 228.9789 - val_loss: 178.8565\n",
      "Epoch 331/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.8309 - val_loss: 178.5363\n",
      "Epoch 332/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.6261 - val_loss: 178.2624\n",
      "Epoch 333/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.3991 - val_loss: 178.0244\n",
      "Epoch 334/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.2166 - val_loss: 177.8014\n",
      "Epoch 335/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 228.0588 - val_loss: 177.6035\n",
      "Epoch 336/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.8863 - val_loss: 177.4157\n",
      "Epoch 337/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.7475 - val_loss: 177.2374\n",
      "Epoch 338/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.6097 - val_loss: 177.0736\n",
      "Epoch 339/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.4631 - val_loss: 176.9514\n",
      "Epoch 340/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 227.3639 - val_loss: 176.7872\n",
      "Epoch 341/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.2130 - val_loss: 176.6545\n",
      "Epoch 342/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 227.1339 - val_loss: 176.4895\n",
      "Epoch 343/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.9923 - val_loss: 176.3331\n",
      "Epoch 344/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.8791 - val_loss: 176.1905\n",
      "Epoch 345/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.8541 - val_loss: 176.0189\n",
      "Epoch 346/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.6755 - val_loss: 175.9106\n",
      "Epoch 347/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.6105 - val_loss: 175.7994\n",
      "Epoch 348/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.4763 - val_loss: 175.7054\n",
      "Epoch 349/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.3963 - val_loss: 175.6154\n",
      "Epoch 350/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 226.3538 - val_loss: 175.4889\n",
      "Epoch 351/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.2742 - val_loss: 175.3845\n",
      "Epoch 352/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.1542 - val_loss: 175.3285\n",
      "Epoch 353/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.0877 - val_loss: 175.2655\n",
      "Epoch 354/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 226.0329 - val_loss: 175.2012\n",
      "Epoch 355/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.9590 - val_loss: 175.1140\n",
      "Epoch 356/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.9303 - val_loss: 175.0129\n",
      "Epoch 357/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.8080 - val_loss: 174.9502\n",
      "Epoch 358/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.7421 - val_loss: 174.8895\n",
      "Epoch 359/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.7064 - val_loss: 174.7888\n",
      "Epoch 360/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.6267 - val_loss: 174.7209\n",
      "Epoch 361/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.5735 - val_loss: 174.7103\n",
      "Epoch 362/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.5233 - val_loss: 174.6236\n",
      "Epoch 363/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.4361 - val_loss: 174.5752\n",
      "Epoch 364/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.3780 - val_loss: 174.5014\n",
      "Epoch 365/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.3371 - val_loss: 174.4413\n",
      "Epoch 366/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 225.3036 - val_loss: 174.3651\n",
      "Epoch 367/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 225.2217 - val_loss: 174.3243\n",
      "Epoch 368/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.1876 - val_loss: 174.3186\n",
      "Epoch 369/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 225.1302 - val_loss: 174.2680\n",
      "Epoch 370/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.0684 - val_loss: 174.2246\n",
      "Epoch 371/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 225.0256 - val_loss: 174.1921\n",
      "Epoch 372/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.9787 - val_loss: 174.1587\n",
      "Epoch 373/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.9428 - val_loss: 174.1203\n",
      "Epoch 374/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.8894 - val_loss: 174.0657\n",
      "Epoch 375/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.8391 - val_loss: 174.0181\n",
      "Epoch 376/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.8175 - val_loss: 173.9648\n",
      "Epoch 377/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.7559 - val_loss: 173.9370\n",
      "Epoch 378/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 224.7336 - val_loss: 173.9400\n",
      "Epoch 379/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.6803 - val_loss: 173.9000\n",
      "Epoch 380/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 224.6195 - val_loss: 173.8657\n",
      "Epoch 381/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 224.5866 - val_loss: 173.8337\n",
      "Epoch 382/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 224.5648 - val_loss: 173.7830\n",
      "Epoch 383/750\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 224.5078 - val_loss: 173.7519\n",
      "Epoch 384/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 224.4680 - val_loss: 173.7249\n",
      "Epoch 385/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 224.4267 - val_loss: 173.7149\n",
      "Epoch 386/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 224.3925 - val_loss: 173.7164\n",
      "Epoch 387/750\n",
      "6/6 [==============================] - 0s 15ms/step - loss: 224.4708 - val_loss: 173.7819\n",
      "Epoch 388/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 224.3202 - val_loss: 173.7157\n",
      "Epoch 389/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 224.2668 - val_loss: 173.7003\n",
      "Epoch 390/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 224.2256 - val_loss: 173.6947\n",
      "Epoch 391/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 224.2334 - val_loss: 173.6251\n",
      "Epoch 392/750\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 224.1571 - val_loss: 173.6084\n",
      "Epoch 393/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 224.1246 - val_loss: 173.6093\n",
      "Epoch 394/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0775 - val_loss: 173.6034\n",
      "Epoch 395/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0533 - val_loss: 173.5834\n",
      "Epoch 396/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 224.0323 - val_loss: 173.5427\n",
      "Epoch 397/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9736 - val_loss: 173.5543\n",
      "Epoch 398/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9488 - val_loss: 173.5601\n",
      "Epoch 399/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.9157 - val_loss: 173.5749\n",
      "Epoch 400/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.8761 - val_loss: 173.5709\n",
      "Epoch 401/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.8505 - val_loss: 173.5509\n",
      "Epoch 402/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.8164 - val_loss: 173.5184\n",
      "Epoch 403/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.7712 - val_loss: 173.5028\n",
      "Epoch 404/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.7496 - val_loss: 173.4507\n",
      "Epoch 405/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.7211 - val_loss: 173.4680\n",
      "Epoch 406/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6855 - val_loss: 173.4221\n",
      "Epoch 407/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.6590 - val_loss: 173.4203\n",
      "Epoch 408/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.6114 - val_loss: 173.4097\n",
      "Epoch 409/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5893 - val_loss: 173.3884\n",
      "Epoch 410/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5446 - val_loss: 173.3783\n",
      "Epoch 411/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.5175 - val_loss: 173.3897\n",
      "Epoch 412/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.4995 - val_loss: 173.3822\n",
      "Epoch 413/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4567 - val_loss: 173.3746\n",
      "Epoch 414/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4289 - val_loss: 173.3725\n",
      "Epoch 415/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.4576 - val_loss: 173.4087\n",
      "Epoch 416/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3788 - val_loss: 173.3748\n",
      "Epoch 417/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3531 - val_loss: 173.3737\n",
      "Epoch 418/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.3285 - val_loss: 173.3943\n",
      "Epoch 419/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.2933 - val_loss: 173.4048\n",
      "Epoch 420/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2547 - val_loss: 173.4009\n",
      "Epoch 421/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2365 - val_loss: 173.3960\n",
      "Epoch 422/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.2020 - val_loss: 173.3684\n",
      "Epoch 423/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1672 - val_loss: 173.3526\n",
      "Epoch 424/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.1873 - val_loss: 173.3689\n",
      "Epoch 425/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.1342 - val_loss: 173.3545\n",
      "Epoch 426/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0992 - val_loss: 173.3283\n",
      "Epoch 427/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0845 - val_loss: 173.2962\n",
      "Epoch 428/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 223.0581 - val_loss: 173.2653\n",
      "Epoch 429/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0506 - val_loss: 173.2455\n",
      "Epoch 430/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0033 - val_loss: 173.2392\n",
      "Epoch 431/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 223.0155 - val_loss: 173.2814\n",
      "Epoch 432/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9614 - val_loss: 173.2469\n",
      "Epoch 433/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.9410 - val_loss: 173.2680\n",
      "Epoch 434/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.9184 - val_loss: 173.2601\n",
      "Epoch 435/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8795 - val_loss: 173.2497\n",
      "Epoch 436/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8618 - val_loss: 173.2518\n",
      "Epoch 437/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8505 - val_loss: 173.2492\n",
      "Epoch 438/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8203 - val_loss: 173.2527\n",
      "Epoch 439/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.8197 - val_loss: 173.2802\n",
      "Epoch 440/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7676 - val_loss: 173.2791\n",
      "Epoch 441/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7708 - val_loss: 173.2654\n",
      "Epoch 442/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7668 - val_loss: 173.3137\n",
      "Epoch 443/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.7072 - val_loss: 173.2951\n",
      "Epoch 444/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.7040 - val_loss: 173.3276\n",
      "Epoch 445/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.6694 - val_loss: 173.3079\n",
      "Epoch 446/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6760 - val_loss: 173.3342\n",
      "Epoch 447/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.6277 - val_loss: 173.3166\n",
      "Epoch 448/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5974 - val_loss: 173.3154\n",
      "Epoch 449/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.6120 - val_loss: 173.2886\n",
      "Epoch 450/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5797 - val_loss: 173.3403\n",
      "Epoch 451/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.5523 - val_loss: 173.3197\n",
      "Epoch 452/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5199 - val_loss: 173.3182\n",
      "Epoch 453/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.5082 - val_loss: 173.3257\n",
      "Epoch 454/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4821 - val_loss: 173.3252\n",
      "Epoch 455/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4796 - val_loss: 173.3288\n",
      "Epoch 456/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.4541 - val_loss: 173.3066\n",
      "Epoch 457/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4405 - val_loss: 173.2883\n",
      "Epoch 458/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4070 - val_loss: 173.2769\n",
      "Epoch 459/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3975 - val_loss: 173.2798\n",
      "Epoch 460/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.4386 - val_loss: 173.2529\n",
      "Epoch 461/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3708 - val_loss: 173.3019\n",
      "Epoch 462/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3384 - val_loss: 173.3181\n",
      "Epoch 463/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3468 - val_loss: 173.3671\n",
      "Epoch 464/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3202 - val_loss: 173.3750\n",
      "Epoch 465/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.3247 - val_loss: 173.3345\n",
      "Epoch 466/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2847 - val_loss: 173.3600\n",
      "Epoch 467/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2826 - val_loss: 173.4101\n",
      "Epoch 468/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2736 - val_loss: 173.4362\n",
      "Epoch 469/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2395 - val_loss: 173.4280\n",
      "Epoch 470/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2289 - val_loss: 173.3738\n",
      "Epoch 471/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2183 - val_loss: 173.3405\n",
      "Epoch 472/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.2397 - val_loss: 173.3745\n",
      "Epoch 473/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.2005 - val_loss: 173.3495\n",
      "Epoch 474/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1685 - val_loss: 173.3502\n",
      "Epoch 475/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1587 - val_loss: 173.3760\n",
      "Epoch 476/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1478 - val_loss: 173.3515\n",
      "Epoch 477/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1455 - val_loss: 173.3577\n",
      "Epoch 478/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1138 - val_loss: 173.3653\n",
      "Epoch 479/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1133 - val_loss: 173.3950\n",
      "Epoch 480/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1116 - val_loss: 173.3842\n",
      "Epoch 481/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.1041 - val_loss: 173.3797\n",
      "Epoch 482/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0790 - val_loss: 173.3899\n",
      "Epoch 483/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0951 - val_loss: 173.4062\n",
      "Epoch 484/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0709 - val_loss: 173.4232\n",
      "Epoch 485/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0559 - val_loss: 173.4038\n",
      "Epoch 486/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0424 - val_loss: 173.4230\n",
      "Epoch 487/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 222.0313 - val_loss: 173.4153\n",
      "Epoch 488/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 222.0282 - val_loss: 173.4001\n",
      "Epoch 489/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0389 - val_loss: 173.3764\n",
      "Epoch 490/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 222.0027 - val_loss: 173.4061\n",
      "Epoch 491/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9934 - val_loss: 173.4539\n",
      "Epoch 492/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9872 - val_loss: 173.4738\n",
      "Epoch 493/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9711 - val_loss: 173.4910\n",
      "Epoch 494/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9715 - val_loss: 173.4957\n",
      "Epoch 495/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9566 - val_loss: 173.5106\n",
      "Epoch 496/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.9427 - val_loss: 173.5462\n",
      "Epoch 497/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9406 - val_loss: 173.5758\n",
      "Epoch 498/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9309 - val_loss: 173.5926\n",
      "Epoch 499/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9092 - val_loss: 173.5714\n",
      "Epoch 500/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9168 - val_loss: 173.5808\n",
      "Epoch 501/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9077 - val_loss: 173.5645\n",
      "Epoch 502/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.9114 - val_loss: 173.5948\n",
      "Epoch 503/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8834 - val_loss: 173.5936\n",
      "Epoch 504/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8853 - val_loss: 173.6144\n",
      "Epoch 505/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8856 - val_loss: 173.6017\n",
      "Epoch 506/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8803 - val_loss: 173.5603\n",
      "Epoch 507/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8727 - val_loss: 173.5900\n",
      "Epoch 508/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8745 - val_loss: 173.5722\n",
      "Epoch 509/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8517 - val_loss: 173.6059\n",
      "Epoch 510/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8484 - val_loss: 173.6114\n",
      "Epoch 511/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8395 - val_loss: 173.6468\n",
      "Epoch 512/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8520 - val_loss: 173.6148\n",
      "Epoch 513/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8213 - val_loss: 173.6611\n",
      "Epoch 514/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8036 - val_loss: 173.6870\n",
      "Epoch 515/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8213 - val_loss: 173.6457\n",
      "Epoch 516/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.8062 - val_loss: 173.6948\n",
      "Epoch 517/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7859 - val_loss: 173.7300\n",
      "Epoch 518/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7827 - val_loss: 173.7458\n",
      "Epoch 519/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7629 - val_loss: 173.7539\n",
      "Epoch 520/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.7552 - val_loss: 173.7716\n",
      "Epoch 521/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7533 - val_loss: 173.7772\n",
      "Epoch 522/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7558 - val_loss: 173.8159\n",
      "Epoch 523/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7428 - val_loss: 173.8256\n",
      "Epoch 524/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7338 - val_loss: 173.8181\n",
      "Epoch 525/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7262 - val_loss: 173.8202\n",
      "Epoch 526/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7328 - val_loss: 173.8737\n",
      "Epoch 527/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7172 - val_loss: 173.8655\n",
      "Epoch 528/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7012 - val_loss: 173.8783\n",
      "Epoch 529/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.7098 - val_loss: 173.8794\n",
      "Epoch 530/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6895 - val_loss: 173.8795\n",
      "Epoch 531/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6932 - val_loss: 173.8781\n",
      "Epoch 532/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6767 - val_loss: 173.9116\n",
      "Epoch 533/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6690 - val_loss: 173.9462\n",
      "Epoch 534/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6976 - val_loss: 173.9830\n",
      "Epoch 535/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6696 - val_loss: 173.9307\n",
      "Epoch 536/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6504 - val_loss: 173.9436\n",
      "Epoch 537/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6629 - val_loss: 173.9541\n",
      "Epoch 538/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6557 - val_loss: 173.9440\n",
      "Epoch 539/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6471 - val_loss: 173.9709\n",
      "Epoch 540/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6331 - val_loss: 173.9475\n",
      "Epoch 541/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6328 - val_loss: 173.9462\n",
      "Epoch 542/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6214 - val_loss: 173.9662\n",
      "Epoch 543/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6159 - val_loss: 173.9423\n",
      "Epoch 544/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6572 - val_loss: 173.9017\n",
      "Epoch 545/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.6128 - val_loss: 173.8945\n",
      "Epoch 546/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5994 - val_loss: 173.9240\n",
      "Epoch 547/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6076 - val_loss: 173.9749\n",
      "Epoch 548/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5828 - val_loss: 173.9835\n",
      "Epoch 549/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5721 - val_loss: 173.9928\n",
      "Epoch 550/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6184 - val_loss: 174.0512\n",
      "Epoch 551/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5620 - val_loss: 174.0415\n",
      "Epoch 552/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5809 - val_loss: 174.0488\n",
      "Epoch 553/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5585 - val_loss: 174.0549\n",
      "Epoch 554/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5608 - val_loss: 174.0233\n",
      "Epoch 555/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5446 - val_loss: 174.0121\n",
      "Epoch 556/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5465 - val_loss: 174.0004\n",
      "Epoch 557/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.6136 - val_loss: 173.9629\n",
      "Epoch 558/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5704 - val_loss: 174.0091\n",
      "Epoch 559/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5399 - val_loss: 173.9962\n",
      "Epoch 560/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5323 - val_loss: 174.0174\n",
      "Epoch 561/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5479 - val_loss: 174.0621\n",
      "Epoch 562/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5518 - val_loss: 174.0588\n",
      "Epoch 563/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5245 - val_loss: 174.0645\n",
      "Epoch 564/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5374 - val_loss: 174.0974\n",
      "Epoch 565/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5248 - val_loss: 174.0780\n",
      "Epoch 566/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5036 - val_loss: 174.1072\n",
      "Epoch 567/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5066 - val_loss: 174.1245\n",
      "Epoch 568/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5184 - val_loss: 174.1307\n",
      "Epoch 569/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4958 - val_loss: 174.1818\n",
      "Epoch 570/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.5036 - val_loss: 174.1871\n",
      "Epoch 571/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4772 - val_loss: 174.2263\n",
      "Epoch 572/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4828 - val_loss: 174.2449\n",
      "Epoch 573/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4843 - val_loss: 174.2887\n",
      "Epoch 574/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.4708 - val_loss: 174.3062\n",
      "Epoch 575/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4921 - val_loss: 174.3539\n",
      "Epoch 576/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4795 - val_loss: 174.3426\n",
      "Epoch 577/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4630 - val_loss: 174.3735\n",
      "Epoch 578/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4730 - val_loss: 174.3771\n",
      "Epoch 579/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.5001 - val_loss: 174.4297\n",
      "Epoch 580/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4547 - val_loss: 174.4577\n",
      "Epoch 581/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4524 - val_loss: 174.4478\n",
      "Epoch 582/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4760 - val_loss: 174.4187\n",
      "Epoch 583/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4410 - val_loss: 174.4350\n",
      "Epoch 584/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4437 - val_loss: 174.4653\n",
      "Epoch 585/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4356 - val_loss: 174.4716\n",
      "Epoch 586/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4467 - val_loss: 174.4522\n",
      "Epoch 587/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4296 - val_loss: 174.4731\n",
      "Epoch 588/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4322 - val_loss: 174.4832\n",
      "Epoch 589/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4353 - val_loss: 174.4781\n",
      "Epoch 590/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4396 - val_loss: 174.5137\n",
      "Epoch 591/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4188 - val_loss: 174.5321\n",
      "Epoch 592/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4155 - val_loss: 174.5408\n",
      "Epoch 593/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4230 - val_loss: 174.5591\n",
      "Epoch 594/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4158 - val_loss: 174.5721\n",
      "Epoch 595/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.4123 - val_loss: 174.5793\n",
      "Epoch 596/750\n",
      "6/6 [==============================] - 0s 16ms/step - loss: 221.4154 - val_loss: 174.5702\n",
      "Epoch 597/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.4000 - val_loss: 174.5908\n",
      "Epoch 598/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.4107 - val_loss: 174.6147\n",
      "Epoch 599/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.4834 - val_loss: 174.6567\n",
      "Epoch 600/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3994 - val_loss: 174.6401\n",
      "Epoch 601/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4001 - val_loss: 174.6438\n",
      "Epoch 602/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.4559 - val_loss: 174.6057\n",
      "Epoch 603/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3858 - val_loss: 174.6422\n",
      "Epoch 604/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.4216 - val_loss: 174.6964\n",
      "Epoch 605/750\n",
      "6/6 [==============================] - 0s 11ms/step - loss: 221.4353 - val_loss: 174.7421\n",
      "Epoch 606/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3789 - val_loss: 174.7055\n",
      "Epoch 607/750\n",
      "6/6 [==============================] - 0s 12ms/step - loss: 221.3944 - val_loss: 174.7033\n",
      "Epoch 608/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3878 - val_loss: 174.7297\n",
      "Epoch 609/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3716 - val_loss: 174.7499\n",
      "Epoch 610/750\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 221.3735 - val_loss: 174.7538\n",
      "Epoch 611/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3676 - val_loss: 174.7631\n",
      "Epoch 612/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3862 - val_loss: 174.7665\n",
      "Epoch 613/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3587 - val_loss: 174.7751\n",
      "Epoch 614/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3695 - val_loss: 174.7794\n",
      "Epoch 615/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3623 - val_loss: 174.7754\n",
      "Epoch 616/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3583 - val_loss: 174.8020\n",
      "Epoch 617/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3529 - val_loss: 174.8123\n",
      "Epoch 618/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3554 - val_loss: 174.8245\n",
      "Epoch 619/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3672 - val_loss: 174.7908\n",
      "Epoch 620/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3616 - val_loss: 174.7900\n",
      "Epoch 621/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3486 - val_loss: 174.8099\n",
      "Epoch 622/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3523 - val_loss: 174.8449\n",
      "Epoch 623/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3403 - val_loss: 174.8599\n",
      "Epoch 624/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3404 - val_loss: 174.8768\n",
      "Epoch 625/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3337 - val_loss: 174.9336\n",
      "Epoch 626/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3328 - val_loss: 174.9446\n",
      "Epoch 627/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3590 - val_loss: 174.9908\n",
      "Epoch 628/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3446 - val_loss: 174.9621\n",
      "Epoch 629/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3602 - val_loss: 174.9437\n",
      "Epoch 630/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3252 - val_loss: 174.9691\n",
      "Epoch 631/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3404 - val_loss: 174.9787\n",
      "Epoch 632/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3393 - val_loss: 175.0035\n",
      "Epoch 633/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3331 - val_loss: 174.9915\n",
      "Epoch 634/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3174 - val_loss: 175.0184\n",
      "Epoch 635/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3182 - val_loss: 175.0276\n",
      "Epoch 636/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3514 - val_loss: 174.9963\n",
      "Epoch 637/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3217 - val_loss: 175.0134\n",
      "Epoch 638/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3108 - val_loss: 175.0413\n",
      "Epoch 639/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3183 - val_loss: 175.0730\n",
      "Epoch 640/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3267 - val_loss: 175.0926\n",
      "Epoch 641/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3180 - val_loss: 175.0820\n",
      "Epoch 642/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3198 - val_loss: 175.0986\n",
      "Epoch 643/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3043 - val_loss: 175.1173\n",
      "Epoch 644/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3015 - val_loss: 175.1275\n",
      "Epoch 645/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2963 - val_loss: 175.1430\n",
      "Epoch 646/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3467 - val_loss: 175.0914\n",
      "Epoch 647/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3019 - val_loss: 175.1057\n",
      "Epoch 648/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.3299 - val_loss: 175.0878\n",
      "Epoch 649/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3285 - val_loss: 175.1677\n",
      "Epoch 650/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2965 - val_loss: 175.1479\n",
      "Epoch 651/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2877 - val_loss: 175.1562\n",
      "Epoch 652/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3213 - val_loss: 175.2165\n",
      "Epoch 653/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2830 - val_loss: 175.2265\n",
      "Epoch 654/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3749 - val_loss: 175.1954\n",
      "Epoch 655/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3187 - val_loss: 175.2407\n",
      "Epoch 656/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.3061 - val_loss: 175.2798\n",
      "Epoch 657/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2814 - val_loss: 175.3069\n",
      "Epoch 658/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2993 - val_loss: 175.3029\n",
      "Epoch 659/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2744 - val_loss: 175.3002\n",
      "Epoch 660/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2813 - val_loss: 175.3109\n",
      "Epoch 661/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2837 - val_loss: 175.2709\n",
      "Epoch 662/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2796 - val_loss: 175.2586\n",
      "Epoch 663/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2845 - val_loss: 175.2868\n",
      "Epoch 664/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2832 - val_loss: 175.2693\n",
      "Epoch 665/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2935 - val_loss: 175.3156\n",
      "Epoch 666/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2733 - val_loss: 175.3064\n",
      "Epoch 667/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2749 - val_loss: 175.3245\n",
      "Epoch 668/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2648 - val_loss: 175.2860\n",
      "Epoch 669/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2905 - val_loss: 175.2518\n",
      "Epoch 670/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2618 - val_loss: 175.2655\n",
      "Epoch 671/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2841 - val_loss: 175.2955\n",
      "Epoch 672/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2711 - val_loss: 175.2744\n",
      "Epoch 673/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2648 - val_loss: 175.2938\n",
      "Epoch 674/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2664 - val_loss: 175.2983\n",
      "Epoch 675/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2589 - val_loss: 175.3046\n",
      "Epoch 676/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2681 - val_loss: 175.3063\n",
      "Epoch 677/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2511 - val_loss: 175.3154\n",
      "Epoch 678/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2551 - val_loss: 175.3203\n",
      "Epoch 679/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2603 - val_loss: 175.3797\n",
      "Epoch 680/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2410 - val_loss: 175.4091\n",
      "Epoch 681/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2741 - val_loss: 175.4821\n",
      "Epoch 682/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2528 - val_loss: 175.4869\n",
      "Epoch 683/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2439 - val_loss: 175.4931\n",
      "Epoch 684/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2735 - val_loss: 175.5080\n",
      "Epoch 685/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2568 - val_loss: 175.4959\n",
      "Epoch 686/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2639 - val_loss: 175.5538\n",
      "Epoch 687/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2636 - val_loss: 175.5189\n",
      "Epoch 688/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2509 - val_loss: 175.5490\n",
      "Epoch 689/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2418 - val_loss: 175.5580\n",
      "Epoch 690/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2378 - val_loss: 175.5786\n",
      "Epoch 691/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2321 - val_loss: 175.5887\n",
      "Epoch 692/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2383 - val_loss: 175.6029\n",
      "Epoch 693/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2585 - val_loss: 175.6375\n",
      "Epoch 694/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2364 - val_loss: 175.6520\n",
      "Epoch 695/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2430 - val_loss: 175.6431\n",
      "Epoch 696/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2362 - val_loss: 175.6304\n",
      "Epoch 697/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2634 - val_loss: 175.5989\n",
      "Epoch 698/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2321 - val_loss: 175.6116\n",
      "Epoch 699/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2415 - val_loss: 175.5980\n",
      "Epoch 700/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2332 - val_loss: 175.6195\n",
      "Epoch 701/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2373 - val_loss: 175.6477\n",
      "Epoch 702/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2265 - val_loss: 175.6489\n",
      "Epoch 703/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2491 - val_loss: 175.6292\n",
      "Epoch 704/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2438 - val_loss: 175.6117\n",
      "Epoch 705/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2224 - val_loss: 175.6332\n",
      "Epoch 706/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2377 - val_loss: 175.6338\n",
      "Epoch 707/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2514 - val_loss: 175.6870\n",
      "Epoch 708/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2214 - val_loss: 175.6932\n",
      "Epoch 709/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2126 - val_loss: 175.6896\n",
      "Epoch 710/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2247 - val_loss: 175.6628\n",
      "Epoch 711/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2188 - val_loss: 175.6608\n",
      "Epoch 712/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2158 - val_loss: 175.6890\n",
      "Epoch 713/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2305 - val_loss: 175.6717\n",
      "Epoch 714/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2380 - val_loss: 175.6562\n",
      "Epoch 715/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2277 - val_loss: 175.6943\n",
      "Epoch 716/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2227 - val_loss: 175.7042\n",
      "Epoch 717/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2074 - val_loss: 175.7377\n",
      "Epoch 718/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2268 - val_loss: 175.7820\n",
      "Epoch 719/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2485 - val_loss: 175.7452\n",
      "Epoch 720/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2143 - val_loss: 175.7700\n",
      "Epoch 721/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2131 - val_loss: 175.7854\n",
      "Epoch 722/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2088 - val_loss: 175.7728\n",
      "Epoch 723/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1985 - val_loss: 175.7752\n",
      "Epoch 724/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2289 - val_loss: 175.8029\n",
      "Epoch 725/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2180 - val_loss: 175.7654\n",
      "Epoch 726/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1963 - val_loss: 175.7696\n",
      "Epoch 727/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2047 - val_loss: 175.7971\n",
      "Epoch 728/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2041 - val_loss: 175.8318\n",
      "Epoch 729/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2184 - val_loss: 175.8111\n",
      "Epoch 730/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2129 - val_loss: 175.8015\n",
      "Epoch 731/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2078 - val_loss: 175.8236\n",
      "Epoch 732/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1911 - val_loss: 175.8433\n",
      "Epoch 733/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2665 - val_loss: 175.8958\n",
      "Epoch 734/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2291 - val_loss: 175.8834\n",
      "Epoch 735/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2061 - val_loss: 175.9079\n",
      "Epoch 736/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1983 - val_loss: 175.9339\n",
      "Epoch 737/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1958 - val_loss: 175.9206\n",
      "Epoch 738/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1952 - val_loss: 175.9153\n",
      "Epoch 739/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1790 - val_loss: 175.9381\n",
      "Epoch 740/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2269 - val_loss: 175.9974\n",
      "Epoch 741/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2290 - val_loss: 175.9689\n",
      "Epoch 742/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1945 - val_loss: 175.9900\n",
      "Epoch 743/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1943 - val_loss: 175.9995\n",
      "Epoch 744/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1915 - val_loss: 176.0294\n",
      "Epoch 745/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1898 - val_loss: 176.0764\n",
      "Epoch 746/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.2101 - val_loss: 176.0557\n",
      "Epoch 747/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1865 - val_loss: 176.0466\n",
      "Epoch 748/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.2113 - val_loss: 176.0399\n",
      "Epoch 749/750\n",
      "6/6 [==============================] - 0s 9ms/step - loss: 221.1827 - val_loss: 176.0242\n",
      "Epoch 750/750\n",
      "6/6 [==============================] - 0s 8ms/step - loss: 221.1929 - val_loss: 176.0541\n",
      ">Saved ./models/comp4948_a2_model_10.h5\n",
      ">loaded ./models/comp4948_a2_model_1.h5\n",
      ">loaded ./models/comp4948_a2_model_2.h5\n",
      ">loaded ./models/comp4948_a2_model_3.h5\n",
      ">loaded ./models/comp4948_a2_model_4.h5\n",
      ">loaded ./models/comp4948_a2_model_5.h5\n",
      ">loaded ./models/comp4948_a2_model_6.h5\n",
      ">loaded ./models/comp4948_a2_model_7.h5\n",
      ">loaded ./models/comp4948_a2_model_8.h5\n",
      ">loaded ./models/comp4948_a2_model_9.h5\n",
      ">loaded ./models/comp4948_a2_model_10.h5\n",
      "Loaded 10 models\n"
     ]
    }
   ],
   "source": [
    "# generate models\n",
    "generateModels(X_train, y_train)\n",
    "\n",
    "# load all models\n",
    "models = load_all_models(NUM_MODEL)\n",
    "print('Loaded %d models' % len(models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c84655c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:28:22.812958Z",
     "iopub.status.busy": "2022-04-29T00:28:22.812587Z",
     "iopub.status.idle": "2022-04-29T00:28:24.316899Z",
     "shell.execute_reply": "2022-04-29T00:28:24.315776Z"
    },
    "papermill": {
     "duration": 8.280396,
     "end_time": "2022-04-29T00:28:24.319835",
     "exception": false,
     "start_time": "2022-04-29T00:28:16.039439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MSE: 175.718\n",
      "Model RMSE: 13.256\n",
      "Model MSE: 175.981\n",
      "Model RMSE: 13.266\n",
      "Model MSE: 175.840\n",
      "Model RMSE: 13.260\n",
      "Model MSE: 176.396\n",
      "Model RMSE: 13.281\n",
      "Model MSE: 175.466\n",
      "Model RMSE: 13.246\n",
      "Model MSE: 175.825\n",
      "Model RMSE: 13.260\n",
      "Model MSE: 175.951\n",
      "Model RMSE: 13.265\n",
      "Model MSE: 175.151\n",
      "Model RMSE: 13.234\n",
      "Model MSE: 176.721\n",
      "Model RMSE: 13.294\n",
      "Model MSE: 176.054\n",
      "Model RMSE: 13.269\n"
     ]
    }
   ],
   "source": [
    "# # evaluate standalone models on test dataset\n",
    "for model in models:\n",
    "    mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Model MSE: %.3f' % mse)\n",
    "    print('Model RMSE: %.3f' % np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a451abfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:28:38.012124Z",
     "iopub.status.busy": "2022-04-29T00:28:38.011844Z",
     "iopub.status.idle": "2022-04-29T00:28:38.019111Z",
     "shell.execute_reply": "2022-04-29T00:28:38.018030Z"
    },
    "papermill": {
     "duration": 6.85762,
     "end_time": "2022-04-29T00:28:38.021103",
     "exception": false,
     "start_time": "2022-04-29T00:28:31.163483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create stacked model input dataset as outputs from the ensemble\n",
    "def getStackedData(models, inputX):\n",
    "    stackXdf = None\n",
    "    for model in models:\n",
    "        # make prediction\n",
    "        yhat = model.predict(inputX, verbose=0)\n",
    "        singleModelPredDf = pd.DataFrame(np.row_stack(yhat))\n",
    "\n",
    "        if stackXdf is None:\n",
    "            stackXdf = singleModelPredDf\n",
    "        else:\n",
    "            numClasses = len(singleModelPredDf.keys())\n",
    "            numStackXCols = len(stackXdf.keys())\n",
    "\n",
    "            # Add new classification columns.\n",
    "            for i in range(0, numClasses):\n",
    "                stackXdf[numStackXCols + i] = singleModelPredDf[i]\n",
    "    return stackXdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4cf9fb59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:28:51.534262Z",
     "iopub.status.busy": "2022-04-29T00:28:51.533955Z",
     "iopub.status.idle": "2022-04-29T00:28:51.538566Z",
     "shell.execute_reply": "2022-04-29T00:28:51.537685Z"
    },
    "papermill": {
     "duration": 6.820146,
     "end_time": "2022-04-29T00:28:51.540795",
     "exception": false,
     "start_time": "2022-04-29T00:28:44.720649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions with the stacked model\n",
    "def stacked_prediction(models, model, inputX):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = getStackedData(models, inputX)\n",
    "    # make a prediction\n",
    "    yhat = model.predict(stackedX)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5cdcd7fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:29:05.175063Z",
     "iopub.status.busy": "2022-04-29T00:29:05.174731Z",
     "iopub.status.idle": "2022-04-29T00:29:05.179971Z",
     "shell.execute_reply": "2022-04-29T00:29:05.178971Z"
    },
    "papermill": {
     "duration": 6.790125,
     "end_time": "2022-04-29T00:29:05.181938",
     "exception": false,
     "start_time": "2022-04-29T00:28:58.391813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit a model based on the outputs from the ensemble models\n",
    "def fit_stacked_model(models, inputX, inputy):\n",
    "    # create dataset using ensemble\n",
    "    stackedX = getStackedData(models, inputX)\n",
    "    # fit standalone model\n",
    "    model = LinearRegression()\n",
    "    history = model.fit(stackedX, inputy)\n",
    "    # showLoss(history, \"Loss - Stacked\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dac6b8da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:29:19.006511Z",
     "iopub.status.busy": "2022-04-29T00:29:19.006221Z",
     "iopub.status.idle": "2022-04-29T00:29:19.948703Z",
     "shell.execute_reply": "2022-04-29T00:29:19.947957Z"
    },
    "papermill": {
     "duration": 7.753557,
     "end_time": "2022-04-29T00:29:19.950963",
     "exception": false,
     "start_time": "2022-04-29T00:29:12.197406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fit stacked model using the ensemble\n",
    "model = fit_stacked_model(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26b09524",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:29:33.638059Z",
     "iopub.status.busy": "2022-04-29T00:29:33.637783Z",
     "iopub.status.idle": "2022-04-29T00:29:34.197088Z",
     "shell.execute_reply": "2022-04-29T00:29:34.196119Z"
    },
    "papermill": {
     "duration": 7.440149,
     "end_time": "2022-04-29T00:29:34.199735",
     "exception": false,
     "start_time": "2022-04-29T00:29:26.759586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****Stacked Test RMSE: 11.637\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test set\n",
    "yhat = stacked_prediction(models, model, X_test)\n",
    "stackedRmse = mean_squared_error(y_test, yhat, squared=False)\n",
    "print('\\n*****Stacked Test RMSE: %.3f' % stackedRmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5cbfef",
   "metadata": {
    "papermill": {
     "duration": 6.816066,
     "end_time": "2022-04-29T00:29:47.796490",
     "exception": false,
     "start_time": "2022-04-29T00:29:40.980424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Evaluation\n",
    "RMSE of each model will be compared and evaluated here to determine the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "438d7124",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:30:01.526104Z",
     "iopub.status.busy": "2022-04-29T00:30:01.525743Z",
     "iopub.status.idle": "2022-04-29T00:30:01.537526Z",
     "shell.execute_reply": "2022-04-29T00:30:01.536622Z"
    },
    "papermill": {
     "duration": 6.900807,
     "end_time": "2022-04-29T00:30:01.539599",
     "exception": false,
     "start_time": "2022-04-29T00:29:54.638792",
     "status": "completed"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Type</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>14.614322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>15.315320330433583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagged Linear Regression</td>\n",
       "      <td>16.529897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stacked Model</td>\n",
       "      <td>11.637384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Type                RMSE\n",
       "0         Linear Regression           14.614322\n",
       "1            Neural Network  15.315320330433583\n",
       "2  Bagged Linear Regression           16.529897\n",
       "3             Stacked Model           11.637384"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelEvalDict = {\n",
    "    \"Model Type\" : [\"Linear Regression\", \"Neural Network\", \"Bagged Linear Regression\", \"Stacked Model\"]\n",
    "    , \"RMSE\" : [lrRMSE, annRMSE, baggedRMSE, stackedRmse]\n",
    "}\n",
    "\n",
    "modelEvalDf = pd.DataFrame(modelEvalDict)\n",
    "\n",
    "modelEvalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedcf5ae",
   "metadata": {
    "papermill": {
     "duration": 6.810559,
     "end_time": "2022-04-29T00:30:15.093380",
     "exception": false,
     "start_time": "2022-04-29T00:30:08.282821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The stacked model outperformed all the other models with the lowest RMSE. Please note the numbers may be a bit different from what are recorded on the notebook as they are results of one sample trial run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a0eb2",
   "metadata": {
    "papermill": {
     "duration": 6.802338,
     "end_time": "2022-04-29T00:30:28.676885",
     "exception": false,
     "start_time": "2022-04-29T00:30:21.874547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion and Suggestions\n",
    "Using the best model built above, we will look into what method and instructor students need to work with to perform better.\n",
    "\n",
    "First, a dummy data will be created to be fed to the model. Each row signifies every possible combinations of student traits (ethnicities + genders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40392fd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:30:42.321423Z",
     "iopub.status.busy": "2022-04-29T00:30:42.321135Z",
     "iopub.status.idle": "2022-04-29T00:30:42.325573Z",
     "shell.execute_reply": "2022-04-29T00:30:42.324609Z"
    },
    "papermill": {
     "duration": 6.724785,
     "end_time": "2022-04-29T00:30:42.327688",
     "exception": false,
     "start_time": "2022-04-29T00:30:35.602903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH_MODEL =  \"D:\\\\Roelle\\\\BCIT\\\\Term 4\\\\COMP 4948 Maching Learning\\\\Assignment\\\\A2\\\\code\\\\model\\\\models\\\\\"\n",
    "MODEL_NAME = \"comp4948_a2_model_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd663f6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:30:55.905576Z",
     "iopub.status.busy": "2022-04-29T00:30:55.905200Z",
     "iopub.status.idle": "2022-04-29T00:30:55.909597Z",
     "shell.execute_reply": "2022-04-29T00:30:55.909012Z"
    },
    "papermill": {
     "duration": 6.75956,
     "end_time": "2022-04-29T00:30:55.911528",
     "exception": false,
     "start_time": "2022-04-29T00:30:49.151968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "teacherLi = [\"Teacher_Smith\", \"Teacher_Wesson\"]\n",
    "genderLi = [\"Gender_Female\", \"Gender_Male\"]\n",
    "ethnicityLi = [\"Ethnic_African-American\", \"Ethnic_Asian\", \"Ethnic_Caucasian\", \"Ethnic_Hispanic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d4c2353",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:31:09.580548Z",
     "iopub.status.busy": "2022-04-29T00:31:09.579971Z",
     "iopub.status.idle": "2022-04-29T00:31:09.587959Z",
     "shell.execute_reply": "2022-04-29T00:31:09.587359Z"
    },
    "papermill": {
     "duration": 6.887518,
     "end_time": "2022-04-29T00:31:09.590042",
     "exception": false,
     "start_time": "2022-04-29T00:31:02.702524",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = studentDf[featureNLi].values\n",
    "y = studentDf['Score'].values\n",
    "\n",
    "ROW_DIM = 0\n",
    "COL_DIM = 1\n",
    "\n",
    "x_arrayReshaped = X.reshape(X.shape[ROW_DIM], X.shape[COL_DIM])\n",
    "y_arrayReshaped = y.reshape(y.shape[ROW_DIM],1)\n",
    "\n",
    "# Split the data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_arrayReshaped,\n",
    "         y_arrayReshaped, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "015433fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:31:23.152817Z",
     "iopub.status.busy": "2022-04-29T00:31:23.152499Z",
     "iopub.status.idle": "2022-04-29T00:31:23.158973Z",
     "shell.execute_reply": "2022-04-29T00:31:23.158120Z"
    },
    "papermill": {
     "duration": 6.749432,
     "end_time": "2022-04-29T00:31:23.160972",
     "exception": false,
     "start_time": "2022-04-29T00:31:16.411540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def buildX(teacher, gender, ethnicity):\n",
    "    \"\"\"\n",
    "    Create a blank template for student data and add 1 to passed values.\n",
    "    :param teacher: str\n",
    "    :param gender: str\n",
    "    :param ethnicity: str\n",
    "    :return: a new dataframe of a single row that represents one student trait.\n",
    "    \"\"\"\n",
    "    blankDf = pd.DataFrame({\n",
    "        \"Teacher_Smith\": [0]\n",
    "        , \"Teacher_Wesson\": [0]\n",
    "        , \"Gender_Female\" : [0]\n",
    "        , \"Gender_Male\" : [0]\n",
    "        , \"Ethnic_African-American\" : [0]\n",
    "        , \"Ethnic_Asian\" : [0]\n",
    "        , \"Ethnic_Caucasian\" : [0]\n",
    "        , \"Ethnic_Hispanic\" : [0]\n",
    "    })\n",
    "\n",
    "    blankDf[teacher] = 1\n",
    "    blankDf[gender] = 1\n",
    "    blankDf[ethnicity] = 1\n",
    "\n",
    "    return blankDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c40fd49c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:31:36.901826Z",
     "iopub.status.busy": "2022-04-29T00:31:36.901495Z",
     "iopub.status.idle": "2022-04-29T00:31:36.935350Z",
     "shell.execute_reply": "2022-04-29T00:31:36.934527Z"
    },
    "papermill": {
     "duration": 6.972833,
     "end_time": "2022-04-29T00:31:36.937566",
     "exception": false,
     "start_time": "2022-04-29T00:31:29.964733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Teacher_Smith</th>\n",
       "      <th>Teacher_Wesson</th>\n",
       "      <th>Gender_Female</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Ethnic_African-American</th>\n",
       "      <th>Ethnic_Asian</th>\n",
       "      <th>Ethnic_Caucasian</th>\n",
       "      <th>Ethnic_Hispanic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Teacher_Smith  Teacher_Wesson  Gender_Female  Gender_Male  \\\n",
       "0              1               0              1            0   \n",
       "0              1               0              1            0   \n",
       "0              1               0              1            0   \n",
       "0              1               0              1            0   \n",
       "0              1               0              0            1   \n",
       "\n",
       "   Ethnic_African-American  Ethnic_Asian  Ethnic_Caucasian  Ethnic_Hispanic  \n",
       "0                        1             0                 0                0  \n",
       "0                        0             1                 0                0  \n",
       "0                        0             0                 1                0  \n",
       "0                        0             0                 0                1  \n",
       "0                        1             0                 0                0  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create all possibilities of student traits coordinating genders, ethnicities and teachers\n",
    "forEvalDf = pd.DataFrame()\n",
    "\n",
    "for teacher in teacherLi:\n",
    "    for gender in genderLi:\n",
    "        for ethnic in ethnicityLi:\n",
    "            newRow = (buildX(teacher, gender, ethnic))\n",
    "            forEvalDf = pd.concat([forEvalDf, newRow])\n",
    "            \n",
    "forEvalDf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f564fe",
   "metadata": {
    "papermill": {
     "duration": 6.743179,
     "end_time": "2022-04-29T00:31:50.610530",
     "exception": false,
     "start_time": "2022-04-29T00:31:43.867351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Each row indicates a unique combination of student's traits. This dataframe will be now fed to the stacked model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c78a190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-29T00:32:04.219489Z",
     "iopub.status.busy": "2022-04-29T00:32:04.218693Z",
     "iopub.status.idle": "2022-04-29T00:32:04.635573Z",
     "shell.execute_reply": "2022-04-29T00:32:04.634709Z"
    },
    "papermill": {
     "duration": 7.20702,
     "end_time": "2022-04-29T00:32:04.637367",
     "exception": true,
     "start_time": "2022-04-29T00:31:57.430347",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: D:\\Roelle\\BCIT\\Term 4\\COMP 4948 Maching Learning\\Assignment\\A2\\code\\model\\models\\comp4948_a2_model_1.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19/2424767152.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_all_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_stacked_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstacked_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforEvalDf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_19/996241232.py\u001b[0m in \u001b[0;36mload_all_models\u001b[0;34m(n_models)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPATH_MODEL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# load model from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# add to list of models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mall_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m   raise IOError(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;31m# Look for metadata file or parse the SavedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m   \u001b[0mobject_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mpath_to_metadata_pb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_METADATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n\u001b[0;32m--> 121\u001b[0;31m          constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: D:\\Roelle\\BCIT\\Term 4\\COMP 4948 Maching Learning\\Assignment\\A2\\code\\model\\models\\comp4948_a2_model_1.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "models = load_all_models(NUM_MODEL)\n",
    "model = fit_stacked_model(models, X_test, y_test)\n",
    "yhat = stacked_prediction(models, model, forEvalDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42914814",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "yhat = yhat.tolist()\n",
    "yhatLi = []\n",
    "\n",
    "# convert to list\n",
    "for y in yhat:\n",
    "    yhatLi.append(y[0])\n",
    "\n",
    "# add prediction array as a Score column\n",
    "forEvalDf['Score'] = np.array(yhatLi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a3455",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Using the prediction data, a new dataframe will be created to easily compare math scores of each student trait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5059de",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def suggestTeacher(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create a new dataframe that compares scores for each student trait.\n",
    "    :param df: pd.DataFrame\n",
    "    :return: new dataframe\n",
    "    \"\"\"\n",
    "    resDf = pd.DataFrame()\n",
    "    for i in range(df.shape[0]):\n",
    "        tmpSeries = df.iloc[i]\n",
    "\n",
    "        if tmpSeries[\"Teacher_Smith\"] == 1:\n",
    "            teacher = \"Smith\"\n",
    "        else:\n",
    "            teacher = \"Wesson\"\n",
    "\n",
    "        if tmpSeries[\"Gender_Female\"] == 1:\n",
    "            gender = \"Female\"\n",
    "        else:\n",
    "            gender = \"male\"\n",
    "\n",
    "        if tmpSeries[\"Ethnic_African-American\"] == 1:\n",
    "            ethnic = \"African-American\"\n",
    "        elif tmpSeries[\"Ethnic_Asian\"] == 1:\n",
    "            ethnic = \"Asian\"\n",
    "        elif tmpSeries[\"Ethnic_Caucasian\"] == 1:\n",
    "            ethnic = \"Caucasian\"\n",
    "        else:\n",
    "            ethnic = \"Hispanic\"\n",
    "\n",
    "        tmpRow = pd.DataFrame({\n",
    "            \"Teacher\" : [teacher]\n",
    "            , \"Gender\" : [gender]\n",
    "            , \"Ethnicity\" : [ethnic]\n",
    "            , \"Score\" : [tmpSeries[\"Score\"]]\n",
    "        })\n",
    "\n",
    "        resDf = pd.concat([resDf, tmpRow], ignore_index=True)\n",
    "        resDf.sort_values(by=[\"Gender\", \"Ethnicity\", \"Score\"], ascending=False, inplace=True, ignore_index=True)\n",
    "\n",
    "    return resDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956bf6a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "suggestionDf = suggestTeacher(forEvalDf)\n",
    "suggestionDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a2b4c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can easily compare predicted scores and find out which teacher is more likely to help students get better scores on Math. Again, Ms. Ruger was removed from the consideration as it appeared her class negatively impacts students' performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe7f66",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "groupTeacher = pd.pivot_table(suggestionDf, index=[\"Gender\", \"Ethnicity\"], columns=[\"Teacher\"], aggfunc=\"sum\", values=\"Score\")\n",
    "\n",
    "plt.figure(figsize = (50, 50))\n",
    "groupTeacher.plot(kind ='bar')\n",
    "plt.title(\"Score Comparisons for Different Teachers By Student Traits\")\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n",
    "          ncol=3, fancybox=True, shadow=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185bf38",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "It is predicted that all categories of students would perform better when they are taught by Ms. Smith. Both Ms. Smith and Ms. Wesson appear to be very capable teachers, but this may indicate the standard-based teaching method would generally improve students' grades."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 735.402927,
   "end_time": "2022-04-29T00:32:14.630934",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-29T00:19:59.228007",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
